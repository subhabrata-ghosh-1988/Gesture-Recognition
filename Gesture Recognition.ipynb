{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Gesture Recognition.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/subhabrata-ghosh-1988/Gesture-Recognition/blob/main/Gesture%20Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7ZfgtjxNVrR"
      },
      "source": [
        "# Gesture Recognition\n",
        "In this group project, you are going to build a 3D Conv model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-eH5pKuNVrV"
      },
      "source": [
        "## Import libraries\n",
        "import numpy as np\n",
        "import os\n",
        "from scipy import misc\n",
        "from imageio import imread\n",
        "import cv2\n",
        "from skimage import transform,io\n",
        "import datetime\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import abc\n",
        "from sys import getsizeof\n",
        "import shutil\n",
        "import abc\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J94bqUhVNVrW"
      },
      "source": [
        "import glob, os , shutil\n",
        "for f in glob.glob(\"/content/model_init*\"):\n",
        "    shutil.rmtree(f)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0Ubj-HfSRY3"
      },
      "source": [
        "## remove the existing zip file\n",
        "shutil.rmtree('/content/Project_data.zip', ignore_errors=True)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4ty5a19SiNl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "outputId": "fae75b50-ba8f-4f7f-a8aa-3ca53e916fc5"
      },
      "source": [
        "## Initiate the file download\n",
        "!pip install gdown\n",
        "import gdown\n",
        "url=\"https://drive.google.com/uc?id=1kM4V7pnLjGbuCaDpfBNHig99gr2rdqyJ\"\n",
        "output = \"Project_data.zip\"\n",
        "gdown.download(url, output, quiet=False)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (3.6.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.41.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (3.0.4)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1kM4V7pnLjGbuCaDpfBNHig99gr2rdqyJ\n",
            "To: /content/Project_data.zip\n",
            "1.71GB [00:17, 100MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Project_data.zip'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-fwe5jbTX8r"
      },
      "source": [
        "## unzip the downloaded folder\n",
        "shutil.rmtree('/content/Project_data', ignore_errors=True)\n",
        "shutil.unpack_archive(\"Project_data.zip\", \"Project_data\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIQHPGi_NVrX"
      },
      "source": [
        "We set the random seed so that the results don't vary drastically."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ci_N-NnWNVrZ"
      },
      "source": [
        "np.random.seed(30)\n",
        "import random as rn\n",
        "rn.seed(30)\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(30)\n",
        "tf.compat.v1.disable_eager_execution()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bWyzW_oNVrZ"
      },
      "source": [
        "**In this block, you read the folder names for training and validation. You also set the batch_size here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyTupvi3NVrb"
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation\n",
        "from keras.layers.convolutional import Conv3D, MaxPooling3D, Conv2D, MaxPooling2D\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras import optimizers\n",
        "from keras.layers import Dropout"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCjeEg_tNVrc"
      },
      "source": [
        "# the entire dataset is placed in below directory\n",
        "main_folder='/content/Project_data/Project_data'"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSUMkqaSXcC4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "99615058-e67f-43f2-e512-fd4d68b5a815"
      },
      "source": [
        "## Checking current TF version\n",
        "tf.version.VERSION"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REgxbVunNVrd"
      },
      "source": [
        "### Plotting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3UJmY03NVre"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot(Model):\n",
        "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,4))\n",
        "    axes[0].plot(Model.history['loss'])   \n",
        "    axes[0].plot(Model.history['val_loss'])\n",
        "    axes[0].legend(['loss','val_loss'])\n",
        "\n",
        "    axes[1].plot(Model.history['categorical_accuracy'])   \n",
        "    axes[1].plot(Model.history['val_categorical_accuracy'])\n",
        "    axes[1].legend(['categorical_accuracy','val_categorical_accuracy'])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtV6apQQNVrf"
      },
      "source": [
        "## Generator\n",
        "This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haSnpGsboLKX"
      },
      "source": [
        "import gc\n",
        "class GCCallback(tf.keras.callbacks.Callback):\n",
        "      def on_epoch_end(self, epoch, logs=None):\n",
        "          print(gc.collect())"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YC-BRedNVri"
      },
      "source": [
        "# referred ABC library use case from this link https://riptutorial.com/python/example/23083/why-how-to-use-abcmeta-and--abstractmethod\n",
        "\n",
        "class ModelBuilder(metaclass= abc.ABCMeta):\n",
        "    \n",
        "    def initialize_src_path(self,main_folder):\n",
        "        self.train_doc = np.random.permutation(open(main_folder + '/' + 'train.csv').readlines())\n",
        "        self.val_doc = np.random.permutation(open(main_folder + '/' + 'val.csv').readlines())\n",
        "        self.train_path = main_folder + '/' + 'train'\n",
        "        self.val_path =  main_folder + '/' + 'val'\n",
        "        self.num_train_sequences = len(self.train_doc)\n",
        "        self.num_val_sequences = len(self.val_doc)\n",
        "        \n",
        "    def initialize_image_properties(self,image_height=100,image_width=100):\n",
        "        self.image_height=image_height\n",
        "        self.image_width=image_width\n",
        "        self.channels=3\n",
        "        self.num_classes=5\n",
        "        self.total_frames=30\n",
        "          \n",
        "    def initialize_hyperparams(self,frames_to_sample=30,batch_size=40,num_epochs=20):\n",
        "        self.frames_to_sample=frames_to_sample\n",
        "        self.batch_size=batch_size\n",
        "        self.num_epochs=num_epochs\n",
        "        \n",
        "        \n",
        "    def generator(self,source_path, folder_list, augment=False):\n",
        "        img_idx = np.round(np.linspace(0,self.total_frames-1,self.frames_to_sample)).astype(int)\n",
        "        batch_size=self.batch_size\n",
        "        while True:\n",
        "            t = np.random.permutation(folder_list)\n",
        "            num_batches = len(t)//batch_size\n",
        "        \n",
        "            for batch in range(num_batches): \n",
        "                batch_data, batch_labels= self.one_batch_data(source_path,t,batch,batch_size,img_idx,augment)\n",
        "                yield batch_data, batch_labels \n",
        "\n",
        "            remaining_seq=len(t)%batch_size\n",
        "        \n",
        "            if (remaining_seq != 0):\n",
        "                batch_data, batch_labels= self.one_batch_data(source_path,t,num_batches,batch_size,img_idx,augment,remaining_seq)\n",
        "                yield batch_data, batch_labels \n",
        "    \n",
        "    \n",
        "    def one_batch_data(self,source_path,t,batch,batch_size,img_idx,augment,remaining_seq=0):\n",
        "    \n",
        "        seq_len = remaining_seq if remaining_seq else batch_size\n",
        "    \n",
        "        batch_data = np.zeros((seq_len,len(img_idx),self.image_height,self.image_width,self.channels)) \n",
        "        batch_labels = np.zeros((seq_len,self.num_classes)) \n",
        "    \n",
        "        if (augment): batch_data_aug = np.zeros((seq_len,len(img_idx),self.image_height,self.image_width,self.channels))\n",
        "\n",
        "        \n",
        "        for folder in range(seq_len): \n",
        "            imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) \n",
        "            for idx,item in enumerate(img_idx): \n",
        "                image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
        "                image_resized=transform.resize(image,(self.image_height,self.image_width,3))\n",
        "            \n",
        "\n",
        "                batch_data[folder,idx,:,:,0] = (image_resized[:,:,0])/255\n",
        "                batch_data[folder,idx,:,:,1] = (image_resized[:,:,1])/255\n",
        "                batch_data[folder,idx,:,:,2] = (image_resized[:,:,2])/255\n",
        "            \n",
        "                if (augment):\n",
        "                    shifted = cv2.warpAffine(image, \n",
        "                                             np.float32([[1, 0, np.random.randint(-30,30)],[0, 1, np.random.randint(-30,30)]]), \n",
        "                                            (image.shape[1], image.shape[0]))\n",
        "                    \n",
        "                    gray = cv2.cvtColor(shifted,cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "                    x0, y0 = np.argwhere(gray > 0).min(axis=0)\n",
        "                    x1, y1 = np.argwhere(gray > 0).max(axis=0) \n",
        "                    \n",
        "                    cropped=shifted[x0:x1,y0:y1,:]\n",
        "                    \n",
        "                    image_resized=transform.resize(cropped,(self.image_height,self.image_width,3))\n",
        "            \n",
        "                    batch_data_aug[folder,idx,:,:,0] = (image_resized[:,:,0])/255\n",
        "                    batch_data_aug[folder,idx,:,:,1] = (image_resized[:,:,1])/255\n",
        "                    batch_data_aug[folder,idx,:,:,2] = (image_resized[:,:,2])/255\n",
        "                \n",
        "            \n",
        "            batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
        "            \n",
        "    \n",
        "        if (augment):\n",
        "            batch_data=np.concatenate([batch_data,batch_data_aug])\n",
        "            batch_labels=np.concatenate([batch_labels,batch_labels])\n",
        "\n",
        "        \n",
        "        return(batch_data,batch_labels)\n",
        "    \n",
        "    \n",
        "    def train_model(self, model, augment_data=False):\n",
        "        train_generator = self.generator(self.train_path, self.train_doc,augment=augment_data)\n",
        "        val_generator = self.generator(self.val_path, self.val_doc)\n",
        "\n",
        "        model_name = 'model_init' + '_' + str(datetime.datetime.now()).replace(' ','').replace(':','_') + '/'\n",
        "    \n",
        "        if not os.path.exists(model_name):\n",
        "            os.mkdir(model_name)\n",
        "        \n",
        "        filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
        "\n",
        "        checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
        "        LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4)\n",
        "        callbacks_list = [checkpoint, LR]\n",
        "\n",
        "        if (self.num_train_sequences%self.batch_size) == 0:\n",
        "            steps_per_epoch = int(self.num_train_sequences/self.batch_size)\n",
        "        else:\n",
        "            steps_per_epoch = (self.num_train_sequences//self.batch_size) + 1\n",
        "\n",
        "        if (self.num_val_sequences%self.batch_size) == 0:\n",
        "            validation_steps = int(self.num_val_sequences/self.batch_size)\n",
        "        else:\n",
        "            validation_steps = (self.num_val_sequences//self.batch_size) + 1\n",
        "    \n",
        "        history=model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=self.num_epochs, verbose=1, \n",
        "                            callbacks=callbacks_list, validation_data=val_generator, \n",
        "                            validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n",
        "        return history\n",
        "\n",
        "    def clear_session(self, model):\n",
        "        del model\n",
        "        gc.collect()\n",
        "        tf.keras.backend.clear_session()\n",
        "        tf.compat.v1.reset_default_graph() # TF graph isn't same as Keras graph\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def define_model(self):\n",
        "        pass\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkdCou-eNVri"
      },
      "source": [
        "## Sample Model\n",
        "Here you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1XR94jpNVrk"
      },
      "source": [
        "class ModelConv3D1(ModelBuilder):\n",
        "    \n",
        "    def define_model(self):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Conv3D(16, (3, 3, 3), padding='same',\n",
        "                 input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(32, (2, 2, 2), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(64, (2, 2, 2), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(128, (2, 2, 2), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(128,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.5))\n",
        "\n",
        "        model.add(Dense(64,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.25))\n",
        "\n",
        "\n",
        "        model.add(Dense(self.num_classes,activation='softmax'))\n",
        "\n",
        "        optimiser = optimizers.Adam()\n",
        "        #optimiser = 'sgd'\n",
        "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "        return model"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PIGA0cbNVrl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5614af7d-1f28-47cf-b8d0-a5ddf37adec4"
      },
      "source": [
        "Conv3D1=ModelConv3D1()\n",
        "Conv3D1.initialize_src_path(main_folder)\n",
        "Conv3D1.initialize_image_properties(image_height=160,image_width=160)\n",
        "Conv3D1.initialize_hyperparams(frames_to_sample=30,batch_size=30,num_epochs=1)\n",
        "Conv3D1_model=Conv3D1.define_model()\n",
        "Conv3D1_model.summary()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv3d (Conv3D)              (None, 30, 160, 160, 16)  1312      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 30, 160, 160, 16)  0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 30, 160, 160, 16)  64        \n",
            "_________________________________________________________________\n",
            "max_pooling3d (MaxPooling3D) (None, 15, 80, 80, 16)    0         \n",
            "_________________________________________________________________\n",
            "conv3d_1 (Conv3D)            (None, 15, 80, 80, 32)    4128      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 15, 80, 80, 32)    0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 15, 80, 80, 32)    128       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_1 (MaxPooling3 (None, 7, 40, 40, 32)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_2 (Conv3D)            (None, 7, 40, 40, 64)     16448     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 7, 40, 40, 64)     0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 7, 40, 40, 64)     256       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_2 (MaxPooling3 (None, 3, 20, 20, 64)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_3 (Conv3D)            (None, 3, 20, 20, 128)    65664     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 3, 20, 20, 128)    0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 3, 20, 20, 128)    512       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_3 (MaxPooling3 (None, 1, 10, 10, 128)    0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 12800)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               1638528   \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 5)                 325       \n",
            "=================================================================\n",
            "Total params: 1,736,389\n",
            "Trainable params: 1,735,525\n",
            "Non-trainable params: 864\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eU6XuXdNVrm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "a2de046b-07f0-49fb-f5a9-a1ee225a60eb"
      },
      "source": [
        "Conv3D1.train_model(Conv3D1_model)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            " 9/23 [==========>...................] - ETA: 1:42 - batch: 4.0000 - size: 30.0000 - loss: 1.7367 - categorical_accuracy: 0.3889"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-35a1b5168f5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mConv3D1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv3D1_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-7036085f51e6>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, model, augment_data)\u001b[0m\n\u001b[1;32m    120\u001b[0m         history=model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=self.num_epochs, verbose=1, \n\u001b[1;32m    121\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m                             validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training_v1.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1257\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m   def evaluate_generator(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training_v1.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    806\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    809\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training_generator_v1.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training_generator_v1.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtarget_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m       \u001b[0mbatch_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mbatch_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training_generator_v1.py\u001b[0m in \u001b[0;36m_get_next_batch\u001b[0;34m(generator)\u001b[0m\n\u001b[1;32m    351\u001b[0m   \u001b[0;34m\"\"\"Retrieves the next batch of input data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m     \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    895\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 897\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaJ-270dCcnY"
      },
      "source": [
        "Conv3D1.clear_session(Conv3D1_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPGUJrJXNVrn"
      },
      "source": [
        "Conv3D1_bs40=ModelConv3D1()\n",
        "Conv3D1_bs40.initialize_src_path(main_folder)\n",
        "Conv3D1_bs40.initialize_image_properties(image_height=160,image_width=160)\n",
        "Conv3D1_bs40.initialize_hyperparams(frames_to_sample=30,batch_size=40,num_epochs=1)\n",
        "Conv3D1_model_bs40=Conv3D1_bs40.define_model()\n",
        "Conv3D1_model_bs40.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHCAyYjiNVro"
      },
      "source": [
        "#### Got below memory exhaust error with image resolution of 160x160, 30 frames and a batch_size of 40\n",
        "ResourceExhaustedError:  OOM when allocating tensor with shape[40,16,15,80,80] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
        "\t [[node gradient_tape/sequential_2/max_pooling3d_8/MaxPool3D/MaxPool3DGrad (defined at <ipython-input-11-c85facc09113>:122) ]]\n",
        "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
        " [Op:__inference_train_function_7489]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7SaHnFVNVro"
      },
      "source": [
        "print(\"Memory util is {} Gigs\". format(getsizeof(np.zeros((40,16,30,160,160)))/(1024*1024*1024)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSsKCHtZNVrp"
      },
      "source": [
        "conv_3d1=ModelConv3D1()\n",
        "conv_3d1.initialize_src_path(main_folder)\n",
        "conv_3d1.initialize_image_properties(image_height=160,image_width=160)\n",
        "conv_3d1.initialize_hyperparams(frames_to_sample=16,batch_size=30,num_epochs=3)\n",
        "conv_3d1_model=conv_3d1.define_model()\n",
        "print(\"Total Params:\", conv_3d1_model.count_params())\n",
        "conv_3d1.train_model(conv_3d1_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jx0kjX0PNVrr"
      },
      "source": [
        "conv_3d1.clear_session(conv_3d1_model)\n",
        "conv_3d1=ModelConv3D1()\n",
        "conv_3d1.initialize_src_path(main_folder)\n",
        "conv_3d1.initialize_image_properties(image_height=100,image_width=100)\n",
        "conv_3d1.initialize_hyperparams(frames_to_sample=30,batch_size=30,num_epochs=2)\n",
        "conv_3d1_model=conv_3d1.define_model()\n",
        "print(\"Total Params:\", conv_3d1_model.count_params())\n",
        "conv_3d1.train_model(conv_3d1_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xReB_wtlNVrx"
      },
      "source": [
        "conv_3d1.clear_session(conv_3d1_model)\n",
        "conv_3d1=ModelConv3D1()\n",
        "conv_3d1.initialize_src_path(main_folder)\n",
        "conv_3d1.initialize_image_properties(image_height=100,image_width=100)\n",
        "conv_3d1.initialize_hyperparams(frames_to_sample=30,batch_size=60,num_epochs=2)\n",
        "conv_3d1_model=conv_3d1.define_model()\n",
        "print(\"Total Params:\", conv_3d1_model.count_params())\n",
        "conv_3d1.train_model(conv_3d1_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoKopL3YNVry"
      },
      "source": [
        "conv_3d1.clear_session(conv_3d1_model)\n",
        "conv_3d1=ModelConv3D1()\n",
        "conv_3d1.initialize_src_path(main_folder)\n",
        "conv_3d1.initialize_image_properties(image_height=100,image_width=100)\n",
        "conv_3d1.initialize_hyperparams(frames_to_sample=16,batch_size=60,num_epochs=2)\n",
        "conv_3d1_model=conv_3d1.define_model()\n",
        "print(\"Total Params:\", conv_3d1_model.count_params())\n",
        "conv_3d1.train_model(conv_3d1_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHABNT4GNVry"
      },
      "source": [
        "conv_3d1.clear_session(conv_3d1_model)\n",
        "conv_3d1=ModelConv3D1()\n",
        "conv_3d1.initialize_src_path(main_folder)\n",
        "conv_3d1.initialize_image_properties(image_height=100,image_width=100)\n",
        "conv_3d1.initialize_hyperparams(frames_to_sample=16,batch_size=80,num_epochs=2)\n",
        "conv_3d1_model=conv_3d1.define_model()\n",
        "print(\"Total Params:\", conv_3d1_model.count_params())\n",
        "conv_3d1.train_model(conv_3d1_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2T9W4HyNVrz"
      },
      "source": [
        "conv_3d1.clear_session(conv_3d1_model)\n",
        "conv_3d1=ModelConv3D1()\n",
        "conv_3d1.initialize_src_path(main_folder)\n",
        "conv_3d1.initialize_image_properties(image_height=160,image_width=160)\n",
        "conv_3d1.initialize_hyperparams(frames_to_sample=30,batch_size=15,num_epochs=2)\n",
        "conv_3d1_model=conv_3d1.define_model()\n",
        "print(\"Total Params:\", conv_3d1_model.count_params())\n",
        "conv_3d1.train_model(conv_3d1_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25WU3xHcNVrz"
      },
      "source": [
        "conv_3d1.clear_session(conv_3d1_model)\n",
        "conv_3d1=ModelConv3D1()\n",
        "conv_3d1.initialize_src_path(main_folder)\n",
        "conv_3d1.initialize_image_properties(image_height=160,image_width=160)\n",
        "conv_3d1.initialize_hyperparams(frames_to_sample=16,batch_size=15,num_epochs=2)\n",
        "conv_3d1_model=conv_3d1.define_model()\n",
        "print(\"Total Params:\", conv_3d1_model.count_params())\n",
        "conv_3d1.train_model(conv_3d1_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPzFJDfUNVr0"
      },
      "source": [
        "conv_3d1.clear_session(conv_3d1_model)\n",
        "conv_3d1=ModelConv3D1()\n",
        "conv_3d1.initialize_src_path(main_folder)\n",
        "conv_3d1.initialize_image_properties(image_height=100,image_width=100)\n",
        "conv_3d1.initialize_hyperparams(frames_to_sample=16,batch_size=15,num_epochs=2)\n",
        "conv_3d1_model=conv_3d1.define_model()\n",
        "print(\"Total Params:\", conv_3d1_model.count_params())\n",
        "conv_3d1.train_model(conv_3d1_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdQ6Aed-NVr4"
      },
      "source": [
        "conv_3d1.clear_session(conv_3d1_model)\n",
        "conv_3d1=ModelConv3D1()\n",
        "conv_3d1.initialize_src_path(main_folder)\n",
        "conv_3d1.initialize_image_properties(image_height=100,image_width=100)\n",
        "conv_3d1.initialize_hyperparams(frames_to_sample=16,batch_size=10,num_epochs=2)\n",
        "conv_3d1_model=conv_3d1.define_model()\n",
        "print(\"Total Params:\", conv_3d1_model.count_params())\n",
        "conv_3d1.train_model(conv_3d1_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OYsvNb7NVr5"
      },
      "source": [
        "conv_3d1.clear_session(conv_3d1_model)\n",
        "conv_3d1=ModelConv3D1()\n",
        "conv_3d1.initialize_src_path(main_folder)\n",
        "conv_3d1.initialize_image_properties(image_height=100,image_width=100)\n",
        "conv_3d1.initialize_hyperparams(frames_to_sample=30,batch_size=10,num_epochs=2)\n",
        "conv_3d1_model=conv_3d1.define_model()\n",
        "print(\"Total Params:\", conv_3d1_model.count_params())\n",
        "conv_3d1.train_model(conv_3d1_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6D4BJx6gNVr6"
      },
      "source": [
        "conv_3d1.clear_session(conv_3d1_model)\n",
        "conv_3d1=ModelConv3D1()\n",
        "conv_3d1.initialize_src_path(main_folder)\n",
        "conv_3d1.initialize_image_properties(image_height=160,image_width=160)\n",
        "conv_3d1.initialize_hyperparams(frames_to_sample=30,batch_size=10,num_epochs=2)\n",
        "conv_3d1_model=conv_3d1.define_model()\n",
        "print(\"Total Params:\", conv_3d1_model.count_params())\n",
        "conv_3d1.train_model(conv_3d1_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oo38TA_HNVr6"
      },
      "source": [
        "conv_3d1.clear_session(conv_3d1_model)\n",
        "conv_3d1=ModelConv3D1()\n",
        "conv_3d1.initialize_src_path(main_folder)\n",
        "conv_3d1.initialize_image_properties(image_height=160,image_width=160)\n",
        "conv_3d1.initialize_hyperparams(frames_to_sample=16,batch_size=10,num_epochs=2)\n",
        "conv_3d1_model=conv_3d1.define_model()\n",
        "print(\"Total Params:\", conv_3d1_model.count_params())\n",
        "conv_3d1.train_model(conv_3d1_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ns4lmR9bNVr7"
      },
      "source": [
        "conv_3d1.clear_session(conv_3d1_model)\n",
        "conv_3d1=ModelConv3D1()\n",
        "conv_3d1.initialize_src_path(main_folder)\n",
        "conv_3d1.initialize_image_properties(image_height=160,image_width=160)\n",
        "conv_3d1.initialize_hyperparams(frames_to_sample=16,batch_size=40,num_epochs=2)\n",
        "conv_3d1_model=conv_3d1.define_model()\n",
        "print(\"Total Params:\", conv_3d1_model.count_params())\n",
        "conv_3d1.train_model(conv_3d1_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVh5rIRCNVr7"
      },
      "source": [
        "### Observation\n",
        "\n",
        "**As we see from the above experiments image resolution and number of frames in sequence have more impact on training time than batch_size.**\n",
        "\n",
        "So experimentations are carried with batch size fixed around 15-40 and changing the resolution and number of image per sequence based on the device memory constraints . Models are designed such that their memory foot print is less than 50 MB which corresponds to 4.3 million parameters assuming the datatype size of parameters to be 12 bytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKbxe-Szy15j"
      },
      "source": [
        "## Model 1 - Base Model - No Data Augmentation Batch Size 40 and Epoch 15"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LVQM4NsNVr7"
      },
      "source": [
        "class ModelConv3D1(ModelBuilder):\n",
        "    \n",
        "    def define_model(self,filtersize=(3,3,3),dense_neurons=64,dropout=0.25):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Conv3D(16, filtersize, padding='same',\n",
        "                 input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(32, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(64, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(128, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "\n",
        "        model.add(Dense(self.num_classes,activation='softmax'))\n",
        "\n",
        "        optimiser = optimizers.Adam()\n",
        "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-89dV5W8WAw3"
      },
      "source": [
        "conv_3d1.clear_session(conv_3d1_model)\n",
        "Conv3D1=ModelConv3D1()\n",
        "Conv3D1.initialize_src_path(main_folder)\n",
        "Conv3D1.initialize_image_properties(image_height=160,image_width=160)\n",
        "Conv3D1.initialize_hyperparams(frames_to_sample=20,batch_size=40,num_epochs=15)\n",
        "Conv3D1_model=Conv3D1.define_model()\n",
        "Conv3D1_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5j884vjoWQD0"
      },
      "source": [
        "print(\"Total Params:\", Conv3D1_model.count_params())\n",
        "accuracy_check_model_1 = Conv3D1.train_model(Conv3D1_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r00TPGbNXXhX"
      },
      "source": [
        "plot(accuracy_check_model_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dtxwayy_14Hn"
      },
      "source": [
        "#### Model is clearly overfitting. So we need to do data augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJk2twS_2lRx"
      },
      "source": [
        "## Model 2 - Augment Data , (3,3,3) filter & 160x160 image resolution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQxHzlc92oOq"
      },
      "source": [
        "Conv3D1.clear_session(Conv3D1_model)\n",
        "Conv3D2=ModelConv3D1()\n",
        "Conv3D2.initialize_src_path(main_folder)\n",
        "Conv3D2.initialize_image_properties(image_height=160,image_width=160)\n",
        "Conv3D2.initialize_hyperparams(frames_to_sample=20,batch_size=20,num_epochs=25)\n",
        "Conv3D2_model=Conv3D2.define_model(dense_neurons=256,dropout=0.5)\n",
        "Conv3D2_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCx8waqu3PY9"
      },
      "source": [
        "print(\"Total Params:\", Conv3D2_model.count_params())\n",
        "accuracy_check_model_2=Conv3D2.train_model(Conv3D2_model,augment_data=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXZ8sThy3XSr"
      },
      "source": [
        "plot(accuracy_check_model_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldsJg52PTDPq"
      },
      "source": [
        "Model is not overfitting and we get a best validation accuracy of ~80% and training accuracy of ~82%. Next we will try to reduce the filter size and image resolution and see if get better results. Moreover since we see minor oscillations in loss, let's try lowering the learning rate to 0.0002"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDO0BmuThfN_"
      },
      "source": [
        "Conv3D2.clear_session(Conv3D2_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qrMlLsVTSA7"
      },
      "source": [
        "## Model 3 - Reduce filter size to (2,2,2) and image res to 120 x 120"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkPpJFg3TRLh"
      },
      "source": [
        "class ModelConv3D3(ModelBuilder):\n",
        "    \n",
        "    def define_model(self,filtersize=(3,3,3),dense_neurons=64,dropout=0.25):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Conv3D(16, filtersize, padding='same',\n",
        "                 input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(32, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(64, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(128, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "\n",
        "        model.add(Dense(self.num_classes,activation='softmax'))\n",
        "\n",
        "        optimiser = optimizers.Adam(lr=0.0002)\n",
        "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lk9L9pt7UBB1"
      },
      "source": [
        "\n",
        "Conv3D3=ModelConv3D3()\n",
        "Conv3D3.initialize_src_path(main_folder)\n",
        "Conv3D3.initialize_image_properties(image_height=120,image_width=120)\n",
        "Conv3D3.initialize_hyperparams(frames_to_sample=16,batch_size=30,num_epochs=30)\n",
        "Conv3D3_model=Conv3D3.define_model(filtersize=(2,2,2),dense_neurons=256,dropout=0.5)\n",
        "Conv3D3_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WW17sf6iUcQh"
      },
      "source": [
        "##tf.compat.v1.enable_eager_execution()\n",
        "print(tf.executing_eagerly())\n",
        "print(\"Total Params:\", Conv3D3_model.count_params())\n",
        "accuracy_check_model_3=Conv3D3.train_model(Conv3D3_model,augment_data=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HghaRZtGQB9"
      },
      "source": [
        "plot(accuracy_check_model_3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0IJgwK77wsF"
      },
      "source": [
        "This Model has a best validation accuracy of 68% and training accuracy of 68% . Also we were able to reduce the parameter size by half the earlier model. Let's trying adding more layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQ8jW39zjSXp"
      },
      "source": [
        "Conv3D3.clear_session(Conv3D3_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WW5VAmu9ihlH"
      },
      "source": [
        "## Model 4 - Adding more layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cyJpLxiitoB"
      },
      "source": [
        "class ModelConv3D4(ModelBuilder):\n",
        "    \n",
        "    def define_model(self,filtersize=(3,3,3),dense_neurons=64,dropout=0.25):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Conv3D(16, filtersize, padding='same',\n",
        "                 input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(Conv3D(16, filtersize, padding='same',\n",
        "                 input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(32, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(Conv3D(32, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(64, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(Conv3D(64, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(128, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(Conv3D(128, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "        \n",
        "\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "\n",
        "        model.add(Dense(self.num_classes,activation='softmax'))\n",
        "\n",
        "        optimiser = optimizers.Adam()\n",
        "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmOxJwmei5-S"
      },
      "source": [
        "Conv3D4=ModelConv3D4()\n",
        "Conv3D4.initialize_src_path(main_folder)\n",
        "Conv3D4.initialize_image_properties(image_height=120,image_width=120)\n",
        "Conv3D4.initialize_hyperparams(frames_to_sample=16,batch_size=20,num_epochs=30)\n",
        "Conv3D4_model=Conv3D4.define_model(filtersize=(3,3,3),dense_neurons=256,dropout=0.5)\n",
        "Conv3D4_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoR4sAUBkGDJ"
      },
      "source": [
        "print(\"Total Params:\", Conv3D4_model.count_params())\n",
        "accuracy_check_model_4=Conv3D4.train_model(Conv3D4_model,augment_data=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8lzWQankJXv"
      },
      "source": [
        "plot(accuracy_check_model_4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Q33XjvbSv8q"
      },
      "source": [
        "With more layers we see some performance improvement. We get a best validation accuracy of 73% . Let's try adding dropouts at the convolution layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PP6UEuorS7-z"
      },
      "source": [
        "## Model 5 Adding dropout at convolution layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjEDLlauS6iK"
      },
      "source": [
        "Conv3D4.clear_session(Conv3D4_model)\n",
        "class ModelConv3D5(ModelBuilder):\n",
        "    \n",
        "    def define_model(self,filtersize=(3,3,3),dense_neurons=64,dropout=0.25):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Conv3D(16, filtersize, padding='same',\n",
        "                 input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(Conv3D(16, filtersize, padding='same',\n",
        "                 input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Conv3D(32, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(Conv3D(32, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Conv3D(64, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(Conv3D(64, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Conv3D(128, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(Conv3D(128, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "        model.add(Dropout(dropout))\n",
        "        \n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "\n",
        "        model.add(Dense(self.num_classes,activation='softmax'))\n",
        "\n",
        "        optimiser = optimizers.Adam()\n",
        "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXzE7zAXTlb5"
      },
      "source": [
        "Conv3D5=ModelConv3D5()\n",
        "Conv3D5.initialize_src_path(main_folder)\n",
        "Conv3D5.initialize_image_properties(image_height=120,image_width=120)\n",
        "Conv3D5.initialize_hyperparams(frames_to_sample=16,batch_size=20,num_epochs=22)\n",
        "Conv3D5_model=Conv3D5.define_model(filtersize=(3,3,3),dense_neurons=256,dropout=0.25)\n",
        "Conv3D5_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JesB78x6TvM3"
      },
      "source": [
        "print(\"Total Params:\", Conv3D5_model.count_params())\n",
        "accuracy_check_model_5=Conv3D5.train_model(Conv3D5_model,augment_data=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSHXypIxT2_c"
      },
      "source": [
        "plot(accuracy_check_model_5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOot9MXnwkBV"
      },
      "source": [
        "\n",
        "Adding dropouts has further reduced validation accuracy as its not to learn generalizable features\n",
        "All models experimental models above have more than 1 million parameters. Let's try to reduce the model size and see the performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCTXMah8xFXr"
      },
      "source": [
        "Conv3D5.clear_session(Conv3D5_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FroiBKKw0un"
      },
      "source": [
        "## Model 6 - reducing the number of parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJj_Nb_IwnLi"
      },
      "source": [
        "class ModelConv3D6(ModelBuilder):\n",
        "    \n",
        "    def define_model(self,dense_neurons=64,dropout=0.25):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Conv3D(16, (3, 3, 3), padding='same',\n",
        "                 input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(32, (2, 2, 2), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(64, (2, 2, 2), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(128, (2, 2, 2), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "        \n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(self.num_classes,activation='softmax'))\n",
        "\n",
        "        optimiser = optimizers.Adam(lr=0.0002)\n",
        "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKI57BD-xhK9"
      },
      "source": [
        "Conv3D6=ModelConv3D6()\n",
        "Conv3D6.initialize_src_path(main_folder)\n",
        "Conv3D6.initialize_image_properties(image_height=100,image_width=100)\n",
        "Conv3D6.initialize_hyperparams(frames_to_sample=16,batch_size=20,num_epochs=30)\n",
        "Conv3D6_model=Conv3D6.define_model(dense_neurons=128,dropout=0.25)\n",
        "Conv3D6_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ydywdxnxwbU"
      },
      "source": [
        "tprint(\"Total Params:\", Conv3D6_model.count_params())\n",
        "accuracy_check_model_6=Conv3D6.train_model(Conv3D6_model,augment_data=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7WQvNWYDybQ"
      },
      "source": [
        "plot(accuracy_check_model_6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBJFD6jaJsTV"
      },
      "source": [
        "For the above low memory foot print model the best validation accuracy of 73%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cArLoK-0RXxf"
      },
      "source": [
        "Conv3D6.clear_session(Conv3D6_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHU3qisEKMLX"
      },
      "source": [
        "## Model 7 - reducing the number of parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huD4YXHfJ9E7"
      },
      "source": [
        "class ModelConv3D7(ModelBuilder):\n",
        "    \n",
        "    def define_model(self,dense_neurons=64,dropout=0.25):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Conv3D(16, (3, 3, 3), padding='same',\n",
        "                 input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(32, (3, 3, 3), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(64, (2, 2, 2), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(128, (2, 2, 2), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "        \n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(self.num_classes,activation='softmax'))\n",
        "\n",
        "        optimiser = optimizers.Adam(lr=0.0002)\n",
        "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VR3gHG0T256h"
      },
      "source": [
        "conv_3d7=ModelConv3D7()\n",
        "conv_3d7.initialize_src_path(main_folder)\n",
        "conv_3d7.initialize_image_properties(image_height=120,image_width=120)\n",
        "conv_3d7.initialize_hyperparams(frames_to_sample=16,batch_size=20,num_epochs=25)\n",
        "conv_3d7_model=conv_3d7.define_model(dense_neurons=64,dropout=0.25)\n",
        "conv_3d7_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s63kvjqd3UKl"
      },
      "source": [
        "print(\"Total Params:\", conv_3d7_model.count_params())\n",
        "accuracy_check_model_7=conv_3d7.train_model(conv_3d7_model,augment_data=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBJGgwaa3kOR"
      },
      "source": [
        "plot(accuracy_check_model_7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bBbPYK-RgAh"
      },
      "source": [
        "conv_3d7.clear_session(conv_3d7_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OW55j6S4f5y"
      },
      "source": [
        "## Model 8 - reducing the number of parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtU-6-0v4e12"
      },
      "source": [
        "class ModelConv3D8(ModelBuilder):\n",
        "    \n",
        "    def define_model(self,dense_neurons=64,dropout=0.25):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Conv3D(8, (3, 3, 3), padding='same',\n",
        "        input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(16, (3, 3, 3), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(32, (2, 2, 2), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(64, (2, 2, 2), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "        \n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(self.num_classes,activation='softmax'))\n",
        "\n",
        "        optimiser = optimizers.Adam(lr=0.0002)\n",
        "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BC1Vxikp44zW"
      },
      "source": [
        "conv_3d8=ModelConv3D8()\n",
        "conv_3d8.initialize_src_path(main_folder)\n",
        "conv_3d8.initialize_image_properties(image_height=120,image_width=120)\n",
        "conv_3d8.initialize_hyperparams(frames_to_sample=16,batch_size=20,num_epochs=30)\n",
        "conv_3d8_model=conv_3d8.define_model(dense_neurons=64,dropout=0.25)\n",
        "conv_3d8_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oikZb8Ij5DNw"
      },
      "source": [
        "print(\"Total Params:\", conv_3d8_model.count_params())\n",
        "accuracy_check_model_8=conv_3d8.train_model(conv_3d8_model,augment_data=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ds6Ddq2y5SsD"
      },
      "source": [
        "plot(accuracy_check_model_8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq14-oPeXfA9"
      },
      "source": [
        "conv_3d8.clear_session(conv_3d8_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGRwZahyW2aW"
      },
      "source": [
        "## Model 9 - CNN- LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuOHUaOhW0gJ"
      },
      "source": [
        "class CNNRNN1(ModelBuilder):\n",
        "    \n",
        "    def define_model(self,lstm_cells=64,dense_neurons=64,dropout=0.25):\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(TimeDistributed(Conv2D(16, (3, 3) , padding='same', activation='relu'),\n",
        "                                  input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
        "        model.add(TimeDistributed(BatchNormalization()))\n",
        "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "        \n",
        "        model.add(TimeDistributed(Conv2D(32, (3, 3) , padding='same', activation='relu')))\n",
        "        model.add(TimeDistributed(BatchNormalization()))\n",
        "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "        \n",
        "        model.add(TimeDistributed(Conv2D(64, (3, 3) , padding='same', activation='relu')))\n",
        "        model.add(TimeDistributed(BatchNormalization()))\n",
        "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "        \n",
        "        model.add(TimeDistributed(Conv2D(128, (3, 3) , padding='same', activation='relu')))\n",
        "        model.add(TimeDistributed(BatchNormalization()))\n",
        "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "        \n",
        "        model.add(TimeDistributed(Conv2D(256, (3, 3) , padding='same', activation='relu')))\n",
        "        model.add(TimeDistributed(BatchNormalization()))\n",
        "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "        \n",
        "        model.add(TimeDistributed(Flatten()))\n",
        "\n",
        "\n",
        "        model.add(LSTM(lstm_cells))\n",
        "        model.add(Dropout(dropout))\n",
        "        \n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(Dropout(dropout))\n",
        "        \n",
        "        model.add(Dense(self.num_classes, activation='softmax'))\n",
        "        optimiser = optimizers.Adam()\n",
        "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCoDy3FXXJj4"
      },
      "source": [
        "cnn_rnn1=CNNRNN1()\n",
        "cnn_rnn1.initialize_src_path(main_folder)\n",
        "cnn_rnn1.initialize_image_properties(image_height=120,image_width=120)\n",
        "cnn_rnn1.initialize_hyperparams(frames_to_sample=18,batch_size=20,num_epochs=20)\n",
        "cnn_rnn1_model=cnn_rnn1.define_model(lstm_cells=128,dense_neurons=128,dropout=0.25)\n",
        "cnn_rnn1_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-t4EsndXVii"
      },
      "source": [
        "print(\"Total Params:\", cnn_rnn1_model.count_params())\n",
        "accuracy_check_model_9=cnn_rnn1.train_model(cnn_rnn1_model,augment_data=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gm-imdTuXwbq"
      },
      "source": [
        "plot(accuracy_check_model_9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZSjWET3rag4"
      },
      "source": [
        "cnn_rnn1.clear_session(cnn_rnn1_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gdy0_yQzOn7y"
      },
      "source": [
        "## Applying More Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onOKdsVRO26u"
      },
      "source": [
        "class ModelBuilderMoreAugmentation(metaclass= abc.ABCMeta):\n",
        "    \n",
        "    def initialize_src_path(self,main_folder):\n",
        "        self.train_doc = np.random.permutation(open(main_folder + '/' + 'train.csv').readlines())\n",
        "        self.val_doc = np.random.permutation(open(main_folder + '/' + 'val.csv').readlines())\n",
        "        self.train_path = main_folder + '/' + 'train'\n",
        "        self.val_path =  main_folder + '/' + 'val'\n",
        "        self.num_train_sequences = len(self.train_doc)\n",
        "        self.num_val_sequences = len(self.val_doc)\n",
        "        \n",
        "    def initialize_image_properties(self,image_height=100,image_width=100):\n",
        "        self.image_height=image_height\n",
        "        self.image_width=image_width\n",
        "        self.channels=3\n",
        "        self.num_classes=5\n",
        "        self.total_frames=30\n",
        "          \n",
        "    def initialize_hyperparams(self,frames_to_sample=30,batch_size=20,num_epochs=20):\n",
        "        self.frames_to_sample=frames_to_sample\n",
        "        self.batch_size=batch_size\n",
        "        self.num_epochs=num_epochs\n",
        "        \n",
        "        \n",
        "    def generator(self,source_path, folder_list, augment=False):\n",
        "        img_idx = np.round(np.linspace(0,self.total_frames-1,self.frames_to_sample)).astype(int)\n",
        "        batch_size=self.batch_size\n",
        "        while True:\n",
        "            t = np.random.permutation(folder_list)\n",
        "            num_batches = len(t)//batch_size\n",
        "        \n",
        "            for batch in range(num_batches): \n",
        "                batch_data, batch_labels= self.one_batch_data(source_path,t,batch,batch_size,img_idx,augment)\n",
        "                yield batch_data, batch_labels \n",
        "\n",
        "            remaining_seq=len(t)%batch_size\n",
        "        \n",
        "            if (remaining_seq != 0):\n",
        "                batch_data, batch_labels= self.one_batch_data(source_path,t,num_batches,batch_size,img_idx,augment,remaining_seq)\n",
        "                yield batch_data, batch_labels \n",
        "    \n",
        "    \n",
        "    def one_batch_data(self,source_path,t,batch,batch_size,img_idx,augment,remaining_seq=0):\n",
        "    \n",
        "        seq_len = remaining_seq if remaining_seq else batch_size\n",
        "    \n",
        "        batch_data = np.zeros((seq_len,len(img_idx),self.image_height,self.image_width,self.channels)) \n",
        "        batch_labels = np.zeros((seq_len,self.num_classes)) \n",
        "    \n",
        "        if (augment): batch_data_aug = np.zeros((seq_len,len(img_idx),self.image_height,self.image_width,self.channels))\n",
        "\n",
        "        \n",
        "        for folder in range(seq_len): \n",
        "            imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) \n",
        "            for idx,item in enumerate(img_idx): \n",
        "                image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
        "                image_resized=transform.resize(image,(self.image_height,self.image_width,3))\n",
        "            \n",
        "\n",
        "                batch_data[folder,idx,:,:,0] = (image_resized[:,:,0])/255\n",
        "                batch_data[folder,idx,:,:,1] = (image_resized[:,:,1])/255\n",
        "                batch_data[folder,idx,:,:,2] = (image_resized[:,:,2])/255\n",
        "            \n",
        "                if (augment):\n",
        "                    shifted = cv2.warpAffine(image, \n",
        "                                             np.float32([[1, 0, np.random.randint(-30,30)],[0, 1, np.random.randint(-30,30)]]), \n",
        "                                            (image.shape[1], image.shape[0]))\n",
        "                    \n",
        "                    gray = cv2.cvtColor(shifted,cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "                    x0, y0 = np.argwhere(gray > 0).min(axis=0)\n",
        "                    x1, y1 = np.argwhere(gray > 0).max(axis=0) \n",
        "                    \n",
        "                    cropped=shifted[x0:x1,y0:y1,:]\n",
        "                    \n",
        "                    image_resized=transform.resize(cropped,(self.image_height,self.image_width,3))\n",
        "                    \n",
        "                    M = cv2.getRotationMatrix2D((self.image_width//2,self.image_height//2),\n",
        "                                                np.random.randint(-10,10), 1.0)\n",
        "                    rotated = cv2.warpAffine(image_resized, M, (self.image_width, self.image_height))\n",
        "                  \n",
        "                    batch_data_aug[folder,idx,:,:,0] = (rotated[:,:,0])/255\n",
        "                    batch_data_aug[folder,idx,:,:,1] = (rotated[:,:,1])/255\n",
        "                    batch_data_aug[folder,idx,:,:,2] = (rotated[:,:,2])/255\n",
        "                \n",
        "            \n",
        "            batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
        "            \n",
        "    \n",
        "        if (augment):\n",
        "            batch_data=np.concatenate([batch_data,batch_data_aug])\n",
        "            batch_labels=np.concatenate([batch_labels,batch_labels])\n",
        "\n",
        "        \n",
        "        return(batch_data,batch_labels)\n",
        "    \n",
        "    \n",
        "    def train_model(self, model, augment_data=False):\n",
        "        train_generator = self.generator(self.train_path, self.train_doc,augment=augment_data)\n",
        "        val_generator = self.generator(self.val_path, self.val_doc)\n",
        "\n",
        "        model_name = 'model_init' + '_' + str(datetime.datetime.now()).replace(' ','').replace(':','_') + '/'\n",
        "    \n",
        "        if not os.path.exists(model_name):\n",
        "            os.mkdir(model_name)\n",
        "        \n",
        "        filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
        "\n",
        "        checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
        "        LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4)\n",
        "        callbacks_list = [checkpoint, LR]\n",
        "\n",
        "        if (self.num_train_sequences%self.batch_size) == 0:\n",
        "            steps_per_epoch = int(self.num_train_sequences/self.batch_size)\n",
        "        else:\n",
        "            steps_per_epoch = (self.num_train_sequences//self.batch_size) + 1\n",
        "\n",
        "        if (self.num_val_sequences%self.batch_size) == 0:\n",
        "            validation_steps = int(self.num_val_sequences/self.batch_size)\n",
        "        else:\n",
        "            validation_steps = (self.num_val_sequences//self.batch_size) + 1\n",
        "    \n",
        "        history=model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=self.num_epochs, verbose=1, \n",
        "                            callbacks=callbacks_list, validation_data=val_generator, \n",
        "                            validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n",
        "        return history\n",
        "\n",
        "        \n",
        "    @abc.abstractmethod\n",
        "    def define_model(self):\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNCI5V1JPp5N"
      },
      "source": [
        "## Model 10 - (3,3,3) Filter & 160x160 Image resolution - similar to Model 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHkhEeqoPoZ-"
      },
      "source": [
        "class ModelConv3D10(ModelBuilderMoreAugmentation):\n",
        "    \n",
        "    def define_model(self,filtersize=(3,3,3),dense_neurons=64,dropout=0.25):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Conv3D(16, filtersize, padding='same',\n",
        "                 input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(32, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(64, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(128, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "\n",
        "        model.add(Dense(self.num_classes,activation='softmax'))\n",
        "\n",
        "        optimiser = optimizers.Adam(lr=0.0002)\n",
        "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zxshqMLQNAK"
      },
      "source": [
        "conv_3d10=ModelConv3D10()\n",
        "conv_3d10.initialize_src_path(main_folder)\n",
        "conv_3d10.initialize_image_properties(image_height=160,image_width=160)\n",
        "conv_3d10.initialize_hyperparams(frames_to_sample=20,batch_size=20,num_epochs=30)\n",
        "conv_3d10_model=conv_3d10.define_model(dense_neurons=256,dropout=0.5)\n",
        "conv_3d10_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Qjf4MvRQSSG"
      },
      "source": [
        "print(\"Total Params:\", conv_3d10_model.count_params())\n",
        "accuracy_check_model_10=conv_3d10.train_model(conv_3d10_model,augment_data=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOdRzLqFpbxp"
      },
      "source": [
        "plot(accuracy_check_model_10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGr80KSVriGs"
      },
      "source": [
        "conv_3d10.clear_session(conv_3d10_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1Nyst3JoisB"
      },
      "source": [
        "## Model 11 - (2,2,2) Filter & 120x120 Image resolution - similar to Model 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HRBTvETolll"
      },
      "source": [
        "class ModelConv3D11(ModelBuilderMoreAugmentation):\n",
        "    \n",
        "    def define_model(self,filtersize=(3,3,3),dense_neurons=64,dropout=0.25):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Conv3D(16, filtersize, padding='same',\n",
        "                 input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(32, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(64, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(128, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "\n",
        "        model.add(Dense(self.num_classes,activation='softmax'))\n",
        "\n",
        "        optimiser = optimizers.Adam(lr=0.0002)\n",
        "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WWwgLRypCj3"
      },
      "source": [
        "conv_3d11=ModelConv3D11()\n",
        "conv_3d11.initialize_src_path(main_folder)\n",
        "conv_3d11.initialize_image_properties(image_height=160,image_width=160)\n",
        "conv_3d11.initialize_hyperparams(frames_to_sample=20,batch_size=20,num_epochs=30)\n",
        "conv_3d11_model=conv_3d11.define_model(dense_neurons=256,dropout=0.5)\n",
        "conv_3d11_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUG3v2hOpJYd"
      },
      "source": [
        "print(\"Total Params:\", conv_3d11_model.count_params())\n",
        "accuracy_check_model_11=conv_3d11.train_model(conv_3d11_model,augment_data=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlA0dEQ8puAY"
      },
      "source": [
        "plot(accuracy_check_model_11)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWgA-0hcrp5V"
      },
      "source": [
        "conv_3d11.clear_session(conv_3d11_model)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}