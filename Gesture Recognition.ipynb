{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Gesture Recognition.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/subhabrata-ghosh-1988/Gesture-Recognition/blob/main/Gesture%20Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7ZfgtjxNVrR"
      },
      "source": [
        "# Gesture Recognition\n",
        "In this group project, you are going to build a 3D Conv model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-eH5pKuNVrV"
      },
      "source": [
        "## Import libraries\n",
        "import numpy as np\n",
        "import os\n",
        "from scipy import misc\n",
        "from imageio import imread\n",
        "import cv2\n",
        "from skimage import transform,io\n",
        "import datetime\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import abc\n",
        "from sys import getsizeof\n",
        "import shutil\n",
        "import abc\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J94bqUhVNVrW"
      },
      "source": [
        "import glob, os , shutil\n",
        "for f in glob.glob(\"/content/model_init*\"):\n",
        "    shutil.rmtree(f)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0Ubj-HfSRY3"
      },
      "source": [
        "## remove the existing zip file\n",
        "shutil.rmtree('/content/Project_data.zip', ignore_errors=True)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4ty5a19SiNl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "outputId": "35bc7460-998e-49dd-f227-863fe1dc940a"
      },
      "source": [
        "## Initiate the file download\n",
        "!pip install gdown\n",
        "import gdown\n",
        "url=\"https://drive.google.com/uc?id=1kM4V7pnLjGbuCaDpfBNHig99gr2rdqyJ\"\n",
        "output = \"Project_data.zip\"\n",
        "gdown.download(url, output, quiet=False)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (3.6.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.41.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2.10)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1kM4V7pnLjGbuCaDpfBNHig99gr2rdqyJ\n",
            "To: /content/Project_data.zip\n",
            "1.71GB [00:11, 143MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Project_data.zip'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-fwe5jbTX8r"
      },
      "source": [
        "## unzip the downloaded folder\n",
        "shutil.rmtree('/content/Project_data', ignore_errors=True)\n",
        "shutil.unpack_archive(\"Project_data.zip\", \"Project_data\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIQHPGi_NVrX"
      },
      "source": [
        "We set the random seed so that the results don't vary drastically."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ci_N-NnWNVrZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a2cc83b-0725-4a91-c2ea-5f4f2e05cffc"
      },
      "source": [
        "np.random.seed(30)\n",
        "import random as rn\n",
        "rn.seed(30)\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(30)\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "print(gpus)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bWyzW_oNVrZ"
      },
      "source": [
        "**In this block, you read the folder names for training and validation. You also set the batch_size here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyTupvi3NVrb"
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation\n",
        "from keras.layers.convolutional import Conv3D, MaxPooling3D, Conv2D, MaxPooling2D\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras import optimizers\n",
        "from keras.layers import Dropout"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCjeEg_tNVrc"
      },
      "source": [
        "# the entire dataset is placed in below directory\n",
        "main_folder='/content/Project_data/Project_data'"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSUMkqaSXcC4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "73b33ac5-326d-4c92-ef4f-8991185b4af9"
      },
      "source": [
        "## Checking current TF version\n",
        "tf.version.VERSION"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REgxbVunNVrd"
      },
      "source": [
        "### Plotting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3UJmY03NVre"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot(Model):\n",
        "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,4))\n",
        "    axes[0].plot(Model.history['loss'])   \n",
        "    axes[0].plot(Model.history['val_loss'])\n",
        "    axes[0].legend(['loss','val_loss'])\n",
        "\n",
        "    axes[1].plot(Model.history['categorical_accuracy'])   \n",
        "    axes[1].plot(Model.history['val_categorical_accuracy'])\n",
        "    axes[1].legend(['categorical_accuracy','val_categorical_accuracy'])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtV6apQQNVrf"
      },
      "source": [
        "## Generator\n",
        "This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haSnpGsboLKX"
      },
      "source": [
        "import gc\n",
        "class GCCallback(tf.keras.callbacks.Callback):\n",
        "      def on_epoch_end(self, epoch, logs=None):\n",
        "          print(gc.collect())"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YC-BRedNVri"
      },
      "source": [
        "# referred ABC library use case from this link https://riptutorial.com/python/example/23083/why-how-to-use-abcmeta-and--abstractmethod\n",
        "\n",
        "class ModelBuilder(metaclass= abc.ABCMeta):\n",
        "    \n",
        "    def initialize_src_path(self,main_folder):\n",
        "        self.train_doc = np.random.permutation(open(main_folder + '/' + 'train.csv').readlines())\n",
        "        self.val_doc = np.random.permutation(open(main_folder + '/' + 'val.csv').readlines())\n",
        "        self.train_path = main_folder + '/' + 'train'\n",
        "        self.val_path =  main_folder + '/' + 'val'\n",
        "        self.num_train_sequences = len(self.train_doc)\n",
        "        self.num_val_sequences = len(self.val_doc)\n",
        "        \n",
        "    def initialize_image_properties(self,image_height=100,image_width=100):\n",
        "        self.image_height=image_height\n",
        "        self.image_width=image_width\n",
        "        self.channels=3\n",
        "        self.num_classes=5\n",
        "        self.total_frames=30\n",
        "          \n",
        "    def initialize_hyperparams(self,frames_to_sample=30,batch_size=40,num_epochs=20):\n",
        "        self.frames_to_sample=frames_to_sample\n",
        "        self.batch_size=batch_size\n",
        "        self.num_epochs=num_epochs\n",
        "        \n",
        "        \n",
        "    def generator(self,source_path, folder_list, augment=False):\n",
        "        img_idx = np.round(np.linspace(0,self.total_frames-1,self.frames_to_sample)).astype(int)\n",
        "        batch_size=self.batch_size\n",
        "        while True:\n",
        "            t = np.random.permutation(folder_list)\n",
        "            num_batches = len(t)//batch_size\n",
        "        \n",
        "            for batch in range(num_batches): \n",
        "                batch_data, batch_labels= self.one_batch_data(source_path,t,batch,batch_size,img_idx,augment)\n",
        "                yield batch_data, batch_labels \n",
        "\n",
        "            remaining_seq=len(t)%batch_size\n",
        "        \n",
        "            if (remaining_seq != 0):\n",
        "                batch_data, batch_labels= self.one_batch_data(source_path,t,num_batches,batch_size,img_idx,augment,remaining_seq)\n",
        "                yield batch_data, batch_labels \n",
        "    \n",
        "    \n",
        "    def one_batch_data(self,source_path,t,batch,batch_size,img_idx,augment,remaining_seq=0):\n",
        "    \n",
        "        seq_len = remaining_seq if remaining_seq else batch_size\n",
        "    \n",
        "        batch_data = np.zeros((seq_len,len(img_idx),self.image_height,self.image_width,self.channels)) \n",
        "        batch_labels = np.zeros((seq_len,self.num_classes)) \n",
        "    \n",
        "        if (augment): batch_data_aug = np.zeros((seq_len,len(img_idx),self.image_height,self.image_width,self.channels))\n",
        "\n",
        "        \n",
        "        for folder in range(seq_len): \n",
        "            imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) \n",
        "            for idx,item in enumerate(img_idx): \n",
        "                image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
        "                image_resized=transform.resize(image,(self.image_height,self.image_width,3))\n",
        "            \n",
        "\n",
        "                batch_data[folder,idx,:,:,0] = (image_resized[:,:,0])/255\n",
        "                batch_data[folder,idx,:,:,1] = (image_resized[:,:,1])/255\n",
        "                batch_data[folder,idx,:,:,2] = (image_resized[:,:,2])/255\n",
        "            \n",
        "                if (augment):\n",
        "                    shifted = cv2.warpAffine(image, \n",
        "                                             np.float32([[1, 0, np.random.randint(-30,30)],[0, 1, np.random.randint(-30,30)]]), \n",
        "                                            (image.shape[1], image.shape[0]))\n",
        "                    \n",
        "                    gray = cv2.cvtColor(shifted,cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "                    x0, y0 = np.argwhere(gray > 0).min(axis=0)\n",
        "                    x1, y1 = np.argwhere(gray > 0).max(axis=0) \n",
        "                    \n",
        "                    cropped=shifted[x0:x1,y0:y1,:]\n",
        "                    \n",
        "                    image_resized=transform.resize(cropped,(self.image_height,self.image_width,3))\n",
        "            \n",
        "                    batch_data_aug[folder,idx,:,:,0] = (image_resized[:,:,0])/255\n",
        "                    batch_data_aug[folder,idx,:,:,1] = (image_resized[:,:,1])/255\n",
        "                    batch_data_aug[folder,idx,:,:,2] = (image_resized[:,:,2])/255\n",
        "                \n",
        "            \n",
        "            batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
        "            \n",
        "    \n",
        "        if (augment):\n",
        "            batch_data=np.concatenate([batch_data,batch_data_aug])\n",
        "            batch_labels=np.concatenate([batch_labels,batch_labels])\n",
        "\n",
        "        \n",
        "        return(batch_data,batch_labels)\n",
        "    \n",
        "    \n",
        "    def train_model(self, model, augment_data=False):\n",
        "        train_generator = self.generator(self.train_path, self.train_doc,augment=augment_data)\n",
        "        val_generator = self.generator(self.val_path, self.val_doc)\n",
        "\n",
        "        model_name = 'model_init' + '_' + str(datetime.datetime.now()).replace(' ','').replace(':','_') + '/'\n",
        "    \n",
        "        if not os.path.exists(model_name):\n",
        "            os.mkdir(model_name)\n",
        "        \n",
        "        filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
        "\n",
        "        checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto',save_freq='epoch')\n",
        "        LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4)\n",
        "        callbacks_list = [checkpoint, LR , GCCallback()]\n",
        "\n",
        "        if (self.num_train_sequences%self.batch_size) == 0:\n",
        "            steps_per_epoch = int(self.num_train_sequences/self.batch_size)\n",
        "        else:\n",
        "            steps_per_epoch = (self.num_train_sequences//self.batch_size) + 1\n",
        "\n",
        "        if (self.num_val_sequences%self.batch_size) == 0:\n",
        "            validation_steps = int(self.num_val_sequences/self.batch_size)\n",
        "        else:\n",
        "            validation_steps = (self.num_val_sequences//self.batch_size) + 1\n",
        "    \n",
        "        history=model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=self.num_epochs, verbose=1, \n",
        "                            callbacks=callbacks_list, validation_data=val_generator, \n",
        "                            validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n",
        "        return history\n",
        "\n",
        "    def clear_session(self, model):\n",
        "        del model\n",
        "        gc.collect()\n",
        "        tf.keras.backend.clear_session()\n",
        "        tf.compat.v1.reset_default_graph() # TF graph isn't same as Keras graph\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def define_model(self):\n",
        "        pass\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkdCou-eNVri"
      },
      "source": [
        "## Sample Model\n",
        "Here you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1XR94jpNVrk"
      },
      "source": [
        "class ModelConv3D1(ModelBuilder):\n",
        "    \n",
        "    def define_model(self):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Conv3D(16, (3, 3, 3), padding='same',\n",
        "        input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(32, (2, 2, 2), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(64, (2, 2, 2), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(128, (2, 2, 2), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(128,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.5))\n",
        "\n",
        "        model.add(Dense(64,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.25))\n",
        "\n",
        "\n",
        "        model.add(Dense(self.num_classes,activation='softmax'))\n",
        "\n",
        "        optimiser = optimizers.Adam()\n",
        "        #optimiser = 'sgd'\n",
        "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "        return model"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PIGA0cbNVrl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adf53a68-0895-4b39-e067-9cc7065004db"
      },
      "source": [
        "Conv3D1=ModelConv3D1()\n",
        "Conv3D1.initialize_src_path(main_folder)\n",
        "Conv3D1.initialize_image_properties(image_height=160,image_width=160)\n",
        "Conv3D1.initialize_hyperparams(frames_to_sample=30,batch_size=30,num_epochs=1)\n",
        "Conv3D1_model=Conv3D1.define_model()\n",
        "Conv3D1_model.summary()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv3d (Conv3D)              (None, 30, 160, 160, 16)  1312      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 30, 160, 160, 16)  0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 30, 160, 160, 16)  64        \n",
            "_________________________________________________________________\n",
            "max_pooling3d (MaxPooling3D) (None, 15, 80, 80, 16)    0         \n",
            "_________________________________________________________________\n",
            "conv3d_1 (Conv3D)            (None, 15, 80, 80, 32)    4128      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 15, 80, 80, 32)    0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 15, 80, 80, 32)    128       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_1 (MaxPooling3 (None, 7, 40, 40, 32)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_2 (Conv3D)            (None, 7, 40, 40, 64)     16448     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 7, 40, 40, 64)     0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 7, 40, 40, 64)     256       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_2 (MaxPooling3 (None, 3, 20, 20, 64)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_3 (Conv3D)            (None, 3, 20, 20, 128)    65664     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 3, 20, 20, 128)    0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 3, 20, 20, 128)    512       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_3 (MaxPooling3 (None, 1, 10, 10, 128)    0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 12800)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               1638528   \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 5)                 325       \n",
            "=================================================================\n",
            "Total params: 1,736,389\n",
            "Trainable params: 1,735,525\n",
            "Non-trainable params: 864\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eU6XuXdNVrm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "958fb8ef-f225-4902-a27d-a3c58ff8b50d"
      },
      "source": [
        "Conv3D1.train_model(Conv3D1_model)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "23/23 [==============================] - ETA: 0s - batch: 11.0000 - size: 28.8261 - loss: 1.4912 - categorical_accuracy: 0.4284\n",
            "Epoch 00001: saving model to model_init_2021-03-2108_26_18.607216/model-00001-1.49124-0.42836-4.86878-0.21000.h5\n",
            "0\n",
            "23/23 [==============================] - 274s 11s/step - batch: 11.0000 - size: 28.8261 - loss: 1.4912 - categorical_accuracy: 0.4284 - val_loss: 4.8688 - val_categorical_accuracy: 0.2100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc14f727c90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaJ-270dCcnY"
      },
      "source": [
        "Conv3D1.clear_session(Conv3D1_model)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPGUJrJXNVrn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72cd7eff-5f18-4b57-ae56-32d341d67867"
      },
      "source": [
        "Conv3D1_bs40=ModelConv3D1()\n",
        "Conv3D1_bs40.initialize_src_path(main_folder)\n",
        "Conv3D1_bs40.initialize_image_properties(image_height=160,image_width=160)\n",
        "Conv3D1_bs40.initialize_hyperparams(frames_to_sample=30,batch_size=40,num_epochs=1)\n",
        "Conv3D1_model_bs40=Conv3D1_bs40.define_model()\n",
        "Conv3D1_model_bs40.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv3d (Conv3D)              (None, 30, 160, 160, 16)  1312      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 30, 160, 160, 16)  0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 30, 160, 160, 16)  64        \n",
            "_________________________________________________________________\n",
            "max_pooling3d (MaxPooling3D) (None, 15, 80, 80, 16)    0         \n",
            "_________________________________________________________________\n",
            "conv3d_1 (Conv3D)            (None, 15, 80, 80, 32)    4128      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 15, 80, 80, 32)    0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 15, 80, 80, 32)    128       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_1 (MaxPooling3 (None, 7, 40, 40, 32)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_2 (Conv3D)            (None, 7, 40, 40, 64)     16448     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 7, 40, 40, 64)     0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 7, 40, 40, 64)     256       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_2 (MaxPooling3 (None, 3, 20, 20, 64)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_3 (Conv3D)            (None, 3, 20, 20, 128)    65664     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 3, 20, 20, 128)    0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 3, 20, 20, 128)    512       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_3 (MaxPooling3 (None, 1, 10, 10, 128)    0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 12800)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               1638528   \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 5)                 325       \n",
            "=================================================================\n",
            "Total params: 1,736,389\n",
            "Trainable params: 1,735,525\n",
            "Non-trainable params: 864\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHCAyYjiNVro"
      },
      "source": [
        "#### Got below memory exhaust error with image resolution of 160x160, 30 frames and a batch_size of 40\n",
        "ResourceExhaustedError:  OOM when allocating tensor with shape[40,16,15,80,80] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
        "\t [[node gradient_tape/sequential_2/max_pooling3d_8/MaxPool3D/MaxPool3DGrad (defined at <ipython-input-11-c85facc09113>:122) ]]\n",
        "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
        " [Op:__inference_train_function_7489]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7SaHnFVNVro",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26b08edd-b93c-4881-8b91-a40372e13eab"
      },
      "source": [
        "print(\"Memory util is {} Gigs\". format(getsizeof(np.zeros((40,16,30,160,160)))/(1024*1024*1024)))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Memory util is 3.662109524011612 Gigs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSsKCHtZNVrp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f99bbc0-5d41-4f28-d232-05b8b57cf771"
      },
      "source": [
        "conv_3d1=ModelConv3D1()\n",
        "conv_3d1.initialize_src_path(main_folder)\n",
        "conv_3d1.initialize_image_properties(image_height=160,image_width=160)\n",
        "conv_3d1.initialize_hyperparams(frames_to_sample=16,batch_size=30,num_epochs=3)\n",
        "conv_3d1_model=conv_3d1.define_model()\n",
        "print(\"Total Params:\", conv_3d1_model.count_params())\n",
        "conv_3d1.train_model(conv_3d1_model)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Params: 1736389\n",
            "Epoch 1/3\n",
            "23/23 [==============================] - ETA: 0s - batch: 11.0000 - size: 28.8261 - loss: 1.4302 - categorical_accuracy: 0.4540\n",
            "Epoch 00001: saving model to model_init_2021-03-2108_31_06.879816/model-00001-1.43021-0.45400-2.03139-0.21000.h5\n",
            "0\n",
            "23/23 [==============================] - 161s 7s/step - batch: 11.0000 - size: 28.8261 - loss: 1.4302 - categorical_accuracy: 0.4540 - val_loss: 2.0314 - val_categorical_accuracy: 0.2100\n",
            "Epoch 2/3\n",
            "23/23 [==============================] - ETA: 0s - batch: 11.0000 - size: 28.8261 - loss: 0.9001 - categorical_accuracy: 0.6621\n",
            "Epoch 00002: saving model to model_init_2021-03-2108_31_06.879816/model-00002-0.90008-0.66214-4.21787-0.17000.h5\n",
            "0\n",
            "23/23 [==============================] - 121s 5s/step - batch: 11.0000 - size: 28.8261 - loss: 0.9001 - categorical_accuracy: 0.6621 - val_loss: 4.2179 - val_categorical_accuracy: 0.1700\n",
            "Epoch 3/3\n",
            "23/23 [==============================] - ETA: 0s - batch: 11.0000 - size: 28.8261 - loss: 0.7425 - categorical_accuracy: 0.7044\n",
            "Epoch 00003: saving model to model_init_2021-03-2108_31_06.879816/model-00003-0.74247-0.70437-5.32029-0.24000.h5\n",
            "0\n",
            "23/23 [==============================] - 123s 6s/step - batch: 11.0000 - size: 28.8261 - loss: 0.7425 - categorical_accuracy: 0.7044 - val_loss: 5.3203 - val_categorical_accuracy: 0.2400\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc0c5322750>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jx0kjX0PNVrr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ffe5561-5263-44bb-d898-1eb260a7ae66"
      },
      "source": [
        "conv_3d1.clear_session(conv_3d1_model)\n",
        "conv_3d1=ModelConv3D1()\n",
        "conv_3d1.initialize_src_path(main_folder)\n",
        "conv_3d1.initialize_image_properties(image_height=100,image_width=100)\n",
        "conv_3d1.initialize_hyperparams(frames_to_sample=30,batch_size=30,num_epochs=2)\n",
        "conv_3d1_model=conv_3d1.define_model()\n",
        "print(\"Total Params:\", conv_3d1_model.count_params())\n",
        "conv_3d1.train_model(conv_3d1_model)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Params: 687813\n",
            "Epoch 1/2\n",
            "23/23 [==============================] - ETA: 0s - batch: 11.0000 - size: 28.8261 - loss: 1.6461 - categorical_accuracy: 0.3771\n",
            "Epoch 00001: saving model to model_init_2021-03-2108_37_54.227943/model-00001-1.64606-0.37707-1.90597-0.18000.h5\n",
            "48\n",
            "23/23 [==============================] - 230s 10s/step - batch: 11.0000 - size: 28.8261 - loss: 1.6461 - categorical_accuracy: 0.3771 - val_loss: 1.9060 - val_categorical_accuracy: 0.1800\n",
            "Epoch 2/2\n",
            "23/23 [==============================] - ETA: 0s - batch: 11.0000 - size: 28.8261 - loss: 1.2087 - categorical_accuracy: 0.5596\n",
            "Epoch 00002: saving model to model_init_2021-03-2108_37_54.227943/model-00002-1.20872-0.55958-2.55964-0.26000.h5\n",
            "0\n",
            "23/23 [==============================] - 197s 9s/step - batch: 11.0000 - size: 28.8261 - loss: 1.2087 - categorical_accuracy: 0.5596 - val_loss: 2.5596 - val_categorical_accuracy: 0.2600\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc0ce0badd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xReB_wtlNVrx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87795f55-b706-460b-b3e0-92896a085279"
      },
      "source": [
        "conv_3d1.clear_session(conv_3d1_model)\n",
        "conv_3d1=ModelConv3D1()\n",
        "conv_3d1.initialize_src_path(main_folder)\n",
        "conv_3d1.initialize_image_properties(image_height=100,image_width=100)\n",
        "conv_3d1.initialize_hyperparams(frames_to_sample=30,batch_size=60,num_epochs=2)\n",
        "conv_3d1_model=conv_3d1.define_model()\n",
        "print(\"Total Params:\", conv_3d1_model.count_params())\n",
        "conv_3d1.train_model(conv_3d1_model)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Params: 687813\n",
            "Epoch 1/2\n",
            "12/12 [==============================] - ETA: 0s - batch: 5.5000 - size: 55.2500 - loss: 1.6515 - categorical_accuracy: 0.3529 \n",
            "Epoch 00001: saving model to model_init_2021-03-2108_45_03.333779/model-00001-1.65146-0.35294-1.57691-0.27000.h5\n",
            "48\n",
            "12/12 [==============================] - 231s 19s/step - batch: 5.5000 - size: 55.2500 - loss: 1.6515 - categorical_accuracy: 0.3529 - val_loss: 1.5769 - val_categorical_accuracy: 0.2700\n",
            "Epoch 2/2\n",
            "12/12 [==============================] - ETA: 0s - batch: 5.5000 - size: 55.2500 - loss: 1.1992 - categorical_accuracy: 0.5505 \n",
            "Epoch 00002: saving model to model_init_2021-03-2108_45_03.333779/model-00002-1.19918-0.55053-2.01046-0.19000.h5\n",
            "0\n",
            "12/12 [==============================] - 202s 18s/step - batch: 5.5000 - size: 55.2500 - loss: 1.1992 - categorical_accuracy: 0.5505 - val_loss: 2.0105 - val_categorical_accuracy: 0.1900\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc1128a1c90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoKopL3YNVry",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8c519f4-268c-41f7-e43e-7f1999ed1a9e"
      },
      "source": [
        "conv_3d1.clear_session(conv_3d1_model)\n",
        "conv_3d1=ModelConv3D1()\n",
        "conv_3d1.initialize_src_path(main_folder)\n",
        "conv_3d1.initialize_image_properties(image_height=100,image_width=100)\n",
        "conv_3d1.initialize_hyperparams(frames_to_sample=16,batch_size=60,num_epochs=2)\n",
        "conv_3d1_model=conv_3d1.define_model()\n",
        "print(\"Total Params:\", conv_3d1_model.count_params())\n",
        "conv_3d1.train_model(conv_3d1_model)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Params: 687813\n",
            "Epoch 1/2\n",
            "12/12 [==============================] - ETA: 0s - batch: 5.5000 - size: 55.2500 - loss: 1.6753 - categorical_accuracy: 0.4057\n",
            "Epoch 00001: saving model to model_init_2021-03-2108_52_18.162235/model-00001-1.67529-0.40573-1.59577-0.30000.h5\n",
            "48\n",
            "12/12 [==============================] - 128s 10s/step - batch: 5.5000 - size: 55.2500 - loss: 1.6753 - categorical_accuracy: 0.4057 - val_loss: 1.5958 - val_categorical_accuracy: 0.3000\n",
            "Epoch 2/2\n",
            "11/12 [==========================>...] - ETA: 8s - batch: 5.0000 - size: 60.0000 - loss: 1.2168 - categorical_accuracy: 0.5136 \n",
            "Epoch 00002: saving model to model_init_2021-03-2108_52_18.162235/model-00002-1.23956-0.51282-2.23712-0.17000.h5\n",
            "0\n",
            "12/12 [==============================] - 107s 10s/step - batch: 5.5000 - size: 55.2500 - loss: 1.2396 - categorical_accuracy: 0.5128 - val_loss: 2.2371 - val_categorical_accuracy: 0.1700\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc112379e10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHABNT4GNVry",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "099c9046-18f3-41b2-d04b-9529404bf1d0"
      },
      "source": [
        "conv_3d1.clear_session(conv_3d1_model)\n",
        "conv_3d1=ModelConv3D1()\n",
        "conv_3d1.initialize_src_path(main_folder)\n",
        "conv_3d1.initialize_image_properties(image_height=100,image_width=100)\n",
        "conv_3d1.initialize_hyperparams(frames_to_sample=16,batch_size=80,num_epochs=2)\n",
        "conv_3d1_model=conv_3d1.define_model()\n",
        "print(\"Total Params:\", conv_3d1_model.count_params())\n",
        "conv_3d1.train_model(conv_3d1_model)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Params: 687813\n",
            "Epoch 1/2\n",
            "9/9 [==============================] - ETA: 0s - batch: 4.0000 - size: 73.6667 - loss: 1.7428 - categorical_accuracy: 0.3620 \n",
            "Epoch 00001: saving model to model_init_2021-03-2108_56_15.459566/model-00001-1.74278-0.36199-1.69829-0.21000.h5\n",
            "48\n",
            "9/9 [==============================] - 125s 13s/step - batch: 4.0000 - size: 73.6667 - loss: 1.7428 - categorical_accuracy: 0.3620 - val_loss: 1.6983 - val_categorical_accuracy: 0.2100\n",
            "Epoch 2/2\n",
            "9/9 [==============================] - ETA: 0s - batch: 4.0000 - size: 73.6667 - loss: 1.1840 - categorical_accuracy: 0.5400 \n",
            "Epoch 00002: saving model to model_init_2021-03-2108_56_15.459566/model-00002-1.18396-0.53997-3.00948-0.16000.h5\n",
            "0\n",
            "9/9 [==============================] - 108s 13s/step - batch: 4.0000 - size: 73.6667 - loss: 1.1840 - categorical_accuracy: 0.5400 - val_loss: 3.0095 - val_categorical_accuracy: 0.1600\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc0c758d390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2T9W4HyNVrz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4a96442-add8-470b-e16b-a792438ba9ef"
      },
      "source": [
        "conv_3d1.clear_session(conv_3d1_model)\n",
        "conv_3d1=ModelConv3D1()\n",
        "conv_3d1.initialize_src_path(main_folder)\n",
        "conv_3d1.initialize_image_properties(image_height=160,image_width=160)\n",
        "conv_3d1.initialize_hyperparams(frames_to_sample=30,batch_size=15,num_epochs=2)\n",
        "conv_3d1_model=conv_3d1.define_model()\n",
        "print(\"Total Params:\", conv_3d1_model.count_params())\n",
        "conv_3d1.train_model(conv_3d1_model)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Params: 1736389\n",
            "Epoch 1/2\n",
            "45/45 [==============================] - ETA: 0s - batch: 22.0000 - size: 14.7333 - loss: 1.5440 - categorical_accuracy: 0.4118\n",
            "Epoch 00001: saving model to model_init_2021-03-2109_00_10.907179/model-00001-1.54404-0.41176-4.48453-0.16000.h5\n",
            "48\n",
            "45/45 [==============================] - 266s 6s/step - batch: 22.0000 - size: 14.7333 - loss: 1.5440 - categorical_accuracy: 0.4118 - val_loss: 4.4845 - val_categorical_accuracy: 0.1600\n",
            "Epoch 2/2\n",
            "45/45 [==============================] - ETA: 0s - batch: 22.0000 - size: 14.7333 - loss: 1.2099 - categorical_accuracy: 0.5611\n",
            "Epoch 00002: saving model to model_init_2021-03-2109_00_10.907179/model-00002-1.20990-0.56109-6.74839-0.26000.h5\n",
            "0\n",
            "45/45 [==============================] - 225s 5s/step - batch: 22.0000 - size: 14.7333 - loss: 1.2099 - categorical_accuracy: 0.5611 - val_loss: 6.7484 - val_categorical_accuracy: 0.2600\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc0c606ab50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25WU3xHcNVrz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7d7c24d-9bc7-419e-8256-d7bf93a10d58"
      },
      "source": [
        "conv_3d1.clear_session(conv_3d1_model)\n",
        "conv_3d1=ModelConv3D1()\n",
        "conv_3d1.initialize_src_path(main_folder)\n",
        "conv_3d1.initialize_image_properties(image_height=160,image_width=160)\n",
        "conv_3d1.initialize_hyperparams(frames_to_sample=16,batch_size=15,num_epochs=2)\n",
        "conv_3d1_model=conv_3d1.define_model()\n",
        "print(\"Total Params:\", conv_3d1_model.count_params())\n",
        "conv_3d1.train_model(conv_3d1_model)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Params: 1736389\n",
            "Epoch 1/2\n",
            "45/45 [==============================] - ETA: 0s - batch: 22.0000 - size: 14.7333 - loss: 1.5820 - categorical_accuracy: 0.4027\n",
            "Epoch 00001: saving model to model_init_2021-03-2109_08_24.499191/model-00001-1.58195-0.40271-6.01646-0.16000.h5\n",
            "48\n",
            "45/45 [==============================] - 143s 3s/step - batch: 22.0000 - size: 14.7333 - loss: 1.5820 - categorical_accuracy: 0.4027 - val_loss: 6.0165 - val_categorical_accuracy: 0.1600\n",
            "Epoch 2/2\n",
            "45/45 [==============================] - ETA: 0s - batch: 22.0000 - size: 14.7333 - loss: 1.0770 - categorical_accuracy: 0.6078\n",
            "Epoch 00002: saving model to model_init_2021-03-2109_08_24.499191/model-00002-1.07701-0.60784-12.25964-0.21000.h5\n",
            "0\n",
            "45/45 [==============================] - 122s 3s/step - batch: 22.0000 - size: 14.7333 - loss: 1.0770 - categorical_accuracy: 0.6078 - val_loss: 12.2596 - val_categorical_accuracy: 0.2100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc0c7dfe3d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPzFJDfUNVr0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6006da32-3094-4197-d18c-8525c111f7f7"
      },
      "source": [
        "conv_3d1.clear_session(conv_3d1_model)\n",
        "conv_3d1=ModelConv3D1()\n",
        "conv_3d1.initialize_src_path(main_folder)\n",
        "conv_3d1.initialize_image_properties(image_height=100,image_width=100)\n",
        "conv_3d1.initialize_hyperparams(frames_to_sample=16,batch_size=15,num_epochs=2)\n",
        "conv_3d1_model=conv_3d1.define_model()\n",
        "print(\"Total Params:\", conv_3d1_model.count_params())\n",
        "conv_3d1.train_model(conv_3d1_model)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Params: 687813\n",
            "Epoch 1/2\n",
            "45/45 [==============================] - ETA: 0s - batch: 22.0000 - size: 14.7333 - loss: 1.6018 - categorical_accuracy: 0.3876\n",
            "Epoch 00001: saving model to model_init_2021-03-2109_12_51.659582/model-00001-1.60176-0.38763-3.83403-0.16000.h5\n",
            "48\n",
            "45/45 [==============================] - 124s 3s/step - batch: 22.0000 - size: 14.7333 - loss: 1.6018 - categorical_accuracy: 0.3876 - val_loss: 3.8340 - val_categorical_accuracy: 0.1600\n",
            "Epoch 2/2\n",
            "45/45 [==============================] - ETA: 0s - batch: 22.0000 - size: 14.7333 - loss: 1.1296 - categorical_accuracy: 0.5475\n",
            "Epoch 00002: saving model to model_init_2021-03-2109_12_51.659582/model-00002-1.12957-0.54751-5.04227-0.14000.h5\n",
            "0\n",
            "45/45 [==============================] - 105s 2s/step - batch: 22.0000 - size: 14.7333 - loss: 1.1296 - categorical_accuracy: 0.5475 - val_loss: 5.0423 - val_categorical_accuracy: 0.1400\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc0c7863390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdQ6Aed-NVr4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80f4cbb7-aec4-4c7c-899f-6c5993d1c8f5"
      },
      "source": [
        "conv_3d1.clear_session(conv_3d1_model)\n",
        "conv_3d1=ModelConv3D1()\n",
        "conv_3d1.initialize_src_path(main_folder)\n",
        "conv_3d1.initialize_image_properties(image_height=100,image_width=100)\n",
        "conv_3d1.initialize_hyperparams(frames_to_sample=16,batch_size=10,num_epochs=2)\n",
        "conv_3d1_model=conv_3d1.define_model()\n",
        "print(\"Total Params:\", conv_3d1_model.count_params())\n",
        "conv_3d1.train_model(conv_3d1_model)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Params: 687813\n",
            "Epoch 1/2\n",
            "67/67 [==============================] - ETA: 0s - batch: 33.0000 - size: 9.8955 - loss: 1.5822 - categorical_accuracy: 0.3801 \n",
            "Epoch 00001: saving model to model_init_2021-03-2109_16_42.514505/model-00001-1.58219-0.38009-4.92011-0.21000.h5\n",
            "48\n",
            "67/67 [==============================] - 124s 2s/step - batch: 33.0000 - size: 9.8955 - loss: 1.5822 - categorical_accuracy: 0.3801 - val_loss: 4.9201 - val_categorical_accuracy: 0.2100\n",
            "Epoch 2/2\n",
            "67/67 [==============================] - ETA: 0s - batch: 33.0000 - size: 9.8955 - loss: 1.1958 - categorical_accuracy: 0.5309 \n",
            "Epoch 00002: saving model to model_init_2021-03-2109_16_42.514505/model-00002-1.19579-0.53092-7.04926-0.22000.h5\n",
            "0\n",
            "67/67 [==============================] - 105s 2s/step - batch: 33.0000 - size: 9.8955 - loss: 1.1958 - categorical_accuracy: 0.5309 - val_loss: 7.0493 - val_categorical_accuracy: 0.2200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc0c72badd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OYsvNb7NVr5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afd43f55-78d1-4586-e84c-326643a50aaa"
      },
      "source": [
        "conv_3d1.clear_session(conv_3d1_model)\n",
        "conv_3d1=ModelConv3D1()\n",
        "conv_3d1.initialize_src_path(main_folder)\n",
        "conv_3d1.initialize_image_properties(image_height=100,image_width=100)\n",
        "conv_3d1.initialize_hyperparams(frames_to_sample=30,batch_size=10,num_epochs=2)\n",
        "conv_3d1_model=conv_3d1.define_model()\n",
        "print(\"Total Params:\", conv_3d1_model.count_params())\n",
        "conv_3d1.train_model(conv_3d1_model)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Params: 687813\n",
            "Epoch 1/2\n",
            "67/67 [==============================] - ETA: 0s - batch: 33.0000 - size: 9.8955 - loss: 1.5933 - categorical_accuracy: 0.4208 \n",
            "Epoch 00001: saving model to model_init_2021-03-2109_20_33.533250/model-00001-1.59332-0.42081-6.02739-0.16000.h5\n",
            "48\n",
            "67/67 [==============================] - 225s 3s/step - batch: 33.0000 - size: 9.8955 - loss: 1.5933 - categorical_accuracy: 0.4208 - val_loss: 6.0274 - val_categorical_accuracy: 0.1600\n",
            "Epoch 2/2\n",
            "67/67 [==============================] - ETA: 0s - batch: 33.0000 - size: 9.8955 - loss: 1.1371 - categorical_accuracy: 0.5581 \n",
            "Epoch 00002: saving model to model_init_2021-03-2109_20_33.533250/model-00002-1.13714-0.55807-10.45722-0.17000.h5\n",
            "0\n",
            "67/67 [==============================] - 202s 3s/step - batch: 33.0000 - size: 9.8955 - loss: 1.1371 - categorical_accuracy: 0.5581 - val_loss: 10.4572 - val_categorical_accuracy: 0.1700\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc0c6d7fc90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6D4BJx6gNVr6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12f2aff5-c717-4684-9d27-083d48884340"
      },
      "source": [
        "conv_3d1.clear_session(conv_3d1_model)\n",
        "conv_3d1=ModelConv3D1()\n",
        "conv_3d1.initialize_src_path(main_folder)\n",
        "conv_3d1.initialize_image_properties(image_height=160,image_width=160)\n",
        "conv_3d1.initialize_hyperparams(frames_to_sample=30,batch_size=10,num_epochs=2)\n",
        "conv_3d1_model=conv_3d1.define_model()\n",
        "print(\"Total Params:\", conv_3d1_model.count_params())\n",
        "conv_3d1.train_model(conv_3d1_model)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Params: 1736389\n",
            "Epoch 1/2\n",
            "67/67 [==============================] - ETA: 0s - batch: 33.0000 - size: 9.8955 - loss: 1.6431 - categorical_accuracy: 0.3756 \n",
            "Epoch 00001: saving model to model_init_2021-03-2109_27_42.910688/model-00001-1.64306-0.37557-11.39790-0.16000.h5\n",
            "48\n",
            "67/67 [==============================] - 265s 4s/step - batch: 33.0000 - size: 9.8955 - loss: 1.6431 - categorical_accuracy: 0.3756 - val_loss: 11.3979 - val_categorical_accuracy: 0.1600\n",
            "Epoch 2/2\n",
            "67/67 [==============================] - ETA: 0s - batch: 33.0000 - size: 9.8955 - loss: 1.2571 - categorical_accuracy: 0.5158 \n",
            "Epoch 00002: saving model to model_init_2021-03-2109_27_42.910688/model-00002-1.25708-0.51584-15.40731-0.16000.h5\n",
            "0\n",
            "67/67 [==============================] - 227s 3s/step - batch: 33.0000 - size: 9.8955 - loss: 1.2571 - categorical_accuracy: 0.5158 - val_loss: 15.4073 - val_categorical_accuracy: 0.1600\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc0c64e4dd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oo38TA_HNVr6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b88138aa-e1b5-497e-9e8b-7db9082667bf"
      },
      "source": [
        "conv_3d1.clear_session(conv_3d1_model)\n",
        "conv_3d1=ModelConv3D1()\n",
        "conv_3d1.initialize_src_path(main_folder)\n",
        "conv_3d1.initialize_image_properties(image_height=160,image_width=160)\n",
        "conv_3d1.initialize_hyperparams(frames_to_sample=16,batch_size=10,num_epochs=2)\n",
        "conv_3d1_model=conv_3d1.define_model()\n",
        "print(\"Total Params:\", conv_3d1_model.count_params())\n",
        "conv_3d1.train_model(conv_3d1_model)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Params: 1736389\n",
            "Epoch 1/2\n",
            "67/67 [==============================] - ETA: 0s - batch: 33.0000 - size: 9.8955 - loss: 1.5755 - categorical_accuracy: 0.4012 \n",
            "Epoch 00001: saving model to model_init_2021-03-2109_35_56.896391/model-00001-1.57553-0.40121-2.34058-0.30000.h5\n",
            "48\n",
            "67/67 [==============================] - 144s 2s/step - batch: 33.0000 - size: 9.8955 - loss: 1.5755 - categorical_accuracy: 0.4012 - val_loss: 2.3406 - val_categorical_accuracy: 0.3000\n",
            "Epoch 2/2\n",
            "67/67 [==============================] - ETA: 0s - batch: 33.0000 - size: 9.8955 - loss: 1.2382 - categorical_accuracy: 0.5083 \n",
            "Epoch 00002: saving model to model_init_2021-03-2109_35_56.896391/model-00002-1.23819-0.50830-3.09266-0.23000.h5\n",
            "0\n",
            "67/67 [==============================] - 123s 2s/step - batch: 33.0000 - size: 9.8955 - loss: 1.2382 - categorical_accuracy: 0.5083 - val_loss: 3.0927 - val_categorical_accuracy: 0.2300\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc110f34790>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ns4lmR9bNVr7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fdc8ac4-ba88-40a6-a775-486881aff9ce"
      },
      "source": [
        "conv_3d1.clear_session(conv_3d1_model)\n",
        "conv_3d1=ModelConv3D1()\n",
        "conv_3d1.initialize_src_path(main_folder)\n",
        "conv_3d1.initialize_image_properties(image_height=160,image_width=160)\n",
        "conv_3d1.initialize_hyperparams(frames_to_sample=16,batch_size=40,num_epochs=2)\n",
        "conv_3d1_model=conv_3d1.define_model()\n",
        "print(\"Total Params:\", conv_3d1_model.count_params())\n",
        "conv_3d1.train_model(conv_3d1_model)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Params: 1736389\n",
            "Epoch 1/2\n",
            "17/17 [==============================] - ETA: 0s - batch: 8.0000 - size: 39.0000 - loss: 1.6114 - categorical_accuracy: 0.3967\n",
            "Epoch 00001: saving model to model_init_2021-03-2109_40_25.844948/model-00001-1.61140-0.39668-2.02471-0.21000.h5\n",
            "48\n",
            "17/17 [==============================] - 144s 8s/step - batch: 8.0000 - size: 39.0000 - loss: 1.6114 - categorical_accuracy: 0.3967 - val_loss: 2.0247 - val_categorical_accuracy: 0.2100\n",
            "Epoch 2/2\n",
            "17/17 [==============================] - ETA: 0s - batch: 8.0000 - size: 39.0000 - loss: 1.0988 - categorical_accuracy: 0.5732\n",
            "Epoch 00002: saving model to model_init_2021-03-2109_40_25.844948/model-00002-1.09876-0.57315-5.65281-0.21000.h5\n",
            "0\n",
            "17/17 [==============================] - 125s 8s/step - batch: 8.0000 - size: 39.0000 - loss: 1.0988 - categorical_accuracy: 0.5732 - val_loss: 5.6528 - val_categorical_accuracy: 0.2100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc111ed1450>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVh5rIRCNVr7"
      },
      "source": [
        "### Observation\n",
        "\n",
        "**As we see from the above experiments image resolution and number of frames in sequence have more impact on training time than batch_size.**\n",
        "\n",
        "So experimentations are carried with batch size fixed around 15-40 and changing the resolution and number of image per sequence based on the device memory constraints . Models are designed such that their memory foot print is less than 50 MB which corresponds to 4.3 million parameters assuming the datatype size of parameters to be 12 bytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKbxe-Szy15j"
      },
      "source": [
        "## Model 1 - Base Model - No Data Augmentation Batch Size 40 and Epoch 15"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LVQM4NsNVr7"
      },
      "source": [
        "class ModelConv3D1(ModelBuilder):\n",
        "    \n",
        "    def define_model(self,filtersize=(3,3,3),dense_neurons=64,dropout=0.25):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Conv3D(16, filtersize, padding='same',\n",
        "        input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(32, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(64, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(128, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "\n",
        "        model.add(Dense(self.num_classes,activation='softmax'))\n",
        "\n",
        "        optimiser = optimizers.Adam()\n",
        "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "        return model"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-89dV5W8WAw3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36e34450-cc68-4c46-a410-734464ccd595"
      },
      "source": [
        "conv_3d1.clear_session(conv_3d1_model)\n",
        "Conv3D1=ModelConv3D1()\n",
        "Conv3D1.initialize_src_path(main_folder)\n",
        "Conv3D1.initialize_image_properties(image_height=160,image_width=160)\n",
        "Conv3D1.initialize_hyperparams(frames_to_sample=20,batch_size=40,num_epochs=15)\n",
        "Conv3D1_model=Conv3D1.define_model()\n",
        "Conv3D1_model.summary()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv3d (Conv3D)              (None, 20, 160, 160, 16)  1312      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 20, 160, 160, 16)  0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 20, 160, 160, 16)  64        \n",
            "_________________________________________________________________\n",
            "max_pooling3d (MaxPooling3D) (None, 10, 80, 80, 16)    0         \n",
            "_________________________________________________________________\n",
            "conv3d_1 (Conv3D)            (None, 10, 80, 80, 32)    13856     \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 10, 80, 80, 32)    0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 10, 80, 80, 32)    128       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_1 (MaxPooling3 (None, 5, 40, 40, 32)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_2 (Conv3D)            (None, 5, 40, 40, 64)     55360     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 5, 40, 40, 64)     0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 5, 40, 40, 64)     256       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_2 (MaxPooling3 (None, 2, 20, 20, 64)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_3 (Conv3D)            (None, 2, 20, 20, 128)    221312    \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 2, 20, 20, 128)    0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 2, 20, 20, 128)    512       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_3 (MaxPooling3 (None, 1, 10, 10, 128)    0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 12800)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                819264    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 5)                 325       \n",
            "=================================================================\n",
            "Total params: 1,117,061\n",
            "Trainable params: 1,116,325\n",
            "Non-trainable params: 736\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5j884vjoWQD0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc271ce6-4548-46f5-b20f-388c9317c3a7"
      },
      "source": [
        "print(\"Total Params:\", Conv3D1_model.count_params())\n",
        "accuracy_check_model_1 = Conv3D1.train_model(Conv3D1_model)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Params: 1117061\n",
            "Epoch 1/15\n",
            "17/17 [==============================] - ETA: 0s - batch: 8.0000 - size: 39.0000 - loss: 1.5218 - categorical_accuracy: 0.4314\n",
            "Epoch 00001: saving model to model_init_2021-03-2109_44_56.788820/model-00001-1.52181-0.43137-1.53799-0.31000.h5\n",
            "48\n",
            "17/17 [==============================] - 182s 10s/step - batch: 8.0000 - size: 39.0000 - loss: 1.5218 - categorical_accuracy: 0.4314 - val_loss: 1.5380 - val_categorical_accuracy: 0.3100\n",
            "Epoch 2/15\n",
            "17/17 [==============================] - ETA: 0s - batch: 8.0000 - size: 39.0000 - loss: 0.9892 - categorical_accuracy: 0.6169\n",
            "Epoch 00002: saving model to model_init_2021-03-2109_44_56.788820/model-00002-0.98919-0.61689-2.06962-0.23000.h5\n",
            "0\n",
            "17/17 [==============================] - 155s 10s/step - batch: 8.0000 - size: 39.0000 - loss: 0.9892 - categorical_accuracy: 0.6169 - val_loss: 2.0696 - val_categorical_accuracy: 0.2300\n",
            "Epoch 3/15\n",
            "17/17 [==============================] - ETA: 0s - batch: 8.0000 - size: 39.0000 - loss: 0.7462 - categorical_accuracy: 0.7285\n",
            "Epoch 00003: saving model to model_init_2021-03-2109_44_56.788820/model-00003-0.74623-0.72851-2.68113-0.22000.h5\n",
            "0\n",
            "17/17 [==============================] - 155s 10s/step - batch: 8.0000 - size: 39.0000 - loss: 0.7462 - categorical_accuracy: 0.7285 - val_loss: 2.6811 - val_categorical_accuracy: 0.2200\n",
            "Epoch 4/15\n",
            "17/17 [==============================] - ETA: 0s - batch: 8.0000 - size: 39.0000 - loss: 0.5651 - categorical_accuracy: 0.8024\n",
            "Epoch 00004: saving model to model_init_2021-03-2109_44_56.788820/model-00004-0.56514-0.80241-3.14692-0.21000.h5\n",
            "0\n",
            "17/17 [==============================] - 153s 10s/step - batch: 8.0000 - size: 39.0000 - loss: 0.5651 - categorical_accuracy: 0.8024 - val_loss: 3.1469 - val_categorical_accuracy: 0.2100\n",
            "Epoch 5/15\n",
            "17/17 [==============================] - ETA: 0s - batch: 8.0000 - size: 39.0000 - loss: 0.4357 - categorical_accuracy: 0.8522\n",
            "Epoch 00005: saving model to model_init_2021-03-2109_44_56.788820/model-00005-0.43573-0.85219-4.25654-0.17000.h5\n",
            "\n",
            "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "0\n",
            "17/17 [==============================] - 153s 9s/step - batch: 8.0000 - size: 39.0000 - loss: 0.4357 - categorical_accuracy: 0.8522 - val_loss: 4.2565 - val_categorical_accuracy: 0.1700\n",
            "Epoch 6/15\n",
            "17/17 [==============================] - ETA: 0s - batch: 8.0000 - size: 39.0000 - loss: 0.3409 - categorical_accuracy: 0.8929\n",
            "Epoch 00006: saving model to model_init_2021-03-2109_44_56.788820/model-00006-0.34086-0.89291-3.95901-0.25000.h5\n",
            "0\n",
            "17/17 [==============================] - 159s 10s/step - batch: 8.0000 - size: 39.0000 - loss: 0.3409 - categorical_accuracy: 0.8929 - val_loss: 3.9590 - val_categorical_accuracy: 0.2500\n",
            "Epoch 7/15\n",
            "17/17 [==============================] - ETA: 0s - batch: 8.0000 - size: 39.0000 - loss: 0.2443 - categorical_accuracy: 0.9351\n",
            "Epoch 00007: saving model to model_init_2021-03-2109_44_56.788820/model-00007-0.24433-0.93514-4.03968-0.21000.h5\n",
            "0\n",
            "17/17 [==============================] - 152s 9s/step - batch: 8.0000 - size: 39.0000 - loss: 0.2443 - categorical_accuracy: 0.9351 - val_loss: 4.0397 - val_categorical_accuracy: 0.2100\n",
            "Epoch 8/15\n",
            "17/17 [==============================] - ETA: 0s - batch: 8.0000 - size: 39.0000 - loss: 0.2487 - categorical_accuracy: 0.9427\n",
            "Epoch 00008: saving model to model_init_2021-03-2109_44_56.788820/model-00008-0.24874-0.94268-4.34314-0.21000.h5\n",
            "0\n",
            "17/17 [==============================] - 155s 10s/step - batch: 8.0000 - size: 39.0000 - loss: 0.2487 - categorical_accuracy: 0.9427 - val_loss: 4.3431 - val_categorical_accuracy: 0.2100\n",
            "Epoch 9/15\n",
            "17/17 [==============================] - ETA: 0s - batch: 8.0000 - size: 39.0000 - loss: 0.2038 - categorical_accuracy: 0.9502\n",
            "Epoch 00009: saving model to model_init_2021-03-2109_44_56.788820/model-00009-0.20381-0.95023-3.92892-0.21000.h5\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "0\n",
            "17/17 [==============================] - 156s 10s/step - batch: 8.0000 - size: 39.0000 - loss: 0.2038 - categorical_accuracy: 0.9502 - val_loss: 3.9289 - val_categorical_accuracy: 0.2100\n",
            "Epoch 10/15\n",
            "17/17 [==============================] - ETA: 0s - batch: 8.0000 - size: 39.0000 - loss: 0.1863 - categorical_accuracy: 0.9578\n",
            "Epoch 00010: saving model to model_init_2021-03-2109_44_56.788820/model-00010-0.18627-0.95777-4.44845-0.21000.h5\n",
            "0\n",
            "17/17 [==============================] - 151s 9s/step - batch: 8.0000 - size: 39.0000 - loss: 0.1863 - categorical_accuracy: 0.9578 - val_loss: 4.4484 - val_categorical_accuracy: 0.2100\n",
            "Epoch 11/15\n",
            "17/17 [==============================] - ETA: 0s - batch: 8.0000 - size: 39.0000 - loss: 0.1924 - categorical_accuracy: 0.9578\n",
            "Epoch 00011: saving model to model_init_2021-03-2109_44_56.788820/model-00011-0.19242-0.95777-4.46708-0.19000.h5\n",
            "0\n",
            "17/17 [==============================] - 153s 10s/step - batch: 8.0000 - size: 39.0000 - loss: 0.1924 - categorical_accuracy: 0.9578 - val_loss: 4.4671 - val_categorical_accuracy: 0.1900\n",
            "Epoch 12/15\n",
            "17/17 [==============================] - ETA: 0s - batch: 8.0000 - size: 39.0000 - loss: 0.1980 - categorical_accuracy: 0.9608\n",
            "Epoch 00012: saving model to model_init_2021-03-2109_44_56.788820/model-00012-0.19801-0.96078-4.24190-0.24000.h5\n",
            "0\n",
            "17/17 [==============================] - 157s 10s/step - batch: 8.0000 - size: 39.0000 - loss: 0.1980 - categorical_accuracy: 0.9608 - val_loss: 4.2419 - val_categorical_accuracy: 0.2400\n",
            "Epoch 13/15\n",
            "17/17 [==============================] - ETA: 0s - batch: 8.0000 - size: 39.0000 - loss: 0.1889 - categorical_accuracy: 0.9638\n",
            "Epoch 00013: saving model to model_init_2021-03-2109_44_56.788820/model-00013-0.18892-0.96380-4.20086-0.21000.h5\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "0\n",
            "17/17 [==============================] - 151s 9s/step - batch: 8.0000 - size: 39.0000 - loss: 0.1889 - categorical_accuracy: 0.9638 - val_loss: 4.2009 - val_categorical_accuracy: 0.2100\n",
            "Epoch 14/15\n",
            "17/17 [==============================] - ETA: 0s - batch: 8.0000 - size: 39.0000 - loss: 0.1622 - categorical_accuracy: 0.9683\n",
            "Epoch 00014: saving model to model_init_2021-03-2109_44_56.788820/model-00014-0.16219-0.96833-4.05397-0.23000.h5\n",
            "0\n",
            "17/17 [==============================] - 157s 10s/step - batch: 8.0000 - size: 39.0000 - loss: 0.1622 - categorical_accuracy: 0.9683 - val_loss: 4.0540 - val_categorical_accuracy: 0.2300\n",
            "Epoch 15/15\n",
            "17/17 [==============================] - ETA: 0s - batch: 8.0000 - size: 39.0000 - loss: 0.1558 - categorical_accuracy: 0.9729\n",
            "Epoch 00015: saving model to model_init_2021-03-2109_44_56.788820/model-00015-0.15576-0.97285-4.08687-0.19000.h5\n",
            "0\n",
            "17/17 [==============================] - 154s 10s/step - batch: 8.0000 - size: 39.0000 - loss: 0.1558 - categorical_accuracy: 0.9729 - val_loss: 4.0869 - val_categorical_accuracy: 0.1900\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r00TPGbNXXhX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "d5a3be7d-1dcc-4b0e-c1e2-d638bba21f3e"
      },
      "source": [
        "plot(accuracy_check_model_1)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dtxwayy_14Hn"
      },
      "source": [
        "#### Model is clearly overfitting. So we need to do data augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJk2twS_2lRx"
      },
      "source": [
        "## Model 2 - Augment Data , (3,3,3) filter & 160x160 image resolution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQxHzlc92oOq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a0a3df1-6d6b-48f6-c28e-873c21da8987"
      },
      "source": [
        "Conv3D1.clear_session(Conv3D1_model)\n",
        "Conv3D2=ModelConv3D1()\n",
        "Conv3D2.initialize_src_path(main_folder)\n",
        "Conv3D2.initialize_image_properties(image_height=160,image_width=160)\n",
        "Conv3D2.initialize_hyperparams(frames_to_sample=20,batch_size=20,num_epochs=25)\n",
        "Conv3D2_model=Conv3D2.define_model(dense_neurons=256,dropout=0.5)\n",
        "Conv3D2_model.summary()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv3d (Conv3D)              (None, 20, 160, 160, 16)  1312      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 20, 160, 160, 16)  0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 20, 160, 160, 16)  64        \n",
            "_________________________________________________________________\n",
            "max_pooling3d (MaxPooling3D) (None, 10, 80, 80, 16)    0         \n",
            "_________________________________________________________________\n",
            "conv3d_1 (Conv3D)            (None, 10, 80, 80, 32)    13856     \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 10, 80, 80, 32)    0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 10, 80, 80, 32)    128       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_1 (MaxPooling3 (None, 5, 40, 40, 32)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_2 (Conv3D)            (None, 5, 40, 40, 64)     55360     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 5, 40, 40, 64)     0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 5, 40, 40, 64)     256       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_2 (MaxPooling3 (None, 2, 20, 20, 64)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_3 (Conv3D)            (None, 2, 20, 20, 128)    221312    \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 2, 20, 20, 128)    0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 2, 20, 20, 128)    512       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_3 (MaxPooling3 (None, 1, 10, 10, 128)    0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 12800)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               3277056   \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 5)                 1285      \n",
            "=================================================================\n",
            "Total params: 3,638,981\n",
            "Trainable params: 3,637,477\n",
            "Non-trainable params: 1,504\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCx8waqu3PY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c529a432-db38-4039-88b4-6d5f2efc3bdd"
      },
      "source": [
        "print(\"Total Params:\", Conv3D2_model.count_params())\n",
        "accuracy_check_model_2=Conv3D2.train_model(Conv3D2_model,augment_data=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Params: 3638981\n",
            "Epoch 1/25\n",
            " 6/34 [====>.........................] - ETA: 3:42 - batch: 2.5000 - size: 40.0000 - loss: 2.6123 - categorical_accuracy: 0.2667"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXZ8sThy3XSr"
      },
      "source": [
        "plot(accuracy_check_model_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldsJg52PTDPq"
      },
      "source": [
        "Model is not overfitting and we get a best validation accuracy of ~80% and training accuracy of ~82%. Next we will try to reduce the filter size and image resolution and see if get better results. Moreover since we see minor oscillations in loss, let's try lowering the learning rate to 0.0002"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDO0BmuThfN_"
      },
      "source": [
        "Conv3D2.clear_session(Conv3D2_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qrMlLsVTSA7"
      },
      "source": [
        "## Model 3 - Reduce filter size to (2,2,2) and image res to 120 x 120"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkPpJFg3TRLh"
      },
      "source": [
        "class ModelConv3D3(ModelBuilder):\n",
        "    \n",
        "    def define_model(self,filtersize=(3,3,3),dense_neurons=64,dropout=0.25):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Conv3D(16, filtersize, padding='same',\n",
        "        input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(32, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(64, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(128, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "\n",
        "        model.add(Dense(self.num_classes,activation='softmax'))\n",
        "\n",
        "        optimiser = optimizers.Adam(lr=0.0002)\n",
        "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lk9L9pt7UBB1"
      },
      "source": [
        "\n",
        "Conv3D3=ModelConv3D3()\n",
        "Conv3D3.initialize_src_path(main_folder)\n",
        "Conv3D3.initialize_image_properties(image_height=120,image_width=120)\n",
        "Conv3D3.initialize_hyperparams(frames_to_sample=16,batch_size=30,num_epochs=30)\n",
        "Conv3D3_model=Conv3D3.define_model(filtersize=(2,2,2),dense_neurons=256,dropout=0.5)\n",
        "Conv3D3_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WW17sf6iUcQh"
      },
      "source": [
        "##tf.compat.v1.enable_eager_execution()\n",
        "print(tf.executing_eagerly())\n",
        "print(\"Total Params:\", Conv3D3_model.count_params())\n",
        "accuracy_check_model_3=Conv3D3.train_model(Conv3D3_model,augment_data=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HghaRZtGQB9"
      },
      "source": [
        "plot(accuracy_check_model_3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0IJgwK77wsF"
      },
      "source": [
        "This Model has a best validation accuracy of 68% and training accuracy of 68% . Also we were able to reduce the parameter size by half the earlier model. Let's trying adding more layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQ8jW39zjSXp"
      },
      "source": [
        "Conv3D3.clear_session(Conv3D3_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WW5VAmu9ihlH"
      },
      "source": [
        "## Model 4 - Adding more layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cyJpLxiitoB"
      },
      "source": [
        "class ModelConv3D4(ModelBuilder):\n",
        "    \n",
        "    def define_model(self,filtersize=(3,3,3),dense_neurons=64,dropout=0.25):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Conv3D(16, filtersize, padding='same',\n",
        "        input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(Conv3D(16, filtersize, padding='same',\n",
        "        input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(32, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(Conv3D(32, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(64, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(Conv3D(64, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(128, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(Conv3D(128, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "        \n",
        "\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "\n",
        "        model.add(Dense(self.num_classes,activation='softmax'))\n",
        "\n",
        "        optimiser = optimizers.Adam()\n",
        "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmOxJwmei5-S"
      },
      "source": [
        "Conv3D4=ModelConv3D4()\n",
        "Conv3D4.initialize_src_path(main_folder)\n",
        "Conv3D4.initialize_image_properties(image_height=120,image_width=120)\n",
        "Conv3D4.initialize_hyperparams(frames_to_sample=16,batch_size=20,num_epochs=30)\n",
        "Conv3D4_model=Conv3D4.define_model(filtersize=(3,3,3),dense_neurons=256,dropout=0.5)\n",
        "Conv3D4_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoR4sAUBkGDJ"
      },
      "source": [
        "print(\"Total Params:\", Conv3D4_model.count_params())\n",
        "accuracy_check_model_4=Conv3D4.train_model(Conv3D4_model,augment_data=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8lzWQankJXv"
      },
      "source": [
        "plot(accuracy_check_model_4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Q33XjvbSv8q"
      },
      "source": [
        "With more layers we see some performance improvement. We get a best validation accuracy of 73% . Let's try adding dropouts at the convolution layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PP6UEuorS7-z"
      },
      "source": [
        "## Model 5 Adding dropout at convolution layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjEDLlauS6iK"
      },
      "source": [
        "Conv3D4.clear_session(Conv3D4_model)\n",
        "class ModelConv3D5(ModelBuilder):\n",
        "    \n",
        "    def define_model(self,filtersize=(3,3,3),dense_neurons=64,dropout=0.25):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Conv3D(16, filtersize, padding='same',\n",
        "        input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(Conv3D(16, filtersize, padding='same',\n",
        "        input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Conv3D(32, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(Conv3D(32, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Conv3D(64, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(Conv3D(64, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Conv3D(128, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(Conv3D(128, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "        model.add(Dropout(dropout))\n",
        "        \n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "\n",
        "        model.add(Dense(self.num_classes,activation='softmax'))\n",
        "\n",
        "        optimiser = optimizers.Adam()\n",
        "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXzE7zAXTlb5"
      },
      "source": [
        "Conv3D5=ModelConv3D5()\n",
        "Conv3D5.initialize_src_path(main_folder)\n",
        "Conv3D5.initialize_image_properties(image_height=120,image_width=120)\n",
        "Conv3D5.initialize_hyperparams(frames_to_sample=16,batch_size=20,num_epochs=22)\n",
        "Conv3D5_model=Conv3D5.define_model(filtersize=(3,3,3),dense_neurons=256,dropout=0.25)\n",
        "Conv3D5_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JesB78x6TvM3"
      },
      "source": [
        "print(\"Total Params:\", Conv3D5_model.count_params())\n",
        "accuracy_check_model_5=Conv3D5.train_model(Conv3D5_model,augment_data=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSHXypIxT2_c"
      },
      "source": [
        "plot(accuracy_check_model_5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOot9MXnwkBV"
      },
      "source": [
        "\n",
        "Adding dropouts has further reduced validation accuracy as its not to learn generalizable features\n",
        "All models experimental models above have more than 1 million parameters. Let's try to reduce the model size and see the performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCTXMah8xFXr"
      },
      "source": [
        "Conv3D5.clear_session(Conv3D5_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FroiBKKw0un"
      },
      "source": [
        "## Model 6 - reducing the number of parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJj_Nb_IwnLi"
      },
      "source": [
        "class ModelConv3D6(ModelBuilder):\n",
        "    \n",
        "    def define_model(self,dense_neurons=64,dropout=0.25):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Conv3D(16, (3, 3, 3), padding='same',\n",
        "        input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(32, (2, 2, 2), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(64, (2, 2, 2), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(128, (2, 2, 2), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "        \n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(self.num_classes,activation='softmax'))\n",
        "\n",
        "        optimiser = optimizers.Adam(lr=0.0002)\n",
        "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKI57BD-xhK9"
      },
      "source": [
        "Conv3D6=ModelConv3D6()\n",
        "Conv3D6.initialize_src_path(main_folder)\n",
        "Conv3D6.initialize_image_properties(image_height=100,image_width=100)\n",
        "Conv3D6.initialize_hyperparams(frames_to_sample=16,batch_size=20,num_epochs=30)\n",
        "Conv3D6_model=Conv3D6.define_model(dense_neurons=128,dropout=0.25)\n",
        "Conv3D6_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ydywdxnxwbU"
      },
      "source": [
        "tprint(\"Total Params:\", Conv3D6_model.count_params())\n",
        "accuracy_check_model_6=Conv3D6.train_model(Conv3D6_model,augment_data=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7WQvNWYDybQ"
      },
      "source": [
        "plot(accuracy_check_model_6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBJFD6jaJsTV"
      },
      "source": [
        "For the above low memory foot print model the best validation accuracy of 73%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cArLoK-0RXxf"
      },
      "source": [
        "Conv3D6.clear_session(Conv3D6_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHU3qisEKMLX"
      },
      "source": [
        "## Model 7 - reducing the number of parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huD4YXHfJ9E7"
      },
      "source": [
        "class ModelConv3D7(ModelBuilder):\n",
        "    \n",
        "    def define_model(self,dense_neurons=64,dropout=0.25):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Conv3D(16, (3, 3, 3), padding='same',\n",
        "        input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(32, (3, 3, 3), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(64, (2, 2, 2), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(128, (2, 2, 2), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "        \n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(self.num_classes,activation='softmax'))\n",
        "\n",
        "        optimiser = optimizers.Adam(lr=0.0002)\n",
        "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VR3gHG0T256h"
      },
      "source": [
        "conv_3d7=ModelConv3D7()\n",
        "conv_3d7.initialize_src_path(main_folder)\n",
        "conv_3d7.initialize_image_properties(image_height=120,image_width=120)\n",
        "conv_3d7.initialize_hyperparams(frames_to_sample=16,batch_size=20,num_epochs=25)\n",
        "conv_3d7_model=conv_3d7.define_model(dense_neurons=64,dropout=0.25)\n",
        "conv_3d7_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s63kvjqd3UKl"
      },
      "source": [
        "print(\"Total Params:\", conv_3d7_model.count_params())\n",
        "accuracy_check_model_7=conv_3d7.train_model(conv_3d7_model,augment_data=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBJGgwaa3kOR"
      },
      "source": [
        "plot(accuracy_check_model_7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bBbPYK-RgAh"
      },
      "source": [
        "conv_3d7.clear_session(conv_3d7_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OW55j6S4f5y"
      },
      "source": [
        "## Model 8 - reducing the number of parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtU-6-0v4e12"
      },
      "source": [
        "class ModelConv3D8(ModelBuilder):\n",
        "    \n",
        "    def define_model(self,dense_neurons=64,dropout=0.25):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Conv3D(8, (3, 3, 3), padding='same',\n",
        "        input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(16, (3, 3, 3), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(32, (2, 2, 2), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(64, (2, 2, 2), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "        \n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(self.num_classes,activation='softmax'))\n",
        "\n",
        "        optimiser = optimizers.Adam(lr=0.0002)\n",
        "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BC1Vxikp44zW"
      },
      "source": [
        "conv_3d8=ModelConv3D8()\n",
        "conv_3d8.initialize_src_path(main_folder)\n",
        "conv_3d8.initialize_image_properties(image_height=120,image_width=120)\n",
        "conv_3d8.initialize_hyperparams(frames_to_sample=16,batch_size=20,num_epochs=30)\n",
        "conv_3d8_model=conv_3d8.define_model(dense_neurons=64,dropout=0.25)\n",
        "conv_3d8_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oikZb8Ij5DNw"
      },
      "source": [
        "print(\"Total Params:\", conv_3d8_model.count_params())\n",
        "accuracy_check_model_8=conv_3d8.train_model(conv_3d8_model,augment_data=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ds6Ddq2y5SsD"
      },
      "source": [
        "plot(accuracy_check_model_8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq14-oPeXfA9"
      },
      "source": [
        "conv_3d8.clear_session(conv_3d8_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGRwZahyW2aW"
      },
      "source": [
        "## Model 9 - CNN- LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuOHUaOhW0gJ"
      },
      "source": [
        "class CNNRNN1(ModelBuilder):\n",
        "    \n",
        "    def define_model(self,lstm_cells=64,dense_neurons=64,dropout=0.25):\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(TimeDistributed(Conv2D(16, (3, 3) , padding='same', activation='relu'),\n",
        "        input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
        "        model.add(TimeDistributed(BatchNormalization()))\n",
        "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "        \n",
        "        model.add(TimeDistributed(Conv2D(32, (3, 3) , padding='same', activation='relu')))\n",
        "        model.add(TimeDistributed(BatchNormalization()))\n",
        "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "        \n",
        "        model.add(TimeDistributed(Conv2D(64, (3, 3) , padding='same', activation='relu')))\n",
        "        model.add(TimeDistributed(BatchNormalization()))\n",
        "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "        \n",
        "        model.add(TimeDistributed(Conv2D(128, (3, 3) , padding='same', activation='relu')))\n",
        "        model.add(TimeDistributed(BatchNormalization()))\n",
        "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "        \n",
        "        model.add(TimeDistributed(Conv2D(256, (3, 3) , padding='same', activation='relu')))\n",
        "        model.add(TimeDistributed(BatchNormalization()))\n",
        "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "        \n",
        "        model.add(TimeDistributed(Flatten()))\n",
        "\n",
        "\n",
        "        model.add(LSTM(lstm_cells))\n",
        "        model.add(Dropout(dropout))\n",
        "        \n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(Dropout(dropout))\n",
        "        \n",
        "        model.add(Dense(self.num_classes, activation='softmax'))\n",
        "        optimiser = optimizers.Adam()\n",
        "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCoDy3FXXJj4"
      },
      "source": [
        "cnn_rnn1=CNNRNN1()\n",
        "cnn_rnn1.initialize_src_path(main_folder)\n",
        "cnn_rnn1.initialize_image_properties(image_height=120,image_width=120)\n",
        "cnn_rnn1.initialize_hyperparams(frames_to_sample=18,batch_size=20,num_epochs=20)\n",
        "cnn_rnn1_model=cnn_rnn1.define_model(lstm_cells=128,dense_neurons=128,dropout=0.25)\n",
        "cnn_rnn1_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-t4EsndXVii"
      },
      "source": [
        "print(\"Total Params:\", cnn_rnn1_model.count_params())\n",
        "accuracy_check_model_9=cnn_rnn1.train_model(cnn_rnn1_model,augment_data=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gm-imdTuXwbq"
      },
      "source": [
        "plot(accuracy_check_model_9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZSjWET3rag4"
      },
      "source": [
        "cnn_rnn1.clear_session(cnn_rnn1_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gdy0_yQzOn7y"
      },
      "source": [
        "## Applying More Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onOKdsVRO26u"
      },
      "source": [
        "class ModelBuilderMoreAugmentation(metaclass= abc.ABCMeta):\n",
        "    \n",
        "    def initialize_src_path(self,main_folder):\n",
        "        self.train_doc = np.random.permutation(open(main_folder + '/' + 'train.csv').readlines())\n",
        "        self.val_doc = np.random.permutation(open(main_folder + '/' + 'val.csv').readlines())\n",
        "        self.train_path = main_folder + '/' + 'train'\n",
        "        self.val_path =  main_folder + '/' + 'val'\n",
        "        self.num_train_sequences = len(self.train_doc)\n",
        "        self.num_val_sequences = len(self.val_doc)\n",
        "        \n",
        "    def initialize_image_properties(self,image_height=100,image_width=100):\n",
        "        self.image_height=image_height\n",
        "        self.image_width=image_width\n",
        "        self.channels=3\n",
        "        self.num_classes=5\n",
        "        self.total_frames=30\n",
        "          \n",
        "    def initialize_hyperparams(self,frames_to_sample=30,batch_size=20,num_epochs=20):\n",
        "        self.frames_to_sample=frames_to_sample\n",
        "        self.batch_size=batch_size\n",
        "        self.num_epochs=num_epochs\n",
        "        \n",
        "        \n",
        "    def generator(self,source_path, folder_list, augment=False):\n",
        "        img_idx = np.round(np.linspace(0,self.total_frames-1,self.frames_to_sample)).astype(int)\n",
        "        batch_size=self.batch_size\n",
        "        while True:\n",
        "            t = np.random.permutation(folder_list)\n",
        "            num_batches = len(t)//batch_size\n",
        "        \n",
        "            for batch in range(num_batches): \n",
        "                batch_data, batch_labels= self.one_batch_data(source_path,t,batch,batch_size,img_idx,augment)\n",
        "                yield batch_data, batch_labels \n",
        "\n",
        "            remaining_seq=len(t)%batch_size\n",
        "        \n",
        "            if (remaining_seq != 0):\n",
        "                batch_data, batch_labels= self.one_batch_data(source_path,t,num_batches,batch_size,img_idx,augment,remaining_seq)\n",
        "                yield batch_data, batch_labels \n",
        "    \n",
        "    \n",
        "    def one_batch_data(self,source_path,t,batch,batch_size,img_idx,augment,remaining_seq=0):\n",
        "    \n",
        "        seq_len = remaining_seq if remaining_seq else batch_size\n",
        "    \n",
        "        batch_data = np.zeros((seq_len,len(img_idx),self.image_height,self.image_width,self.channels)) \n",
        "        batch_labels = np.zeros((seq_len,self.num_classes)) \n",
        "    \n",
        "        if (augment): batch_data_aug = np.zeros((seq_len,len(img_idx),self.image_height,self.image_width,self.channels))\n",
        "\n",
        "        \n",
        "        for folder in range(seq_len): \n",
        "            imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) \n",
        "            for idx,item in enumerate(img_idx): \n",
        "                image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
        "                image_resized=transform.resize(image,(self.image_height,self.image_width,3))\n",
        "            \n",
        "\n",
        "                batch_data[folder,idx,:,:,0] = (image_resized[:,:,0])/255\n",
        "                batch_data[folder,idx,:,:,1] = (image_resized[:,:,1])/255\n",
        "                batch_data[folder,idx,:,:,2] = (image_resized[:,:,2])/255\n",
        "            \n",
        "                if (augment):\n",
        "                    shifted = cv2.warpAffine(image, \n",
        "                                             np.float32([[1, 0, np.random.randint(-30,30)],[0, 1, np.random.randint(-30,30)]]), \n",
        "                                            (image.shape[1], image.shape[0]))\n",
        "                    \n",
        "                    gray = cv2.cvtColor(shifted,cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "                    x0, y0 = np.argwhere(gray > 0).min(axis=0)\n",
        "                    x1, y1 = np.argwhere(gray > 0).max(axis=0) \n",
        "                    \n",
        "                    cropped=shifted[x0:x1,y0:y1,:]\n",
        "                    \n",
        "                    image_resized=transform.resize(cropped,(self.image_height,self.image_width,3))\n",
        "                    \n",
        "                    M = cv2.getRotationMatrix2D((self.image_width//2,self.image_height//2),\n",
        "                                                np.random.randint(-10,10), 1.0)\n",
        "                    rotated = cv2.warpAffine(image_resized, M, (self.image_width, self.image_height))\n",
        "                  \n",
        "                    batch_data_aug[folder,idx,:,:,0] = (rotated[:,:,0])/255\n",
        "                    batch_data_aug[folder,idx,:,:,1] = (rotated[:,:,1])/255\n",
        "                    batch_data_aug[folder,idx,:,:,2] = (rotated[:,:,2])/255\n",
        "                \n",
        "            \n",
        "            batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
        "            \n",
        "    \n",
        "        if (augment):\n",
        "            batch_data=np.concatenate([batch_data,batch_data_aug])\n",
        "            batch_labels=np.concatenate([batch_labels,batch_labels])\n",
        "\n",
        "        \n",
        "        return(batch_data,batch_labels)\n",
        "    \n",
        "    \n",
        "    def train_model(self, model, augment_data=False):\n",
        "        train_generator = self.generator(self.train_path, self.train_doc,augment=augment_data)\n",
        "        val_generator = self.generator(self.val_path, self.val_doc)\n",
        "\n",
        "        model_name = 'model_init' + '_' + str(datetime.datetime.now()).replace(' ','').replace(':','_') + '/'\n",
        "    \n",
        "        if not os.path.exists(model_name):\n",
        "            os.mkdir(model_name)\n",
        "        \n",
        "        filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
        "\n",
        "        checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
        "        LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4)\n",
        "        callbacks_list = [checkpoint, LR]\n",
        "\n",
        "        if (self.num_train_sequences%self.batch_size) == 0:\n",
        "            steps_per_epoch = int(self.num_train_sequences/self.batch_size)\n",
        "        else:\n",
        "            steps_per_epoch = (self.num_train_sequences//self.batch_size) + 1\n",
        "\n",
        "        if (self.num_val_sequences%self.batch_size) == 0:\n",
        "            validation_steps = int(self.num_val_sequences/self.batch_size)\n",
        "        else:\n",
        "            validation_steps = (self.num_val_sequences//self.batch_size) + 1\n",
        "    \n",
        "        history=model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=self.num_epochs, verbose=1, \n",
        "                            callbacks=callbacks_list, validation_data=val_generator, \n",
        "                            validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n",
        "        return history\n",
        "\n",
        "        \n",
        "    @abc.abstractmethod\n",
        "    def define_model(self):\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNCI5V1JPp5N"
      },
      "source": [
        "## Model 10 - (3,3,3) Filter & 160x160 Image resolution - similar to Model 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHkhEeqoPoZ-"
      },
      "source": [
        "class ModelConv3D10(ModelBuilderMoreAugmentation):\n",
        "    \n",
        "    def define_model(self,filtersize=(3,3,3),dense_neurons=64,dropout=0.25):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Conv3D(16, filtersize, padding='same',\n",
        "        input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(32, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(64, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(128, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "\n",
        "        model.add(Dense(self.num_classes,activation='softmax'))\n",
        "\n",
        "        optimiser = optimizers.Adam(lr=0.0002)\n",
        "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zxshqMLQNAK"
      },
      "source": [
        "conv_3d10=ModelConv3D10()\n",
        "conv_3d10.initialize_src_path(main_folder)\n",
        "conv_3d10.initialize_image_properties(image_height=160,image_width=160)\n",
        "conv_3d10.initialize_hyperparams(frames_to_sample=20,batch_size=20,num_epochs=30)\n",
        "conv_3d10_model=conv_3d10.define_model(dense_neurons=256,dropout=0.5)\n",
        "conv_3d10_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Qjf4MvRQSSG"
      },
      "source": [
        "print(\"Total Params:\", conv_3d10_model.count_params())\n",
        "accuracy_check_model_10=conv_3d10.train_model(conv_3d10_model,augment_data=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOdRzLqFpbxp"
      },
      "source": [
        "plot(accuracy_check_model_10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGr80KSVriGs"
      },
      "source": [
        "conv_3d10.clear_session(conv_3d10_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1Nyst3JoisB"
      },
      "source": [
        "## Model 11 - (2,2,2) Filter & 120x120 Image resolution - similar to Model 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HRBTvETolll"
      },
      "source": [
        "class ModelConv3D11(ModelBuilderMoreAugmentation):\n",
        "    \n",
        "    def define_model(self,filtersize=(3,3,3),dense_neurons=64,dropout=0.25):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Conv3D(16, filtersize, padding='same',\n",
        "        input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(32, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(64, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(128, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "\n",
        "        model.add(Dense(self.num_classes,activation='softmax'))\n",
        "\n",
        "        optimiser = optimizers.Adam(lr=0.0002)\n",
        "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WWwgLRypCj3"
      },
      "source": [
        "conv_3d11=ModelConv3D11()\n",
        "conv_3d11.initialize_src_path(main_folder)\n",
        "conv_3d11.initialize_image_properties(image_height=160,image_width=160)\n",
        "conv_3d11.initialize_hyperparams(frames_to_sample=20,batch_size=20,num_epochs=30)\n",
        "conv_3d11_model=conv_3d11.define_model(dense_neurons=256,dropout=0.5)\n",
        "conv_3d11_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUG3v2hOpJYd"
      },
      "source": [
        "print(\"Total Params:\", conv_3d11_model.count_params())\n",
        "accuracy_check_model_11=conv_3d11.train_model(conv_3d11_model,augment_data=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlA0dEQ8puAY"
      },
      "source": [
        "plot(accuracy_check_model_11)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWgA-0hcrp5V"
      },
      "source": [
        "conv_3d11.clear_session(conv_3d11_model)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}