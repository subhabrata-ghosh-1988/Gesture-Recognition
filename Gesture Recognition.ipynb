{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Gesture Recognition.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/subhabrata-ghosh-1988/Gesture-Recognition/blob/main/Gesture%20Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7ZfgtjxNVrR"
      },
      "source": [
        "# Gesture Recognition\n",
        "In this group project, you are going to build a 3D Conv model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-eH5pKuNVrV"
      },
      "source": [
        "## Import libraries\n",
        "import numpy as np\n",
        "import os\n",
        "from scipy import misc\n",
        "from imageio import imread\n",
        "import cv2\n",
        "from skimage import transform,io\n",
        "import datetime\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import abc\n",
        "from sys import getsizeof\n",
        "import shutil\n",
        "import abc\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J94bqUhVNVrW"
      },
      "source": [
        "import glob, os , shutil\n",
        "for f in glob.glob(\"/content/model_init*\"):\n",
        "    shutil.rmtree(f)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0Ubj-HfSRY3"
      },
      "source": [
        "## remove the existing zip file\n",
        "shutil.rmtree('/content/Project_data.zip', ignore_errors=True)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4ty5a19SiNl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "outputId": "35bc7460-998e-49dd-f227-863fe1dc940a"
      },
      "source": [
        "## Initiate the file download\n",
        "!pip install gdown\n",
        "import gdown\n",
        "url=\"https://drive.google.com/uc?id=1kM4V7pnLjGbuCaDpfBNHig99gr2rdqyJ\"\n",
        "output = \"Project_data.zip\"\n",
        "gdown.download(url, output, quiet=False)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (3.6.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.41.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2.10)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1kM4V7pnLjGbuCaDpfBNHig99gr2rdqyJ\n",
            "To: /content/Project_data.zip\n",
            "1.71GB [00:11, 143MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Project_data.zip'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-fwe5jbTX8r"
      },
      "source": [
        "## unzip the downloaded folder\n",
        "shutil.rmtree('/content/Project_data', ignore_errors=True)\n",
        "shutil.unpack_archive(\"Project_data.zip\", \"Project_data\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIQHPGi_NVrX"
      },
      "source": [
        "We set the random seed so that the results don't vary drastically."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ci_N-NnWNVrZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a2cc83b-0725-4a91-c2ea-5f4f2e05cffc"
      },
      "source": [
        "np.random.seed(30)\n",
        "import random as rn\n",
        "rn.seed(30)\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(30)\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "print(gpus)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bWyzW_oNVrZ"
      },
      "source": [
        "**In this block, you read the folder names for training and validation. You also set the batch_size here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyTupvi3NVrb"
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation\n",
        "from keras.layers.convolutional import Conv3D, MaxPooling3D, Conv2D, MaxPooling2D\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras import optimizers\n",
        "from keras.layers import Dropout"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCjeEg_tNVrc"
      },
      "source": [
        "# the entire dataset is placed in below directory\n",
        "main_folder='/content/Project_data/Project_data'"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSUMkqaSXcC4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "73b33ac5-326d-4c92-ef4f-8991185b4af9"
      },
      "source": [
        "## Checking current TF version\n",
        "tf.version.VERSION"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REgxbVunNVrd"
      },
      "source": [
        "### Plotting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3UJmY03NVre"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot(Model):\n",
        "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,4))\n",
        "    axes[0].plot(Model.history['loss'])   \n",
        "    axes[0].plot(Model.history['val_loss'])\n",
        "    axes[0].legend(['loss','val_loss'])\n",
        "\n",
        "    axes[1].plot(Model.history['categorical_accuracy'])   \n",
        "    axes[1].plot(Model.history['val_categorical_accuracy'])\n",
        "    axes[1].legend(['categorical_accuracy','val_categorical_accuracy'])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtV6apQQNVrf"
      },
      "source": [
        "## Generator\n",
        "This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haSnpGsboLKX"
      },
      "source": [
        "import gc\n",
        "class GCCallback(tf.keras.callbacks.Callback):\n",
        "      def on_epoch_end(self, epoch, logs=None):\n",
        "          print(gc.collect())"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YC-BRedNVri"
      },
      "source": [
        "# referred ABC library use case from this link https://riptutorial.com/python/example/23083/why-how-to-use-abcmeta-and--abstractmethod\n",
        "\n",
        "class ModelBuilder(metaclass= abc.ABCMeta):\n",
        "    \n",
        "    def initialize_src_path(self,main_folder):\n",
        "        self.train_doc = np.random.permutation(open(main_folder + '/' + 'train.csv').readlines())\n",
        "        self.val_doc = np.random.permutation(open(main_folder + '/' + 'val.csv').readlines())\n",
        "        self.train_path = main_folder + '/' + 'train'\n",
        "        self.val_path =  main_folder + '/' + 'val'\n",
        "        self.num_train_sequences = len(self.train_doc)\n",
        "        self.num_val_sequences = len(self.val_doc)\n",
        "        \n",
        "    def initialize_image_properties(self,image_height=100,image_width=100):\n",
        "        self.image_height=image_height\n",
        "        self.image_width=image_width\n",
        "        self.channels=3\n",
        "        self.num_classes=5\n",
        "        self.total_frames=30\n",
        "          \n",
        "    def initialize_hyperparams(self,frames_to_sample=30,batch_size=40,num_epochs=20):\n",
        "        self.frames_to_sample=frames_to_sample\n",
        "        self.batch_size=batch_size\n",
        "        self.num_epochs=num_epochs\n",
        "        \n",
        "        \n",
        "    def generator(self,source_path, folder_list, augment=False):\n",
        "        img_idx = np.round(np.linspace(0,self.total_frames-1,self.frames_to_sample)).astype(int)\n",
        "        batch_size=self.batch_size\n",
        "        while True:\n",
        "            t = np.random.permutation(folder_list)\n",
        "            num_batches = len(t)//batch_size\n",
        "        \n",
        "            for batch in range(num_batches): \n",
        "                batch_data, batch_labels= self.one_batch_data(source_path,t,batch,batch_size,img_idx,augment)\n",
        "                yield batch_data, batch_labels \n",
        "\n",
        "            remaining_seq=len(t)%batch_size\n",
        "        \n",
        "            if (remaining_seq != 0):\n",
        "                batch_data, batch_labels= self.one_batch_data(source_path,t,num_batches,batch_size,img_idx,augment,remaining_seq)\n",
        "                yield batch_data, batch_labels \n",
        "    \n",
        "    \n",
        "    def one_batch_data(self,source_path,t,batch,batch_size,img_idx,augment,remaining_seq=0):\n",
        "    \n",
        "        seq_len = remaining_seq if remaining_seq else batch_size\n",
        "    \n",
        "        batch_data = np.zeros((seq_len,len(img_idx),self.image_height,self.image_width,self.channels)) \n",
        "        batch_labels = np.zeros((seq_len,self.num_classes)) \n",
        "    \n",
        "        if (augment): batch_data_aug = np.zeros((seq_len,len(img_idx),self.image_height,self.image_width,self.channels))\n",
        "\n",
        "        \n",
        "        for folder in range(seq_len): \n",
        "            imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) \n",
        "            for idx,item in enumerate(img_idx): \n",
        "                image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
        "                image_resized=transform.resize(image,(self.image_height,self.image_width,3))\n",
        "            \n",
        "\n",
        "                batch_data[folder,idx,:,:,0] = (image_resized[:,:,0])/255\n",
        "                batch_data[folder,idx,:,:,1] = (image_resized[:,:,1])/255\n",
        "                batch_data[folder,idx,:,:,2] = (image_resized[:,:,2])/255\n",
        "            \n",
        "                if (augment):\n",
        "                    shifted = cv2.warpAffine(image, \n",
        "                                             np.float32([[1, 0, np.random.randint(-30,30)],[0, 1, np.random.randint(-30,30)]]), \n",
        "                                            (image.shape[1], image.shape[0]))\n",
        "                    \n",
        "                    gray = cv2.cvtColor(shifted,cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "                    x0, y0 = np.argwhere(gray > 0).min(axis=0)\n",
        "                    x1, y1 = np.argwhere(gray > 0).max(axis=0) \n",
        "                    \n",
        "                    cropped=shifted[x0:x1,y0:y1,:]\n",
        "                    \n",
        "                    image_resized=transform.resize(cropped,(self.image_height,self.image_width,3))\n",
        "            \n",
        "                    batch_data_aug[folder,idx,:,:,0] = (image_resized[:,:,0])/255\n",
        "                    batch_data_aug[folder,idx,:,:,1] = (image_resized[:,:,1])/255\n",
        "                    batch_data_aug[folder,idx,:,:,2] = (image_resized[:,:,2])/255\n",
        "                \n",
        "            \n",
        "            batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
        "            \n",
        "    \n",
        "        if (augment):\n",
        "            batch_data=np.concatenate([batch_data,batch_data_aug])\n",
        "            batch_labels=np.concatenate([batch_labels,batch_labels])\n",
        "\n",
        "        \n",
        "        return(batch_data,batch_labels)\n",
        "    \n",
        "    \n",
        "    def train_model(self, model, augment_data=False):\n",
        "        train_generator = self.generator(self.train_path, self.train_doc,augment=augment_data)\n",
        "        val_generator = self.generator(self.val_path, self.val_doc)\n",
        "\n",
        "        model_name = 'model_init' + '_' + str(datetime.datetime.now()).replace(' ','').replace(':','_') + '/'\n",
        "    \n",
        "        if not os.path.exists(model_name):\n",
        "            os.mkdir(model_name)\n",
        "        \n",
        "        filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
        "\n",
        "        checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto',save_freq='epoch')\n",
        "        LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4)\n",
        "        callbacks_list = [checkpoint, LR , GCCallback()]\n",
        "\n",
        "        if (self.num_train_sequences%self.batch_size) == 0:\n",
        "            steps_per_epoch = int(self.num_train_sequences/self.batch_size)\n",
        "        else:\n",
        "            steps_per_epoch = (self.num_train_sequences//self.batch_size) + 1\n",
        "\n",
        "        if (self.num_val_sequences%self.batch_size) == 0:\n",
        "            validation_steps = int(self.num_val_sequences/self.batch_size)\n",
        "        else:\n",
        "            validation_steps = (self.num_val_sequences//self.batch_size) + 1\n",
        "    \n",
        "        history=model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=self.num_epochs, verbose=1, \n",
        "                            callbacks=callbacks_list, validation_data=val_generator, \n",
        "                            validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n",
        "        return history\n",
        "\n",
        "    def clear_session(self, model):\n",
        "        del model\n",
        "        gc.collect()\n",
        "        tf.keras.backend.clear_session()\n",
        "        tf.compat.v1.reset_default_graph() # TF graph isn't same as Keras graph\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def define_model(self):\n",
        "        pass\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkdCou-eNVri"
      },
      "source": [
        "## Sample Model\n",
        "Here you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1XR94jpNVrk"
      },
      "source": [
        "class ModelConv3D1(ModelBuilder):\n",
        "    \n",
        "    def define_model(self):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Conv3D(16, (3, 3, 3), padding='same',\n",
        "        input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(32, (2, 2, 2), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(64, (2, 2, 2), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(128, (2, 2, 2), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(128,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.5))\n",
        "\n",
        "        model.add(Dense(64,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.25))\n",
        "\n",
        "\n",
        "        model.add(Dense(self.num_classes,activation='softmax'))\n",
        "\n",
        "        optimiser = optimizers.Adam()\n",
        "        #optimiser = 'sgd'\n",
        "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "        return model"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PIGA0cbNVrl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adf53a68-0895-4b39-e067-9cc7065004db"
      },
      "source": [
        "Conv3D1=ModelConv3D1()\n",
        "Conv3D1.initialize_src_path(main_folder)\n",
        "Conv3D1.initialize_image_properties(image_height=160,image_width=160)\n",
        "Conv3D1.initialize_hyperparams(frames_to_sample=30,batch_size=30,num_epochs=1)\n",
        "Conv3D1_model=Conv3D1.define_model()\n",
        "Conv3D1_model.summary()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv3d (Conv3D)              (None, 30, 160, 160, 16)  1312      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 30, 160, 160, 16)  0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 30, 160, 160, 16)  64        \n",
            "_________________________________________________________________\n",
            "max_pooling3d (MaxPooling3D) (None, 15, 80, 80, 16)    0         \n",
            "_________________________________________________________________\n",
            "conv3d_1 (Conv3D)            (None, 15, 80, 80, 32)    4128      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 15, 80, 80, 32)    0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 15, 80, 80, 32)    128       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_1 (MaxPooling3 (None, 7, 40, 40, 32)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_2 (Conv3D)            (None, 7, 40, 40, 64)     16448     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 7, 40, 40, 64)     0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 7, 40, 40, 64)     256       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_2 (MaxPooling3 (None, 3, 20, 20, 64)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_3 (Conv3D)            (None, 3, 20, 20, 128)    65664     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 3, 20, 20, 128)    0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 3, 20, 20, 128)    512       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_3 (MaxPooling3 (None, 1, 10, 10, 128)    0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 12800)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               1638528   \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 5)                 325       \n",
            "=================================================================\n",
            "Total params: 1,736,389\n",
            "Trainable params: 1,735,525\n",
            "Non-trainable params: 864\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eU6XuXdNVrm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "958fb8ef-f225-4902-a27d-a3c58ff8b50d"
      },
      "source": [
        "Conv3D1.train_model(Conv3D1_model)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "23/23 [==============================] - ETA: 0s - batch: 11.0000 - size: 28.8261 - loss: 1.4912 - categorical_accuracy: 0.4284\n",
            "Epoch 00001: saving model to model_init_2021-03-2108_26_18.607216/model-00001-1.49124-0.42836-4.86878-0.21000.h5\n",
            "0\n",
            "23/23 [==============================] - 274s 11s/step - batch: 11.0000 - size: 28.8261 - loss: 1.4912 - categorical_accuracy: 0.4284 - val_loss: 4.8688 - val_categorical_accuracy: 0.2100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc14f727c90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaJ-270dCcnY"
      },
      "source": [
        "Conv3D1.clear_session(Conv3D1_model)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPGUJrJXNVrn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72cd7eff-5f18-4b57-ae56-32d341d67867"
      },
      "source": [
        "Conv3D1_bs40=ModelConv3D1()\n",
        "Conv3D1_bs40.initialize_src_path(main_folder)\n",
        "Conv3D1_bs40.initialize_image_properties(image_height=160,image_width=160)\n",
        "Conv3D1_bs40.initialize_hyperparams(frames_to_sample=30,batch_size=40,num_epochs=1)\n",
        "Conv3D1_model_bs40=Conv3D1_bs40.define_model()\n",
        "Conv3D1_model_bs40.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv3d (Conv3D)              (None, 30, 160, 160, 16)  1312      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 30, 160, 160, 16)  0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 30, 160, 160, 16)  64        \n",
            "_________________________________________________________________\n",
            "max_pooling3d (MaxPooling3D) (None, 15, 80, 80, 16)    0         \n",
            "_________________________________________________________________\n",
            "conv3d_1 (Conv3D)            (None, 15, 80, 80, 32)    4128      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 15, 80, 80, 32)    0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 15, 80, 80, 32)    128       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_1 (MaxPooling3 (None, 7, 40, 40, 32)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_2 (Conv3D)            (None, 7, 40, 40, 64)     16448     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 7, 40, 40, 64)     0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 7, 40, 40, 64)     256       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_2 (MaxPooling3 (None, 3, 20, 20, 64)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_3 (Conv3D)            (None, 3, 20, 20, 128)    65664     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 3, 20, 20, 128)    0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 3, 20, 20, 128)    512       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_3 (MaxPooling3 (None, 1, 10, 10, 128)    0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 12800)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               1638528   \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 5)                 325       \n",
            "=================================================================\n",
            "Total params: 1,736,389\n",
            "Trainable params: 1,735,525\n",
            "Non-trainable params: 864\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHCAyYjiNVro"
      },
      "source": [
        "#### Got below memory exhaust error with image resolution of 160x160, 30 frames and a batch_size of 40\n",
        "ResourceExhaustedError:  OOM when allocating tensor with shape[40,16,15,80,80] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
        "\t [[node gradient_tape/sequential_2/max_pooling3d_8/MaxPool3D/MaxPool3DGrad (defined at <ipython-input-11-c85facc09113>:122) ]]\n",
        "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
        " [Op:__inference_train_function_7489]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7SaHnFVNVro",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26b08edd-b93c-4881-8b91-a40372e13eab"
      },
      "source": [
        "print(\"Memory util is {} Gigs\". format(getsizeof(np.zeros((40,16,30,160,160)))/(1024*1024*1024)))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Memory util is 3.662109524011612 Gigs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSsKCHtZNVrp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f99bbc0-5d41-4f28-d232-05b8b57cf771"
      },
      "source": [
        "conv_3d1=ModelConv3D1()\n",
        "conv_3d1.initialize_src_path(main_folder)\n",
        "conv_3d1.initialize_image_properties(image_height=160,image_width=160)\n",
        "conv_3d1.initialize_hyperparams(frames_to_sample=16,batch_size=30,num_epochs=3)\n",
        "conv_3d1_model=conv_3d1.define_model()\n",
        "print(\"Total Params:\", conv_3d1_model.count_params())\n",
        "conv_3d1.train_model(conv_3d1_model)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Params: 1736389\n",
            "Epoch 1/3\n",
            "23/23 [==============================] - ETA: 0s - batch: 11.0000 - size: 28.8261 - loss: 1.4302 - categorical_accuracy: 0.4540\n",
            "Epoch 00001: saving model to model_init_2021-03-2108_31_06.879816/model-00001-1.43021-0.45400-2.03139-0.21000.h5\n",
            "0\n",
            "23/23 [==============================] - 161s 7s/step - batch: 11.0000 - size: 28.8261 - loss: 1.4302 - categorical_accuracy: 0.4540 - val_loss: 2.0314 - val_categorical_accuracy: 0.2100\n",
            "Epoch 2/3\n",
            "23/23 [==============================] - ETA: 0s - batch: 11.0000 - size: 28.8261 - loss: 0.9001 - categorical_accuracy: 0.6621\n",
            "Epoch 00002: saving model to model_init_2021-03-2108_31_06.879816/model-00002-0.90008-0.66214-4.21787-0.17000.h5\n",
            "0\n",
            "23/23 [==============================] - 121s 5s/step - batch: 11.0000 - size: 28.8261 - loss: 0.9001 - categorical_accuracy: 0.6621 - val_loss: 4.2179 - val_categorical_accuracy: 0.1700\n",
            "Epoch 3/3\n",
            "23/23 [==============================] - ETA: 0s - batch: 11.0000 - size: 28.8261 - loss: 0.7425 - categorical_accuracy: 0.7044\n",
            "Epoch 00003: saving model to model_init_2021-03-2108_31_06.879816/model-00003-0.74247-0.70437-5.32029-0.24000.h5\n",
            "0\n",
            "23/23 [==============================] - 123s 6s/step - batch: 11.0000 - size: 28.8261 - loss: 0.7425 - categorical_accuracy: 0.7044 - val_loss: 5.3203 - val_categorical_accuracy: 0.2400\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc0c5322750>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jx0kjX0PNVrr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ffe5561-5263-44bb-d898-1eb260a7ae66"
      },
      "source": [
        "conv_3d1.clear_session(conv_3d1_model)\n",
        "conv_3d1=ModelConv3D1()\n",
        "conv_3d1.initialize_src_path(main_folder)\n",
        "conv_3d1.initialize_image_properties(image_height=100,image_width=100)\n",
        "conv_3d1.initialize_hyperparams(frames_to_sample=30,batch_size=30,num_epochs=2)\n",
        "conv_3d1_model=conv_3d1.define_model()\n",
        "print(\"Total Params:\", conv_3d1_model.count_params())\n",
        "conv_3d1.train_model(conv_3d1_model)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Params: 687813\n",
            "Epoch 1/2\n",
            "23/23 [==============================] - ETA: 0s - batch: 11.0000 - size: 28.8261 - loss: 1.6461 - categorical_accuracy: 0.3771\n",
            "Epoch 00001: saving model to model_init_2021-03-2108_37_54.227943/model-00001-1.64606-0.37707-1.90597-0.18000.h5\n",
            "48\n",
            "23/23 [==============================] - 230s 10s/step - batch: 11.0000 - size: 28.8261 - loss: 1.6461 - categorical_accuracy: 0.3771 - val_loss: 1.9060 - val_categorical_accuracy: 0.1800\n",
            "Epoch 2/2\n",
            "23/23 [==============================] - ETA: 0s - batch: 11.0000 - size: 28.8261 - loss: 1.2087 - categorical_accuracy: 0.5596\n",
            "Epoch 00002: saving model to model_init_2021-03-2108_37_54.227943/model-00002-1.20872-0.55958-2.55964-0.26000.h5\n",
            "0\n",
            "23/23 [==============================] - 197s 9s/step - batch: 11.0000 - size: 28.8261 - loss: 1.2087 - categorical_accuracy: 0.5596 - val_loss: 2.5596 - val_categorical_accuracy: 0.2600\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc0ce0badd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xReB_wtlNVrx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87795f55-b706-460b-b3e0-92896a085279"
      },
      "source": [
        "conv_3d1.clear_session(conv_3d1_model)\n",
        "conv_3d1=ModelConv3D1()\n",
        "conv_3d1.initialize_src_path(main_folder)\n",
        "conv_3d1.initialize_image_properties(image_height=100,image_width=100)\n",
        "conv_3d1.initialize_hyperparams(frames_to_sample=30,batch_size=60,num_epochs=2)\n",
        "conv_3d1_model=conv_3d1.define_model()\n",
        "print(\"Total Params:\", conv_3d1_model.count_params())\n",
        "conv_3d1.train_model(conv_3d1_model)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Params: 687813\n",
            "Epoch 1/2\n",
            "12/12 [==============================] - ETA: 0s - batch: 5.5000 - size: 55.2500 - loss: 1.6515 - categorical_accuracy: 0.3529 \n",
            "Epoch 00001: saving model to model_init_2021-03-2108_45_03.333779/model-00001-1.65146-0.35294-1.57691-0.27000.h5\n",
            "48\n",
            "12/12 [==============================] - 231s 19s/step - batch: 5.5000 - size: 55.2500 - loss: 1.6515 - categorical_accuracy: 0.3529 - val_loss: 1.5769 - val_categorical_accuracy: 0.2700\n",
            "Epoch 2/2\n",
            "12/12 [==============================] - ETA: 0s - batch: 5.5000 - size: 55.2500 - loss: 1.1992 - categorical_accuracy: 0.5505 \n",
            "Epoch 00002: saving model to model_init_2021-03-2108_45_03.333779/model-00002-1.19918-0.55053-2.01046-0.19000.h5\n",
            "0\n",
            "12/12 [==============================] - 202s 18s/step - batch: 5.5000 - size: 55.2500 - loss: 1.1992 - categorical_accuracy: 0.5505 - val_loss: 2.0105 - val_categorical_accuracy: 0.1900\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc1128a1c90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoKopL3YNVry",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8c519f4-268c-41f7-e43e-7f1999ed1a9e"
      },
      "source": [
        "conv_3d1.clear_session(conv_3d1_model)\n",
        "conv_3d1=ModelConv3D1()\n",
        "conv_3d1.initialize_src_path(main_folder)\n",
        "conv_3d1.initialize_image_properties(image_height=100,image_width=100)\n",
        "conv_3d1.initialize_hyperparams(frames_to_sample=16,batch_size=60,num_epochs=2)\n",
        "conv_3d1_model=conv_3d1.define_model()\n",
        "print(\"Total Params:\", conv_3d1_model.count_params())\n",
        "conv_3d1.train_model(conv_3d1_model)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Params: 687813\n",
            "Epoch 1/2\n",
            "12/12 [==============================] - ETA: 0s - batch: 5.5000 - size: 55.2500 - loss: 1.6753 - categorical_accuracy: 0.4057\n",
            "Epoch 00001: saving model to model_init_2021-03-2108_52_18.162235/model-00001-1.67529-0.40573-1.59577-0.30000.h5\n",
            "48\n",
            "12/12 [==============================] - 128s 10s/step - batch: 5.5000 - size: 55.2500 - loss: 1.6753 - categorical_accuracy: 0.4057 - val_loss: 1.5958 - val_categorical_accuracy: 0.3000\n",
            "Epoch 2/2\n",
            "11/12 [==========================>...] - ETA: 8s - batch: 5.0000 - size: 60.0000 - loss: 1.2168 - categorical_accuracy: 0.5136 \n",
            "Epoch 00002: saving model to model_init_2021-03-2108_52_18.162235/model-00002-1.23956-0.51282-2.23712-0.17000.h5\n",
            "0\n",
            "12/12 [==============================] - 107s 10s/step - batch: 5.5000 - size: 55.2500 - loss: 1.2396 - categorical_accuracy: 0.5128 - val_loss: 2.2371 - val_categorical_accuracy: 0.1700\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc112379e10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHABNT4GNVry",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "099c9046-18f3-41b2-d04b-9529404bf1d0"
      },
      "source": [
        "conv_3d1.clear_session(conv_3d1_model)\n",
        "conv_3d1=ModelConv3D1()\n",
        "conv_3d1.initialize_src_path(main_folder)\n",
        "conv_3d1.initialize_image_properties(image_height=100,image_width=100)\n",
        "conv_3d1.initialize_hyperparams(frames_to_sample=16,batch_size=80,num_epochs=2)\n",
        "conv_3d1_model=conv_3d1.define_model()\n",
        "print(\"Total Params:\", conv_3d1_model.count_params())\n",
        "conv_3d1.train_model(conv_3d1_model)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Params: 687813\n",
            "Epoch 1/2\n",
            "9/9 [==============================] - ETA: 0s - batch: 4.0000 - size: 73.6667 - loss: 1.7428 - categorical_accuracy: 0.3620 \n",
            "Epoch 00001: saving model to model_init_2021-03-2108_56_15.459566/model-00001-1.74278-0.36199-1.69829-0.21000.h5\n",
            "48\n",
            "9/9 [==============================] - 125s 13s/step - batch: 4.0000 - size: 73.6667 - loss: 1.7428 - categorical_accuracy: 0.3620 - val_loss: 1.6983 - val_categorical_accuracy: 0.2100\n",
            "Epoch 2/2\n",
            "9/9 [==============================] - ETA: 0s - batch: 4.0000 - size: 73.6667 - loss: 1.1840 - categorical_accuracy: 0.5400 \n",
            "Epoch 00002: saving model to model_init_2021-03-2108_56_15.459566/model-00002-1.18396-0.53997-3.00948-0.16000.h5\n",
            "0\n",
            "9/9 [==============================] - 108s 13s/step - batch: 4.0000 - size: 73.6667 - loss: 1.1840 - categorical_accuracy: 0.5400 - val_loss: 3.0095 - val_categorical_accuracy: 0.1600\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc0c758d390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2T9W4HyNVrz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4a96442-add8-470b-e16b-a792438ba9ef"
      },
      "source": [
        "conv_3d1.clear_session(conv_3d1_model)\n",
        "conv_3d1=ModelConv3D1()\n",
        "conv_3d1.initialize_src_path(main_folder)\n",
        "conv_3d1.initialize_image_properties(image_height=160,image_width=160)\n",
        "conv_3d1.initialize_hyperparams(frames_to_sample=30,batch_size=15,num_epochs=2)\n",
        "conv_3d1_model=conv_3d1.define_model()\n",
        "print(\"Total Params:\", conv_3d1_model.count_params())\n",
        "conv_3d1.train_model(conv_3d1_model)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Params: 1736389\n",
            "Epoch 1/2\n",
            "45/45 [==============================] - ETA: 0s - batch: 22.0000 - size: 14.7333 - loss: 1.5440 - categorical_accuracy: 0.4118\n",
            "Epoch 00001: saving model to model_init_2021-03-2109_00_10.907179/model-00001-1.54404-0.41176-4.48453-0.16000.h5\n",
            "48\n",
            "45/45 [==============================] - 266s 6s/step - batch: 22.0000 - size: 14.7333 - loss: 1.5440 - categorical_accuracy: 0.4118 - val_loss: 4.4845 - val_categorical_accuracy: 0.1600\n",
            "Epoch 2/2\n",
            "45/45 [==============================] - ETA: 0s - batch: 22.0000 - size: 14.7333 - loss: 1.2099 - categorical_accuracy: 0.5611\n",
            "Epoch 00002: saving model to model_init_2021-03-2109_00_10.907179/model-00002-1.20990-0.56109-6.74839-0.26000.h5\n",
            "0\n",
            "45/45 [==============================] - 225s 5s/step - batch: 22.0000 - size: 14.7333 - loss: 1.2099 - categorical_accuracy: 0.5611 - val_loss: 6.7484 - val_categorical_accuracy: 0.2600\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc0c606ab50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25WU3xHcNVrz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7d7c24d-9bc7-419e-8256-d7bf93a10d58"
      },
      "source": [
        "conv_3d1.clear_session(conv_3d1_model)\n",
        "conv_3d1=ModelConv3D1()\n",
        "conv_3d1.initialize_src_path(main_folder)\n",
        "conv_3d1.initialize_image_properties(image_height=160,image_width=160)\n",
        "conv_3d1.initialize_hyperparams(frames_to_sample=16,batch_size=15,num_epochs=2)\n",
        "conv_3d1_model=conv_3d1.define_model()\n",
        "print(\"Total Params:\", conv_3d1_model.count_params())\n",
        "conv_3d1.train_model(conv_3d1_model)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Params: 1736389\n",
            "Epoch 1/2\n",
            "45/45 [==============================] - ETA: 0s - batch: 22.0000 - size: 14.7333 - loss: 1.5820 - categorical_accuracy: 0.4027\n",
            "Epoch 00001: saving model to model_init_2021-03-2109_08_24.499191/model-00001-1.58195-0.40271-6.01646-0.16000.h5\n",
            "48\n",
            "45/45 [==============================] - 143s 3s/step - batch: 22.0000 - size: 14.7333 - loss: 1.5820 - categorical_accuracy: 0.4027 - val_loss: 6.0165 - val_categorical_accuracy: 0.1600\n",
            "Epoch 2/2\n",
            "45/45 [==============================] - ETA: 0s - batch: 22.0000 - size: 14.7333 - loss: 1.0770 - categorical_accuracy: 0.6078\n",
            "Epoch 00002: saving model to model_init_2021-03-2109_08_24.499191/model-00002-1.07701-0.60784-12.25964-0.21000.h5\n",
            "0\n",
            "45/45 [==============================] - 122s 3s/step - batch: 22.0000 - size: 14.7333 - loss: 1.0770 - categorical_accuracy: 0.6078 - val_loss: 12.2596 - val_categorical_accuracy: 0.2100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc0c7dfe3d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPzFJDfUNVr0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6006da32-3094-4197-d18c-8525c111f7f7"
      },
      "source": [
        "conv_3d1.clear_session(conv_3d1_model)\n",
        "conv_3d1=ModelConv3D1()\n",
        "conv_3d1.initialize_src_path(main_folder)\n",
        "conv_3d1.initialize_image_properties(image_height=100,image_width=100)\n",
        "conv_3d1.initialize_hyperparams(frames_to_sample=16,batch_size=15,num_epochs=2)\n",
        "conv_3d1_model=conv_3d1.define_model()\n",
        "print(\"Total Params:\", conv_3d1_model.count_params())\n",
        "conv_3d1.train_model(conv_3d1_model)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Params: 687813\n",
            "Epoch 1/2\n",
            "45/45 [==============================] - ETA: 0s - batch: 22.0000 - size: 14.7333 - loss: 1.6018 - categorical_accuracy: 0.3876\n",
            "Epoch 00001: saving model to model_init_2021-03-2109_12_51.659582/model-00001-1.60176-0.38763-3.83403-0.16000.h5\n",
            "48\n",
            "45/45 [==============================] - 124s 3s/step - batch: 22.0000 - size: 14.7333 - loss: 1.6018 - categorical_accuracy: 0.3876 - val_loss: 3.8340 - val_categorical_accuracy: 0.1600\n",
            "Epoch 2/2\n",
            "45/45 [==============================] - ETA: 0s - batch: 22.0000 - size: 14.7333 - loss: 1.1296 - categorical_accuracy: 0.5475\n",
            "Epoch 00002: saving model to model_init_2021-03-2109_12_51.659582/model-00002-1.12957-0.54751-5.04227-0.14000.h5\n",
            "0\n",
            "45/45 [==============================] - 105s 2s/step - batch: 22.0000 - size: 14.7333 - loss: 1.1296 - categorical_accuracy: 0.5475 - val_loss: 5.0423 - val_categorical_accuracy: 0.1400\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc0c7863390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdQ6Aed-NVr4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80f4cbb7-aec4-4c7c-899f-6c5993d1c8f5"
      },
      "source": [
        "conv_3d1.clear_session(conv_3d1_model)\n",
        "conv_3d1=ModelConv3D1()\n",
        "conv_3d1.initialize_src_path(main_folder)\n",
        "conv_3d1.initialize_image_properties(image_height=100,image_width=100)\n",
        "conv_3d1.initialize_hyperparams(frames_to_sample=16,batch_size=10,num_epochs=2)\n",
        "conv_3d1_model=conv_3d1.define_model()\n",
        "print(\"Total Params:\", conv_3d1_model.count_params())\n",
        "conv_3d1.train_model(conv_3d1_model)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Params: 687813\n",
            "Epoch 1/2\n",
            "67/67 [==============================] - ETA: 0s - batch: 33.0000 - size: 9.8955 - loss: 1.5822 - categorical_accuracy: 0.3801 \n",
            "Epoch 00001: saving model to model_init_2021-03-2109_16_42.514505/model-00001-1.58219-0.38009-4.92011-0.21000.h5\n",
            "48\n",
            "67/67 [==============================] - 124s 2s/step - batch: 33.0000 - size: 9.8955 - loss: 1.5822 - categorical_accuracy: 0.3801 - val_loss: 4.9201 - val_categorical_accuracy: 0.2100\n",
            "Epoch 2/2\n",
            "67/67 [==============================] - ETA: 0s - batch: 33.0000 - size: 9.8955 - loss: 1.1958 - categorical_accuracy: 0.5309 \n",
            "Epoch 00002: saving model to model_init_2021-03-2109_16_42.514505/model-00002-1.19579-0.53092-7.04926-0.22000.h5\n",
            "0\n",
            "67/67 [==============================] - 105s 2s/step - batch: 33.0000 - size: 9.8955 - loss: 1.1958 - categorical_accuracy: 0.5309 - val_loss: 7.0493 - val_categorical_accuracy: 0.2200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc0c72badd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OYsvNb7NVr5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afd43f55-78d1-4586-e84c-326643a50aaa"
      },
      "source": [
        "conv_3d1.clear_session(conv_3d1_model)\n",
        "conv_3d1=ModelConv3D1()\n",
        "conv_3d1.initialize_src_path(main_folder)\n",
        "conv_3d1.initialize_image_properties(image_height=100,image_width=100)\n",
        "conv_3d1.initialize_hyperparams(frames_to_sample=30,batch_size=10,num_epochs=2)\n",
        "conv_3d1_model=conv_3d1.define_model()\n",
        "print(\"Total Params:\", conv_3d1_model.count_params())\n",
        "conv_3d1.train_model(conv_3d1_model)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Params: 687813\n",
            "Epoch 1/2\n",
            "67/67 [==============================] - ETA: 0s - batch: 33.0000 - size: 9.8955 - loss: 1.5933 - categorical_accuracy: 0.4208 \n",
            "Epoch 00001: saving model to model_init_2021-03-2109_20_33.533250/model-00001-1.59332-0.42081-6.02739-0.16000.h5\n",
            "48\n",
            "67/67 [==============================] - 225s 3s/step - batch: 33.0000 - size: 9.8955 - loss: 1.5933 - categorical_accuracy: 0.4208 - val_loss: 6.0274 - val_categorical_accuracy: 0.1600\n",
            "Epoch 2/2\n",
            "67/67 [==============================] - ETA: 0s - batch: 33.0000 - size: 9.8955 - loss: 1.1371 - categorical_accuracy: 0.5581 \n",
            "Epoch 00002: saving model to model_init_2021-03-2109_20_33.533250/model-00002-1.13714-0.55807-10.45722-0.17000.h5\n",
            "0\n",
            "67/67 [==============================] - 202s 3s/step - batch: 33.0000 - size: 9.8955 - loss: 1.1371 - categorical_accuracy: 0.5581 - val_loss: 10.4572 - val_categorical_accuracy: 0.1700\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc0c6d7fc90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6D4BJx6gNVr6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12f2aff5-c717-4684-9d27-083d48884340"
      },
      "source": [
        "conv_3d1.clear_session(conv_3d1_model)\n",
        "conv_3d1=ModelConv3D1()\n",
        "conv_3d1.initialize_src_path(main_folder)\n",
        "conv_3d1.initialize_image_properties(image_height=160,image_width=160)\n",
        "conv_3d1.initialize_hyperparams(frames_to_sample=30,batch_size=10,num_epochs=2)\n",
        "conv_3d1_model=conv_3d1.define_model()\n",
        "print(\"Total Params:\", conv_3d1_model.count_params())\n",
        "conv_3d1.train_model(conv_3d1_model)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Params: 1736389\n",
            "Epoch 1/2\n",
            "67/67 [==============================] - ETA: 0s - batch: 33.0000 - size: 9.8955 - loss: 1.6431 - categorical_accuracy: 0.3756 \n",
            "Epoch 00001: saving model to model_init_2021-03-2109_27_42.910688/model-00001-1.64306-0.37557-11.39790-0.16000.h5\n",
            "48\n",
            "67/67 [==============================] - 265s 4s/step - batch: 33.0000 - size: 9.8955 - loss: 1.6431 - categorical_accuracy: 0.3756 - val_loss: 11.3979 - val_categorical_accuracy: 0.1600\n",
            "Epoch 2/2\n",
            "67/67 [==============================] - ETA: 0s - batch: 33.0000 - size: 9.8955 - loss: 1.2571 - categorical_accuracy: 0.5158 \n",
            "Epoch 00002: saving model to model_init_2021-03-2109_27_42.910688/model-00002-1.25708-0.51584-15.40731-0.16000.h5\n",
            "0\n",
            "67/67 [==============================] - 227s 3s/step - batch: 33.0000 - size: 9.8955 - loss: 1.2571 - categorical_accuracy: 0.5158 - val_loss: 15.4073 - val_categorical_accuracy: 0.1600\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc0c64e4dd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oo38TA_HNVr6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b88138aa-e1b5-497e-9e8b-7db9082667bf"
      },
      "source": [
        "conv_3d1.clear_session(conv_3d1_model)\n",
        "conv_3d1=ModelConv3D1()\n",
        "conv_3d1.initialize_src_path(main_folder)\n",
        "conv_3d1.initialize_image_properties(image_height=160,image_width=160)\n",
        "conv_3d1.initialize_hyperparams(frames_to_sample=16,batch_size=10,num_epochs=2)\n",
        "conv_3d1_model=conv_3d1.define_model()\n",
        "print(\"Total Params:\", conv_3d1_model.count_params())\n",
        "conv_3d1.train_model(conv_3d1_model)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Params: 1736389\n",
            "Epoch 1/2\n",
            "67/67 [==============================] - ETA: 0s - batch: 33.0000 - size: 9.8955 - loss: 1.5755 - categorical_accuracy: 0.4012 \n",
            "Epoch 00001: saving model to model_init_2021-03-2109_35_56.896391/model-00001-1.57553-0.40121-2.34058-0.30000.h5\n",
            "48\n",
            "67/67 [==============================] - 144s 2s/step - batch: 33.0000 - size: 9.8955 - loss: 1.5755 - categorical_accuracy: 0.4012 - val_loss: 2.3406 - val_categorical_accuracy: 0.3000\n",
            "Epoch 2/2\n",
            "67/67 [==============================] - ETA: 0s - batch: 33.0000 - size: 9.8955 - loss: 1.2382 - categorical_accuracy: 0.5083 \n",
            "Epoch 00002: saving model to model_init_2021-03-2109_35_56.896391/model-00002-1.23819-0.50830-3.09266-0.23000.h5\n",
            "0\n",
            "67/67 [==============================] - 123s 2s/step - batch: 33.0000 - size: 9.8955 - loss: 1.2382 - categorical_accuracy: 0.5083 - val_loss: 3.0927 - val_categorical_accuracy: 0.2300\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc110f34790>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ns4lmR9bNVr7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fdc8ac4-ba88-40a6-a775-486881aff9ce"
      },
      "source": [
        "conv_3d1.clear_session(conv_3d1_model)\n",
        "conv_3d1=ModelConv3D1()\n",
        "conv_3d1.initialize_src_path(main_folder)\n",
        "conv_3d1.initialize_image_properties(image_height=160,image_width=160)\n",
        "conv_3d1.initialize_hyperparams(frames_to_sample=16,batch_size=40,num_epochs=2)\n",
        "conv_3d1_model=conv_3d1.define_model()\n",
        "print(\"Total Params:\", conv_3d1_model.count_params())\n",
        "conv_3d1.train_model(conv_3d1_model)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Params: 1736389\n",
            "Epoch 1/2\n",
            "17/17 [==============================] - ETA: 0s - batch: 8.0000 - size: 39.0000 - loss: 1.6114 - categorical_accuracy: 0.3967\n",
            "Epoch 00001: saving model to model_init_2021-03-2109_40_25.844948/model-00001-1.61140-0.39668-2.02471-0.21000.h5\n",
            "48\n",
            "17/17 [==============================] - 144s 8s/step - batch: 8.0000 - size: 39.0000 - loss: 1.6114 - categorical_accuracy: 0.3967 - val_loss: 2.0247 - val_categorical_accuracy: 0.2100\n",
            "Epoch 2/2\n",
            "17/17 [==============================] - ETA: 0s - batch: 8.0000 - size: 39.0000 - loss: 1.0988 - categorical_accuracy: 0.5732\n",
            "Epoch 00002: saving model to model_init_2021-03-2109_40_25.844948/model-00002-1.09876-0.57315-5.65281-0.21000.h5\n",
            "0\n",
            "17/17 [==============================] - 125s 8s/step - batch: 8.0000 - size: 39.0000 - loss: 1.0988 - categorical_accuracy: 0.5732 - val_loss: 5.6528 - val_categorical_accuracy: 0.2100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc111ed1450>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVh5rIRCNVr7"
      },
      "source": [
        "### Observation\n",
        "\n",
        "**As we see from the above experiments image resolution and number of frames in sequence have more impact on training time than batch_size.**\n",
        "\n",
        "So experimentations are carried with batch size fixed around 15-40 and changing the resolution and number of image per sequence based on the device memory constraints . Models are designed such that their memory foot print is less than 50 MB which corresponds to 4.3 million parameters assuming the datatype size of parameters to be 12 bytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKbxe-Szy15j"
      },
      "source": [
        "## Model 1 - Base Model - No Data Augmentation Batch Size 40 and Epoch 15"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LVQM4NsNVr7"
      },
      "source": [
        "class ModelConv3D1(ModelBuilder):\n",
        "    \n",
        "    def define_model(self,filtersize=(3,3,3),dense_neurons=64,dropout=0.25):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Conv3D(16, filtersize, padding='same',\n",
        "        input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(32, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(64, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(128, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "\n",
        "        model.add(Dense(self.num_classes,activation='softmax'))\n",
        "\n",
        "        optimiser = optimizers.Adam()\n",
        "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "        return model"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-89dV5W8WAw3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36e34450-cc68-4c46-a410-734464ccd595"
      },
      "source": [
        "conv_3d1.clear_session(conv_3d1_model)\n",
        "Conv3D1=ModelConv3D1()\n",
        "Conv3D1.initialize_src_path(main_folder)\n",
        "Conv3D1.initialize_image_properties(image_height=160,image_width=160)\n",
        "Conv3D1.initialize_hyperparams(frames_to_sample=20,batch_size=40,num_epochs=15)\n",
        "Conv3D1_model=Conv3D1.define_model()\n",
        "Conv3D1_model.summary()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv3d (Conv3D)              (None, 20, 160, 160, 16)  1312      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 20, 160, 160, 16)  0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 20, 160, 160, 16)  64        \n",
            "_________________________________________________________________\n",
            "max_pooling3d (MaxPooling3D) (None, 10, 80, 80, 16)    0         \n",
            "_________________________________________________________________\n",
            "conv3d_1 (Conv3D)            (None, 10, 80, 80, 32)    13856     \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 10, 80, 80, 32)    0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 10, 80, 80, 32)    128       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_1 (MaxPooling3 (None, 5, 40, 40, 32)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_2 (Conv3D)            (None, 5, 40, 40, 64)     55360     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 5, 40, 40, 64)     0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 5, 40, 40, 64)     256       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_2 (MaxPooling3 (None, 2, 20, 20, 64)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_3 (Conv3D)            (None, 2, 20, 20, 128)    221312    \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 2, 20, 20, 128)    0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 2, 20, 20, 128)    512       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_3 (MaxPooling3 (None, 1, 10, 10, 128)    0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 12800)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                819264    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 5)                 325       \n",
            "=================================================================\n",
            "Total params: 1,117,061\n",
            "Trainable params: 1,116,325\n",
            "Non-trainable params: 736\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5j884vjoWQD0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc271ce6-4548-46f5-b20f-388c9317c3a7"
      },
      "source": [
        "print(\"Total Params:\", Conv3D1_model.count_params())\n",
        "accuracy_check_model_1 = Conv3D1.train_model(Conv3D1_model)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Params: 1117061\n",
            "Epoch 1/15\n",
            "17/17 [==============================] - ETA: 0s - batch: 8.0000 - size: 39.0000 - loss: 1.5218 - categorical_accuracy: 0.4314\n",
            "Epoch 00001: saving model to model_init_2021-03-2109_44_56.788820/model-00001-1.52181-0.43137-1.53799-0.31000.h5\n",
            "48\n",
            "17/17 [==============================] - 182s 10s/step - batch: 8.0000 - size: 39.0000 - loss: 1.5218 - categorical_accuracy: 0.4314 - val_loss: 1.5380 - val_categorical_accuracy: 0.3100\n",
            "Epoch 2/15\n",
            "17/17 [==============================] - ETA: 0s - batch: 8.0000 - size: 39.0000 - loss: 0.9892 - categorical_accuracy: 0.6169\n",
            "Epoch 00002: saving model to model_init_2021-03-2109_44_56.788820/model-00002-0.98919-0.61689-2.06962-0.23000.h5\n",
            "0\n",
            "17/17 [==============================] - 155s 10s/step - batch: 8.0000 - size: 39.0000 - loss: 0.9892 - categorical_accuracy: 0.6169 - val_loss: 2.0696 - val_categorical_accuracy: 0.2300\n",
            "Epoch 3/15\n",
            "17/17 [==============================] - ETA: 0s - batch: 8.0000 - size: 39.0000 - loss: 0.7462 - categorical_accuracy: 0.7285\n",
            "Epoch 00003: saving model to model_init_2021-03-2109_44_56.788820/model-00003-0.74623-0.72851-2.68113-0.22000.h5\n",
            "0\n",
            "17/17 [==============================] - 155s 10s/step - batch: 8.0000 - size: 39.0000 - loss: 0.7462 - categorical_accuracy: 0.7285 - val_loss: 2.6811 - val_categorical_accuracy: 0.2200\n",
            "Epoch 4/15\n",
            "17/17 [==============================] - ETA: 0s - batch: 8.0000 - size: 39.0000 - loss: 0.5651 - categorical_accuracy: 0.8024\n",
            "Epoch 00004: saving model to model_init_2021-03-2109_44_56.788820/model-00004-0.56514-0.80241-3.14692-0.21000.h5\n",
            "0\n",
            "17/17 [==============================] - 153s 10s/step - batch: 8.0000 - size: 39.0000 - loss: 0.5651 - categorical_accuracy: 0.8024 - val_loss: 3.1469 - val_categorical_accuracy: 0.2100\n",
            "Epoch 5/15\n",
            "17/17 [==============================] - ETA: 0s - batch: 8.0000 - size: 39.0000 - loss: 0.4357 - categorical_accuracy: 0.8522\n",
            "Epoch 00005: saving model to model_init_2021-03-2109_44_56.788820/model-00005-0.43573-0.85219-4.25654-0.17000.h5\n",
            "\n",
            "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "0\n",
            "17/17 [==============================] - 153s 9s/step - batch: 8.0000 - size: 39.0000 - loss: 0.4357 - categorical_accuracy: 0.8522 - val_loss: 4.2565 - val_categorical_accuracy: 0.1700\n",
            "Epoch 6/15\n",
            "17/17 [==============================] - ETA: 0s - batch: 8.0000 - size: 39.0000 - loss: 0.3409 - categorical_accuracy: 0.8929\n",
            "Epoch 00006: saving model to model_init_2021-03-2109_44_56.788820/model-00006-0.34086-0.89291-3.95901-0.25000.h5\n",
            "0\n",
            "17/17 [==============================] - 159s 10s/step - batch: 8.0000 - size: 39.0000 - loss: 0.3409 - categorical_accuracy: 0.8929 - val_loss: 3.9590 - val_categorical_accuracy: 0.2500\n",
            "Epoch 7/15\n",
            "17/17 [==============================] - ETA: 0s - batch: 8.0000 - size: 39.0000 - loss: 0.2443 - categorical_accuracy: 0.9351\n",
            "Epoch 00007: saving model to model_init_2021-03-2109_44_56.788820/model-00007-0.24433-0.93514-4.03968-0.21000.h5\n",
            "0\n",
            "17/17 [==============================] - 152s 9s/step - batch: 8.0000 - size: 39.0000 - loss: 0.2443 - categorical_accuracy: 0.9351 - val_loss: 4.0397 - val_categorical_accuracy: 0.2100\n",
            "Epoch 8/15\n",
            "17/17 [==============================] - ETA: 0s - batch: 8.0000 - size: 39.0000 - loss: 0.2487 - categorical_accuracy: 0.9427\n",
            "Epoch 00008: saving model to model_init_2021-03-2109_44_56.788820/model-00008-0.24874-0.94268-4.34314-0.21000.h5\n",
            "0\n",
            "17/17 [==============================] - 155s 10s/step - batch: 8.0000 - size: 39.0000 - loss: 0.2487 - categorical_accuracy: 0.9427 - val_loss: 4.3431 - val_categorical_accuracy: 0.2100\n",
            "Epoch 9/15\n",
            "17/17 [==============================] - ETA: 0s - batch: 8.0000 - size: 39.0000 - loss: 0.2038 - categorical_accuracy: 0.9502\n",
            "Epoch 00009: saving model to model_init_2021-03-2109_44_56.788820/model-00009-0.20381-0.95023-3.92892-0.21000.h5\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "0\n",
            "17/17 [==============================] - 156s 10s/step - batch: 8.0000 - size: 39.0000 - loss: 0.2038 - categorical_accuracy: 0.9502 - val_loss: 3.9289 - val_categorical_accuracy: 0.2100\n",
            "Epoch 10/15\n",
            "17/17 [==============================] - ETA: 0s - batch: 8.0000 - size: 39.0000 - loss: 0.1863 - categorical_accuracy: 0.9578\n",
            "Epoch 00010: saving model to model_init_2021-03-2109_44_56.788820/model-00010-0.18627-0.95777-4.44845-0.21000.h5\n",
            "0\n",
            "17/17 [==============================] - 151s 9s/step - batch: 8.0000 - size: 39.0000 - loss: 0.1863 - categorical_accuracy: 0.9578 - val_loss: 4.4484 - val_categorical_accuracy: 0.2100\n",
            "Epoch 11/15\n",
            "17/17 [==============================] - ETA: 0s - batch: 8.0000 - size: 39.0000 - loss: 0.1924 - categorical_accuracy: 0.9578\n",
            "Epoch 00011: saving model to model_init_2021-03-2109_44_56.788820/model-00011-0.19242-0.95777-4.46708-0.19000.h5\n",
            "0\n",
            "17/17 [==============================] - 153s 10s/step - batch: 8.0000 - size: 39.0000 - loss: 0.1924 - categorical_accuracy: 0.9578 - val_loss: 4.4671 - val_categorical_accuracy: 0.1900\n",
            "Epoch 12/15\n",
            "17/17 [==============================] - ETA: 0s - batch: 8.0000 - size: 39.0000 - loss: 0.1980 - categorical_accuracy: 0.9608\n",
            "Epoch 00012: saving model to model_init_2021-03-2109_44_56.788820/model-00012-0.19801-0.96078-4.24190-0.24000.h5\n",
            "0\n",
            "17/17 [==============================] - 157s 10s/step - batch: 8.0000 - size: 39.0000 - loss: 0.1980 - categorical_accuracy: 0.9608 - val_loss: 4.2419 - val_categorical_accuracy: 0.2400\n",
            "Epoch 13/15\n",
            "17/17 [==============================] - ETA: 0s - batch: 8.0000 - size: 39.0000 - loss: 0.1889 - categorical_accuracy: 0.9638\n",
            "Epoch 00013: saving model to model_init_2021-03-2109_44_56.788820/model-00013-0.18892-0.96380-4.20086-0.21000.h5\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "0\n",
            "17/17 [==============================] - 151s 9s/step - batch: 8.0000 - size: 39.0000 - loss: 0.1889 - categorical_accuracy: 0.9638 - val_loss: 4.2009 - val_categorical_accuracy: 0.2100\n",
            "Epoch 14/15\n",
            "17/17 [==============================] - ETA: 0s - batch: 8.0000 - size: 39.0000 - loss: 0.1622 - categorical_accuracy: 0.9683\n",
            "Epoch 00014: saving model to model_init_2021-03-2109_44_56.788820/model-00014-0.16219-0.96833-4.05397-0.23000.h5\n",
            "0\n",
            "17/17 [==============================] - 157s 10s/step - batch: 8.0000 - size: 39.0000 - loss: 0.1622 - categorical_accuracy: 0.9683 - val_loss: 4.0540 - val_categorical_accuracy: 0.2300\n",
            "Epoch 15/15\n",
            "17/17 [==============================] - ETA: 0s - batch: 8.0000 - size: 39.0000 - loss: 0.1558 - categorical_accuracy: 0.9729\n",
            "Epoch 00015: saving model to model_init_2021-03-2109_44_56.788820/model-00015-0.15576-0.97285-4.08687-0.19000.h5\n",
            "0\n",
            "17/17 [==============================] - 154s 10s/step - batch: 8.0000 - size: 39.0000 - loss: 0.1558 - categorical_accuracy: 0.9729 - val_loss: 4.0869 - val_categorical_accuracy: 0.1900\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r00TPGbNXXhX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "d5a3be7d-1dcc-4b0e-c1e2-d638bba21f3e"
      },
      "source": [
        "plot(accuracy_check_model_1)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dtxwayy_14Hn"
      },
      "source": [
        "#### Model is clearly overfitting. So we need to do data augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJk2twS_2lRx"
      },
      "source": [
        "## Model 2 - Augment Data , (3,3,3) filter & 160x160 image resolution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQxHzlc92oOq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a0a3df1-6d6b-48f6-c28e-873c21da8987"
      },
      "source": [
        "Conv3D1.clear_session(Conv3D1_model)\n",
        "Conv3D2=ModelConv3D1()\n",
        "Conv3D2.initialize_src_path(main_folder)\n",
        "Conv3D2.initialize_image_properties(image_height=160,image_width=160)\n",
        "Conv3D2.initialize_hyperparams(frames_to_sample=20,batch_size=20,num_epochs=25)\n",
        "Conv3D2_model=Conv3D2.define_model(dense_neurons=256,dropout=0.5)\n",
        "Conv3D2_model.summary()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv3d (Conv3D)              (None, 20, 160, 160, 16)  1312      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 20, 160, 160, 16)  0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 20, 160, 160, 16)  64        \n",
            "_________________________________________________________________\n",
            "max_pooling3d (MaxPooling3D) (None, 10, 80, 80, 16)    0         \n",
            "_________________________________________________________________\n",
            "conv3d_1 (Conv3D)            (None, 10, 80, 80, 32)    13856     \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 10, 80, 80, 32)    0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 10, 80, 80, 32)    128       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_1 (MaxPooling3 (None, 5, 40, 40, 32)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_2 (Conv3D)            (None, 5, 40, 40, 64)     55360     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 5, 40, 40, 64)     0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 5, 40, 40, 64)     256       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_2 (MaxPooling3 (None, 2, 20, 20, 64)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_3 (Conv3D)            (None, 2, 20, 20, 128)    221312    \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 2, 20, 20, 128)    0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 2, 20, 20, 128)    512       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_3 (MaxPooling3 (None, 1, 10, 10, 128)    0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 12800)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               3277056   \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 5)                 1285      \n",
            "=================================================================\n",
            "Total params: 3,638,981\n",
            "Trainable params: 3,637,477\n",
            "Non-trainable params: 1,504\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCx8waqu3PY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c529a432-db38-4039-88b4-6d5f2efc3bdd"
      },
      "source": [
        "print(\"Total Params:\", Conv3D2_model.count_params())\n",
        "accuracy_check_model_2=Conv3D2.train_model(Conv3D2_model,augment_data=True)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Params: 3638981\n",
            "Epoch 1/25\n",
            "34/34 [==============================] - ETA: 0s - batch: 16.5000 - size: 39.0000 - loss: 1.9587 - categorical_accuracy: 0.3869\n",
            "Epoch 00001: saving model to model_init_2021-03-2110_24_03.959552/model-00001-1.95875-0.38688-1.97477-0.35000.h5\n",
            "48\n",
            "34/34 [==============================] - 312s 9s/step - batch: 16.5000 - size: 39.0000 - loss: 1.9587 - categorical_accuracy: 0.3869 - val_loss: 1.9748 - val_categorical_accuracy: 0.3500\n",
            "Epoch 2/25\n",
            "34/34 [==============================] - ETA: 0s - batch: 16.5000 - size: 39.0000 - loss: 1.4944 - categorical_accuracy: 0.5045\n",
            "Epoch 00002: saving model to model_init_2021-03-2110_24_03.959552/model-00002-1.49443-0.50452-5.45264-0.23000.h5\n",
            "0\n",
            "34/34 [==============================] - 281s 8s/step - batch: 16.5000 - size: 39.0000 - loss: 1.4944 - categorical_accuracy: 0.5045 - val_loss: 5.4526 - val_categorical_accuracy: 0.2300\n",
            "Epoch 3/25\n",
            "34/34 [==============================] - ETA: 0s - batch: 16.5000 - size: 39.0000 - loss: 1.3878 - categorical_accuracy: 0.5332\n",
            "Epoch 00003: saving model to model_init_2021-03-2110_24_03.959552/model-00003-1.38777-0.53318-8.57610-0.23000.h5\n",
            "0\n",
            "34/34 [==============================] - 278s 8s/step - batch: 16.5000 - size: 39.0000 - loss: 1.3878 - categorical_accuracy: 0.5332 - val_loss: 8.5761 - val_categorical_accuracy: 0.2300\n",
            "Epoch 4/25\n",
            "34/34 [==============================] - ETA: 0s - batch: 16.5000 - size: 39.0000 - loss: 1.3168 - categorical_accuracy: 0.5294\n",
            "Epoch 00004: saving model to model_init_2021-03-2110_24_03.959552/model-00004-1.31678-0.52941-5.06784-0.16000.h5\n",
            "0\n",
            "34/34 [==============================] - 285s 9s/step - batch: 16.5000 - size: 39.0000 - loss: 1.3168 - categorical_accuracy: 0.5294 - val_loss: 5.0678 - val_categorical_accuracy: 0.1600\n",
            "Epoch 5/25\n",
            "34/34 [==============================] - ETA: 0s - batch: 16.5000 - size: 39.0000 - loss: 0.9911 - categorical_accuracy: 0.6448\n",
            "Epoch 00005: saving model to model_init_2021-03-2110_24_03.959552/model-00005-0.99109-0.64480-4.91971-0.23000.h5\n",
            "\n",
            "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "0\n",
            "34/34 [==============================] - 279s 8s/step - batch: 16.5000 - size: 39.0000 - loss: 0.9911 - categorical_accuracy: 0.6448 - val_loss: 4.9197 - val_categorical_accuracy: 0.2300\n",
            "Epoch 6/25\n",
            "34/34 [==============================] - ETA: 0s - batch: 16.5000 - size: 39.0000 - loss: 0.8449 - categorical_accuracy: 0.6840\n",
            "Epoch 00006: saving model to model_init_2021-03-2110_24_03.959552/model-00006-0.84493-0.68401-5.36019-0.25000.h5\n",
            "0\n",
            "34/34 [==============================] - 285s 9s/step - batch: 16.5000 - size: 39.0000 - loss: 0.8449 - categorical_accuracy: 0.6840 - val_loss: 5.3602 - val_categorical_accuracy: 0.2500\n",
            "Epoch 7/25\n",
            "34/34 [==============================] - ETA: 0s - batch: 16.5000 - size: 39.0000 - loss: 0.8552 - categorical_accuracy: 0.6923\n",
            "Epoch 00007: saving model to model_init_2021-03-2110_24_03.959552/model-00007-0.85523-0.69231-4.57261-0.26000.h5\n",
            "0\n",
            "34/34 [==============================] - 284s 9s/step - batch: 16.5000 - size: 39.0000 - loss: 0.8552 - categorical_accuracy: 0.6923 - val_loss: 4.5726 - val_categorical_accuracy: 0.2600\n",
            "Epoch 8/25\n",
            "34/34 [==============================] - ETA: 0s - batch: 16.5000 - size: 39.0000 - loss: 0.7811 - categorical_accuracy: 0.7066\n",
            "Epoch 00008: saving model to model_init_2021-03-2110_24_03.959552/model-00008-0.78114-0.70664-3.96139-0.15000.h5\n",
            "0\n",
            "34/34 [==============================] - 282s 9s/step - batch: 16.5000 - size: 39.0000 - loss: 0.7811 - categorical_accuracy: 0.7066 - val_loss: 3.9614 - val_categorical_accuracy: 0.1500\n",
            "Epoch 9/25\n",
            "34/34 [==============================] - ETA: 0s - batch: 16.5000 - size: 39.0000 - loss: 0.8363 - categorical_accuracy: 0.6976\n",
            "Epoch 00009: saving model to model_init_2021-03-2110_24_03.959552/model-00009-0.83625-0.69759-3.67369-0.29000.h5\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "0\n",
            "34/34 [==============================] - 280s 8s/step - batch: 16.5000 - size: 39.0000 - loss: 0.8363 - categorical_accuracy: 0.6976 - val_loss: 3.6737 - val_categorical_accuracy: 0.2900\n",
            "Epoch 10/25\n",
            "34/34 [==============================] - ETA: 0s - batch: 16.5000 - size: 39.0000 - loss: 0.7306 - categorical_accuracy: 0.7134\n",
            "Epoch 00010: saving model to model_init_2021-03-2110_24_03.959552/model-00010-0.73060-0.71342-3.08736-0.28000.h5\n",
            "0\n",
            "34/34 [==============================] - 282s 9s/step - batch: 16.5000 - size: 39.0000 - loss: 0.7306 - categorical_accuracy: 0.7134 - val_loss: 3.0874 - val_categorical_accuracy: 0.2800\n",
            "Epoch 11/25\n",
            "34/34 [==============================] - ETA: 0s - batch: 16.5000 - size: 39.0000 - loss: 0.7051 - categorical_accuracy: 0.7436\n",
            "Epoch 00011: saving model to model_init_2021-03-2110_24_03.959552/model-00011-0.70506-0.74359-2.85392-0.30000.h5\n",
            "0\n",
            "34/34 [==============================] - 282s 9s/step - batch: 16.5000 - size: 39.0000 - loss: 0.7051 - categorical_accuracy: 0.7436 - val_loss: 2.8539 - val_categorical_accuracy: 0.3000\n",
            "Epoch 12/25\n",
            "34/34 [==============================] - ETA: 0s - batch: 16.5000 - size: 39.0000 - loss: 0.6584 - categorical_accuracy: 0.7383\n",
            "Epoch 00012: saving model to model_init_2021-03-2110_24_03.959552/model-00012-0.65843-0.73831-2.60858-0.34000.h5\n",
            "0\n",
            "34/34 [==============================] - 280s 8s/step - batch: 16.5000 - size: 39.0000 - loss: 0.6584 - categorical_accuracy: 0.7383 - val_loss: 2.6086 - val_categorical_accuracy: 0.3400\n",
            "Epoch 13/25\n",
            "34/34 [==============================] - ETA: 0s - batch: 16.5000 - size: 39.0000 - loss: 0.6748 - categorical_accuracy: 0.7579\n",
            "Epoch 00013: saving model to model_init_2021-03-2110_24_03.959552/model-00013-0.67484-0.75792-2.42164-0.36000.h5\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "0\n",
            "34/34 [==============================] - 287s 9s/step - batch: 16.5000 - size: 39.0000 - loss: 0.6748 - categorical_accuracy: 0.7579 - val_loss: 2.4216 - val_categorical_accuracy: 0.3600\n",
            "Epoch 14/25\n",
            "34/34 [==============================] - ETA: 0s - batch: 16.5000 - size: 39.0000 - loss: 0.6512 - categorical_accuracy: 0.7564\n",
            "Epoch 00014: saving model to model_init_2021-03-2110_24_03.959552/model-00014-0.65123-0.75641-1.85534-0.48000.h5\n",
            "0\n",
            "34/34 [==============================] - 280s 8s/step - batch: 16.5000 - size: 39.0000 - loss: 0.6512 - categorical_accuracy: 0.7564 - val_loss: 1.8553 - val_categorical_accuracy: 0.4800\n",
            "Epoch 15/25\n",
            "34/34 [==============================] - ETA: 0s - batch: 16.5000 - size: 39.0000 - loss: 0.6445 - categorical_accuracy: 0.7715\n",
            "Epoch 00015: saving model to model_init_2021-03-2110_24_03.959552/model-00015-0.64450-0.77149-1.63579-0.53000.h5\n",
            "0\n",
            "34/34 [==============================] - 283s 9s/step - batch: 16.5000 - size: 39.0000 - loss: 0.6445 - categorical_accuracy: 0.7715 - val_loss: 1.6358 - val_categorical_accuracy: 0.5300\n",
            "Epoch 16/25\n",
            "34/34 [==============================] - ETA: 0s - batch: 16.5000 - size: 39.0000 - loss: 0.6533 - categorical_accuracy: 0.7632\n",
            "Epoch 00016: saving model to model_init_2021-03-2110_24_03.959552/model-00016-0.65331-0.76320-1.35087-0.57000.h5\n",
            "0\n",
            "34/34 [==============================] - 287s 9s/step - batch: 16.5000 - size: 39.0000 - loss: 0.6533 - categorical_accuracy: 0.7632 - val_loss: 1.3509 - val_categorical_accuracy: 0.5700\n",
            "Epoch 17/25\n",
            "34/34 [==============================] - ETA: 0s - batch: 16.5000 - size: 39.0000 - loss: 0.7124 - categorical_accuracy: 0.7564\n",
            "Epoch 00017: saving model to model_init_2021-03-2110_24_03.959552/model-00017-0.71239-0.75641-1.18667-0.64000.h5\n",
            "0\n",
            "34/34 [==============================] - 284s 9s/step - batch: 16.5000 - size: 39.0000 - loss: 0.7124 - categorical_accuracy: 0.7564 - val_loss: 1.1867 - val_categorical_accuracy: 0.6400\n",
            "Epoch 18/25\n",
            "34/34 [==============================] - ETA: 0s - batch: 16.5000 - size: 39.0000 - loss: 0.6561 - categorical_accuracy: 0.7617\n",
            "Epoch 00018: saving model to model_init_2021-03-2110_24_03.959552/model-00018-0.65611-0.76169-0.84335-0.66000.h5\n",
            "0\n",
            "34/34 [==============================] - 286s 9s/step - batch: 16.5000 - size: 39.0000 - loss: 0.6561 - categorical_accuracy: 0.7617 - val_loss: 0.8433 - val_categorical_accuracy: 0.6600\n",
            "Epoch 19/25\n",
            "34/34 [==============================] - ETA: 0s - batch: 16.5000 - size: 39.0000 - loss: 0.6575 - categorical_accuracy: 0.7655\n",
            "Epoch 00019: saving model to model_init_2021-03-2110_24_03.959552/model-00019-0.65752-0.76546-0.94943-0.72000.h5\n",
            "0\n",
            "34/34 [==============================] - 286s 9s/step - batch: 16.5000 - size: 39.0000 - loss: 0.6575 - categorical_accuracy: 0.7655 - val_loss: 0.9494 - val_categorical_accuracy: 0.7200\n",
            "Epoch 20/25\n",
            "34/34 [==============================] - ETA: 0s - batch: 16.5000 - size: 39.0000 - loss: 0.6562 - categorical_accuracy: 0.7670\n",
            "Epoch 00020: saving model to model_init_2021-03-2110_24_03.959552/model-00020-0.65620-0.76697-0.83624-0.74000.h5\n",
            "0\n",
            "34/34 [==============================] - 284s 9s/step - batch: 16.5000 - size: 39.0000 - loss: 0.6562 - categorical_accuracy: 0.7670 - val_loss: 0.8362 - val_categorical_accuracy: 0.7400\n",
            "Epoch 21/25\n",
            "34/34 [==============================] - ETA: 0s - batch: 16.5000 - size: 39.0000 - loss: 0.6336 - categorical_accuracy: 0.7557\n",
            "Epoch 00021: saving model to model_init_2021-03-2110_24_03.959552/model-00021-0.63358-0.75566-0.79407-0.78000.h5\n",
            "0\n",
            "34/34 [==============================] - 289s 9s/step - batch: 16.5000 - size: 39.0000 - loss: 0.6336 - categorical_accuracy: 0.7557 - val_loss: 0.7941 - val_categorical_accuracy: 0.7800\n",
            "Epoch 22/25\n",
            "34/34 [==============================] - ETA: 0s - batch: 16.5000 - size: 39.0000 - loss: 0.6217 - categorical_accuracy: 0.7685\n",
            "Epoch 00022: saving model to model_init_2021-03-2110_24_03.959552/model-00022-0.62173-0.76848-0.76536-0.79000.h5\n",
            "0\n",
            "34/34 [==============================] - 285s 9s/step - batch: 16.5000 - size: 39.0000 - loss: 0.6217 - categorical_accuracy: 0.7685 - val_loss: 0.7654 - val_categorical_accuracy: 0.7900\n",
            "Epoch 23/25\n",
            "34/34 [==============================] - ETA: 0s - batch: 16.5000 - size: 39.0000 - loss: 0.6568 - categorical_accuracy: 0.7753\n",
            "Epoch 00023: saving model to model_init_2021-03-2110_24_03.959552/model-00023-0.65683-0.77526-0.78275-0.77000.h5\n",
            "0\n",
            "34/34 [==============================] - 284s 9s/step - batch: 16.5000 - size: 39.0000 - loss: 0.6568 - categorical_accuracy: 0.7753 - val_loss: 0.7828 - val_categorical_accuracy: 0.7700\n",
            "Epoch 24/25\n",
            "34/34 [==============================] - ETA: 0s - batch: 16.5000 - size: 39.0000 - loss: 0.7172 - categorical_accuracy: 0.7436\n",
            "Epoch 00024: saving model to model_init_2021-03-2110_24_03.959552/model-00024-0.71725-0.74359-0.65641-0.75000.h5\n",
            "0\n",
            "34/34 [==============================] - 291s 9s/step - batch: 16.5000 - size: 39.0000 - loss: 0.7172 - categorical_accuracy: 0.7436 - val_loss: 0.6564 - val_categorical_accuracy: 0.7500\n",
            "Epoch 25/25\n",
            "34/34 [==============================] - ETA: 0s - batch: 16.5000 - size: 39.0000 - loss: 0.6603 - categorical_accuracy: 0.7534\n",
            "Epoch 00025: saving model to model_init_2021-03-2110_24_03.959552/model-00025-0.66028-0.75339-0.60351-0.81000.h5\n",
            "0\n",
            "34/34 [==============================] - 288s 9s/step - batch: 16.5000 - size: 39.0000 - loss: 0.6603 - categorical_accuracy: 0.7534 - val_loss: 0.6035 - val_categorical_accuracy: 0.8100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXZ8sThy3XSr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "9988baa7-c8c9-436a-d6d9-776c5afac78a"
      },
      "source": [
        "plot(accuracy_check_model_2)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldsJg52PTDPq"
      },
      "source": [
        "Model is not overfitting and we get a best validation accuracy of ~80% and training accuracy of ~82%. Next we will try to reduce the filter size and image resolution and see if get better results. Moreover since we see minor oscillations in loss, let's try lowering the learning rate to 0.0002"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDO0BmuThfN_"
      },
      "source": [
        "Conv3D2.clear_session(Conv3D2_model)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qrMlLsVTSA7"
      },
      "source": [
        "## Model 3 - Reduce filter size to (2,2,2) and image res to 120 x 120"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkPpJFg3TRLh"
      },
      "source": [
        "class ModelConv3D3(ModelBuilder):\n",
        "    \n",
        "    def define_model(self,filtersize=(3,3,3),dense_neurons=64,dropout=0.25):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Conv3D(16, filtersize, padding='same',\n",
        "        input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(32, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(64, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(128, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "\n",
        "        model.add(Dense(self.num_classes,activation='softmax'))\n",
        "\n",
        "        optimiser = optimizers.Adam(lr=0.0002)\n",
        "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "        return model"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lk9L9pt7UBB1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6f78a1d-d9c4-4a2d-b852-c462b8e1d05a"
      },
      "source": [
        "\n",
        "Conv3D3=ModelConv3D3()\n",
        "Conv3D3.initialize_src_path(main_folder)\n",
        "Conv3D3.initialize_image_properties(image_height=120,image_width=120)\n",
        "Conv3D3.initialize_hyperparams(frames_to_sample=16,batch_size=30,num_epochs=30)\n",
        "Conv3D3_model=Conv3D3.define_model(filtersize=(2,2,2),dense_neurons=256,dropout=0.5)\n",
        "Conv3D3_model.summary()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv3d (Conv3D)              (None, 16, 120, 120, 16)  400       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 16, 120, 120, 16)  0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 16, 120, 120, 16)  64        \n",
            "_________________________________________________________________\n",
            "max_pooling3d (MaxPooling3D) (None, 8, 60, 60, 16)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_1 (Conv3D)            (None, 8, 60, 60, 32)     4128      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 8, 60, 60, 32)     0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 8, 60, 60, 32)     128       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_1 (MaxPooling3 (None, 4, 30, 30, 32)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_2 (Conv3D)            (None, 4, 30, 30, 64)     16448     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 4, 30, 30, 64)     0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 4, 30, 30, 64)     256       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_2 (MaxPooling3 (None, 2, 15, 15, 64)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_3 (Conv3D)            (None, 2, 15, 15, 128)    65664     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 2, 15, 15, 128)    0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 2, 15, 15, 128)    512       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_3 (MaxPooling3 (None, 1, 7, 7, 128)      0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 6272)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               1605888   \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 5)                 1285      \n",
            "=================================================================\n",
            "Total params: 1,762,613\n",
            "Trainable params: 1,761,109\n",
            "Non-trainable params: 1,504\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WW17sf6iUcQh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d1c2e19-775f-44fc-a8d2-ec1ad0b7e446"
      },
      "source": [
        "##tf.compat.v1.enable_eager_execution()\n",
        "print(tf.executing_eagerly())\n",
        "print(\"Total Params:\", Conv3D3_model.count_params())\n",
        "accuracy_check_model_3=Conv3D3.train_model(Conv3D3_model,augment_data=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n",
            "Total Params: 1762613\n",
            "Epoch 1/30\n",
            "23/23 [==============================] - ETA: 0s - batch: 11.0000 - size: 57.6522 - loss: 2.2907 - categorical_accuracy: 0.3265\n",
            "Epoch 00001: saving model to model_init_2021-03-2112_22_51.590771/model-00001-2.29074-0.32655-1.77032-0.20000.h5\n",
            "48\n",
            "23/23 [==============================] - 222s 9s/step - batch: 11.0000 - size: 57.6522 - loss: 2.2907 - categorical_accuracy: 0.3265 - val_loss: 1.7703 - val_categorical_accuracy: 0.2000\n",
            "Epoch 2/30\n",
            "23/23 [==============================] - ETA: 0s - batch: 11.0000 - size: 57.6522 - loss: 1.6408 - categorical_accuracy: 0.4721\n",
            "Epoch 00002: saving model to model_init_2021-03-2112_22_51.590771/model-00002-1.64077-0.47210-2.51669-0.20000.h5\n",
            "0\n",
            "23/23 [==============================] - 204s 9s/step - batch: 11.0000 - size: 57.6522 - loss: 1.6408 - categorical_accuracy: 0.4721 - val_loss: 2.5167 - val_categorical_accuracy: 0.2000\n",
            "Epoch 3/30\n",
            "23/23 [==============================] - ETA: 0s - batch: 11.0000 - size: 57.6522 - loss: 1.3202 - categorical_accuracy: 0.5415\n",
            "Epoch 00003: saving model to model_init_2021-03-2112_22_51.590771/model-00003-1.32017-0.54148-3.03564-0.19000.h5\n",
            "0\n",
            "23/23 [==============================] - 203s 9s/step - batch: 11.0000 - size: 57.6522 - loss: 1.3202 - categorical_accuracy: 0.5415 - val_loss: 3.0356 - val_categorical_accuracy: 0.1900\n",
            "Epoch 4/30\n",
            "23/23 [==============================] - ETA: 0s - batch: 11.0000 - size: 57.6522 - loss: 1.2237 - categorical_accuracy: 0.5754\n",
            "Epoch 00004: saving model to model_init_2021-03-2112_22_51.590771/model-00004-1.22367-0.57541-3.96051-0.19000.h5\n",
            "0\n",
            "23/23 [==============================] - 198s 9s/step - batch: 11.0000 - size: 57.6522 - loss: 1.2237 - categorical_accuracy: 0.5754 - val_loss: 3.9605 - val_categorical_accuracy: 0.1900\n",
            "Epoch 5/30\n",
            " 9/23 [==========>...................] - ETA: 1:37 - batch: 4.0000 - size: 60.0000 - loss: 1.0708 - categorical_accuracy: 0.6148"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HghaRZtGQB9"
      },
      "source": [
        "plot(accuracy_check_model_3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0IJgwK77wsF"
      },
      "source": [
        "This Model has a best validation accuracy of 68% and training accuracy of 68% . Also we were able to reduce the parameter size by half the earlier model. Let's trying adding more layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQ8jW39zjSXp"
      },
      "source": [
        "Conv3D3.clear_session(Conv3D3_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WW5VAmu9ihlH"
      },
      "source": [
        "## Model 4 - Adding more layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cyJpLxiitoB"
      },
      "source": [
        "class ModelConv3D4(ModelBuilder):\n",
        "    \n",
        "    def define_model(self,filtersize=(3,3,3),dense_neurons=64,dropout=0.25):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Conv3D(16, filtersize, padding='same',\n",
        "        input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(Conv3D(16, filtersize, padding='same',\n",
        "        input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(32, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(Conv3D(32, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(64, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(Conv3D(64, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(128, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(Conv3D(128, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "        \n",
        "\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "\n",
        "        model.add(Dense(self.num_classes,activation='softmax'))\n",
        "\n",
        "        optimiser = optimizers.Adam()\n",
        "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmOxJwmei5-S"
      },
      "source": [
        "Conv3D4=ModelConv3D4()\n",
        "Conv3D4.initialize_src_path(main_folder)\n",
        "Conv3D4.initialize_image_properties(image_height=120,image_width=120)\n",
        "Conv3D4.initialize_hyperparams(frames_to_sample=16,batch_size=20,num_epochs=30)\n",
        "Conv3D4_model=Conv3D4.define_model(filtersize=(3,3,3),dense_neurons=256,dropout=0.5)\n",
        "Conv3D4_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoR4sAUBkGDJ"
      },
      "source": [
        "print(\"Total Params:\", Conv3D4_model.count_params())\n",
        "accuracy_check_model_4=Conv3D4.train_model(Conv3D4_model,augment_data=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8lzWQankJXv"
      },
      "source": [
        "plot(accuracy_check_model_4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Q33XjvbSv8q"
      },
      "source": [
        "With more layers we see some performance improvement. We get a best validation accuracy of 73% . Let's try adding dropouts at the convolution layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PP6UEuorS7-z"
      },
      "source": [
        "## Model 5 Adding dropout at convolution layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjEDLlauS6iK"
      },
      "source": [
        "Conv3D4.clear_session(Conv3D4_model)\n",
        "class ModelConv3D5(ModelBuilder):\n",
        "    \n",
        "    def define_model(self,filtersize=(3,3,3),dense_neurons=64,dropout=0.25):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Conv3D(16, filtersize, padding='same',\n",
        "        input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(Conv3D(16, filtersize, padding='same',\n",
        "        input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Conv3D(32, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(Conv3D(32, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Conv3D(64, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(Conv3D(64, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Conv3D(128, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(Conv3D(128, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "        model.add(Dropout(dropout))\n",
        "        \n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "\n",
        "        model.add(Dense(self.num_classes,activation='softmax'))\n",
        "\n",
        "        optimiser = optimizers.Adam()\n",
        "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXzE7zAXTlb5"
      },
      "source": [
        "Conv3D5=ModelConv3D5()\n",
        "Conv3D5.initialize_src_path(main_folder)\n",
        "Conv3D5.initialize_image_properties(image_height=120,image_width=120)\n",
        "Conv3D5.initialize_hyperparams(frames_to_sample=16,batch_size=20,num_epochs=22)\n",
        "Conv3D5_model=Conv3D5.define_model(filtersize=(3,3,3),dense_neurons=256,dropout=0.25)\n",
        "Conv3D5_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JesB78x6TvM3"
      },
      "source": [
        "print(\"Total Params:\", Conv3D5_model.count_params())\n",
        "accuracy_check_model_5=Conv3D5.train_model(Conv3D5_model,augment_data=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSHXypIxT2_c"
      },
      "source": [
        "plot(accuracy_check_model_5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOot9MXnwkBV"
      },
      "source": [
        "\n",
        "Adding dropouts has further reduced validation accuracy as its not to learn generalizable features\n",
        "All models experimental models above have more than 1 million parameters. Let's try to reduce the model size and see the performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCTXMah8xFXr"
      },
      "source": [
        "Conv3D5.clear_session(Conv3D5_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FroiBKKw0un"
      },
      "source": [
        "## Model 6 - reducing the number of parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJj_Nb_IwnLi"
      },
      "source": [
        "class ModelConv3D6(ModelBuilder):\n",
        "    \n",
        "    def define_model(self,dense_neurons=64,dropout=0.25):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Conv3D(16, (3, 3, 3), padding='same',\n",
        "        input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(32, (2, 2, 2), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(64, (2, 2, 2), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(128, (2, 2, 2), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "        \n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(self.num_classes,activation='softmax'))\n",
        "\n",
        "        optimiser = optimizers.Adam(lr=0.0002)\n",
        "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKI57BD-xhK9"
      },
      "source": [
        "Conv3D6=ModelConv3D6()\n",
        "Conv3D6.initialize_src_path(main_folder)\n",
        "Conv3D6.initialize_image_properties(image_height=100,image_width=100)\n",
        "Conv3D6.initialize_hyperparams(frames_to_sample=16,batch_size=20,num_epochs=30)\n",
        "Conv3D6_model=Conv3D6.define_model(dense_neurons=128,dropout=0.25)\n",
        "Conv3D6_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ydywdxnxwbU"
      },
      "source": [
        "tprint(\"Total Params:\", Conv3D6_model.count_params())\n",
        "accuracy_check_model_6=Conv3D6.train_model(Conv3D6_model,augment_data=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7WQvNWYDybQ"
      },
      "source": [
        "plot(accuracy_check_model_6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBJFD6jaJsTV"
      },
      "source": [
        "For the above low memory foot print model the best validation accuracy of 73%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cArLoK-0RXxf"
      },
      "source": [
        "Conv3D6.clear_session(Conv3D6_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHU3qisEKMLX"
      },
      "source": [
        "## Model 7 - reducing the number of parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huD4YXHfJ9E7"
      },
      "source": [
        "class ModelConv3D7(ModelBuilder):\n",
        "    \n",
        "    def define_model(self,dense_neurons=64,dropout=0.25):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Conv3D(16, (3, 3, 3), padding='same',\n",
        "        input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(32, (3, 3, 3), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(64, (2, 2, 2), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(128, (2, 2, 2), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "        \n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(self.num_classes,activation='softmax'))\n",
        "\n",
        "        optimiser = optimizers.Adam(lr=0.0002)\n",
        "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VR3gHG0T256h"
      },
      "source": [
        "conv_3d7=ModelConv3D7()\n",
        "conv_3d7.initialize_src_path(main_folder)\n",
        "conv_3d7.initialize_image_properties(image_height=120,image_width=120)\n",
        "conv_3d7.initialize_hyperparams(frames_to_sample=16,batch_size=20,num_epochs=25)\n",
        "conv_3d7_model=conv_3d7.define_model(dense_neurons=64,dropout=0.25)\n",
        "conv_3d7_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s63kvjqd3UKl"
      },
      "source": [
        "print(\"Total Params:\", conv_3d7_model.count_params())\n",
        "accuracy_check_model_7=conv_3d7.train_model(conv_3d7_model,augment_data=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBJGgwaa3kOR"
      },
      "source": [
        "plot(accuracy_check_model_7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bBbPYK-RgAh"
      },
      "source": [
        "conv_3d7.clear_session(conv_3d7_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OW55j6S4f5y"
      },
      "source": [
        "## Model 8 - reducing the number of parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtU-6-0v4e12"
      },
      "source": [
        "class ModelConv3D8(ModelBuilder):\n",
        "    \n",
        "    def define_model(self,dense_neurons=64,dropout=0.25):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Conv3D(8, (3, 3, 3), padding='same',\n",
        "        input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(16, (3, 3, 3), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(32, (2, 2, 2), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(64, (2, 2, 2), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "        \n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(self.num_classes,activation='softmax'))\n",
        "\n",
        "        optimiser = optimizers.Adam(lr=0.0002)\n",
        "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BC1Vxikp44zW"
      },
      "source": [
        "conv_3d8=ModelConv3D8()\n",
        "conv_3d8.initialize_src_path(main_folder)\n",
        "conv_3d8.initialize_image_properties(image_height=120,image_width=120)\n",
        "conv_3d8.initialize_hyperparams(frames_to_sample=16,batch_size=20,num_epochs=30)\n",
        "conv_3d8_model=conv_3d8.define_model(dense_neurons=64,dropout=0.25)\n",
        "conv_3d8_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oikZb8Ij5DNw"
      },
      "source": [
        "print(\"Total Params:\", conv_3d8_model.count_params())\n",
        "accuracy_check_model_8=conv_3d8.train_model(conv_3d8_model,augment_data=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ds6Ddq2y5SsD"
      },
      "source": [
        "plot(accuracy_check_model_8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq14-oPeXfA9"
      },
      "source": [
        "conv_3d8.clear_session(conv_3d8_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGRwZahyW2aW"
      },
      "source": [
        "## Model 9 - CNN- LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuOHUaOhW0gJ"
      },
      "source": [
        "class CNNRNN1(ModelBuilder):\n",
        "    \n",
        "    def define_model(self,lstm_cells=64,dense_neurons=64,dropout=0.25):\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(TimeDistributed(Conv2D(16, (3, 3) , padding='same', activation='relu'),\n",
        "        input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
        "        model.add(TimeDistributed(BatchNormalization()))\n",
        "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "        \n",
        "        model.add(TimeDistributed(Conv2D(32, (3, 3) , padding='same', activation='relu')))\n",
        "        model.add(TimeDistributed(BatchNormalization()))\n",
        "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "        \n",
        "        model.add(TimeDistributed(Conv2D(64, (3, 3) , padding='same', activation='relu')))\n",
        "        model.add(TimeDistributed(BatchNormalization()))\n",
        "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "        \n",
        "        model.add(TimeDistributed(Conv2D(128, (3, 3) , padding='same', activation='relu')))\n",
        "        model.add(TimeDistributed(BatchNormalization()))\n",
        "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "        \n",
        "        model.add(TimeDistributed(Conv2D(256, (3, 3) , padding='same', activation='relu')))\n",
        "        model.add(TimeDistributed(BatchNormalization()))\n",
        "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "        \n",
        "        model.add(TimeDistributed(Flatten()))\n",
        "\n",
        "\n",
        "        model.add(LSTM(lstm_cells))\n",
        "        model.add(Dropout(dropout))\n",
        "        \n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(Dropout(dropout))\n",
        "        \n",
        "        model.add(Dense(self.num_classes, activation='softmax'))\n",
        "        optimiser = optimizers.Adam()\n",
        "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCoDy3FXXJj4"
      },
      "source": [
        "cnn_rnn1=CNNRNN1()\n",
        "cnn_rnn1.initialize_src_path(main_folder)\n",
        "cnn_rnn1.initialize_image_properties(image_height=120,image_width=120)\n",
        "cnn_rnn1.initialize_hyperparams(frames_to_sample=18,batch_size=20,num_epochs=20)\n",
        "cnn_rnn1_model=cnn_rnn1.define_model(lstm_cells=128,dense_neurons=128,dropout=0.25)\n",
        "cnn_rnn1_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-t4EsndXVii"
      },
      "source": [
        "print(\"Total Params:\", cnn_rnn1_model.count_params())\n",
        "accuracy_check_model_9=cnn_rnn1.train_model(cnn_rnn1_model,augment_data=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gm-imdTuXwbq"
      },
      "source": [
        "plot(accuracy_check_model_9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZSjWET3rag4"
      },
      "source": [
        "cnn_rnn1.clear_session(cnn_rnn1_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gdy0_yQzOn7y"
      },
      "source": [
        "## Applying More Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onOKdsVRO26u"
      },
      "source": [
        "class ModelBuilderMoreAugmentation(metaclass= abc.ABCMeta):\n",
        "    \n",
        "    def initialize_src_path(self,main_folder):\n",
        "        self.train_doc = np.random.permutation(open(main_folder + '/' + 'train.csv').readlines())\n",
        "        self.val_doc = np.random.permutation(open(main_folder + '/' + 'val.csv').readlines())\n",
        "        self.train_path = main_folder + '/' + 'train'\n",
        "        self.val_path =  main_folder + '/' + 'val'\n",
        "        self.num_train_sequences = len(self.train_doc)\n",
        "        self.num_val_sequences = len(self.val_doc)\n",
        "        \n",
        "    def initialize_image_properties(self,image_height=100,image_width=100):\n",
        "        self.image_height=image_height\n",
        "        self.image_width=image_width\n",
        "        self.channels=3\n",
        "        self.num_classes=5\n",
        "        self.total_frames=30\n",
        "          \n",
        "    def initialize_hyperparams(self,frames_to_sample=30,batch_size=20,num_epochs=20):\n",
        "        self.frames_to_sample=frames_to_sample\n",
        "        self.batch_size=batch_size\n",
        "        self.num_epochs=num_epochs\n",
        "        \n",
        "        \n",
        "    def generator(self,source_path, folder_list, augment=False):\n",
        "        img_idx = np.round(np.linspace(0,self.total_frames-1,self.frames_to_sample)).astype(int)\n",
        "        batch_size=self.batch_size\n",
        "        while True:\n",
        "            t = np.random.permutation(folder_list)\n",
        "            num_batches = len(t)//batch_size\n",
        "        \n",
        "            for batch in range(num_batches): \n",
        "                batch_data, batch_labels= self.one_batch_data(source_path,t,batch,batch_size,img_idx,augment)\n",
        "                yield batch_data, batch_labels \n",
        "\n",
        "            remaining_seq=len(t)%batch_size\n",
        "        \n",
        "            if (remaining_seq != 0):\n",
        "                batch_data, batch_labels= self.one_batch_data(source_path,t,num_batches,batch_size,img_idx,augment,remaining_seq)\n",
        "                yield batch_data, batch_labels \n",
        "    \n",
        "    \n",
        "    def one_batch_data(self,source_path,t,batch,batch_size,img_idx,augment,remaining_seq=0):\n",
        "    \n",
        "        seq_len = remaining_seq if remaining_seq else batch_size\n",
        "    \n",
        "        batch_data = np.zeros((seq_len,len(img_idx),self.image_height,self.image_width,self.channels)) \n",
        "        batch_labels = np.zeros((seq_len,self.num_classes)) \n",
        "    \n",
        "        if (augment): batch_data_aug = np.zeros((seq_len,len(img_idx),self.image_height,self.image_width,self.channels))\n",
        "\n",
        "        \n",
        "        for folder in range(seq_len): \n",
        "            imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) \n",
        "            for idx,item in enumerate(img_idx): \n",
        "                image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
        "                image_resized=transform.resize(image,(self.image_height,self.image_width,3))\n",
        "            \n",
        "\n",
        "                batch_data[folder,idx,:,:,0] = (image_resized[:,:,0])/255\n",
        "                batch_data[folder,idx,:,:,1] = (image_resized[:,:,1])/255\n",
        "                batch_data[folder,idx,:,:,2] = (image_resized[:,:,2])/255\n",
        "            \n",
        "                if (augment):\n",
        "                    shifted = cv2.warpAffine(image, \n",
        "                                             np.float32([[1, 0, np.random.randint(-30,30)],[0, 1, np.random.randint(-30,30)]]), \n",
        "                                            (image.shape[1], image.shape[0]))\n",
        "                    \n",
        "                    gray = cv2.cvtColor(shifted,cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "                    x0, y0 = np.argwhere(gray > 0).min(axis=0)\n",
        "                    x1, y1 = np.argwhere(gray > 0).max(axis=0) \n",
        "                    \n",
        "                    cropped=shifted[x0:x1,y0:y1,:]\n",
        "                    \n",
        "                    image_resized=transform.resize(cropped,(self.image_height,self.image_width,3))\n",
        "                    \n",
        "                    M = cv2.getRotationMatrix2D((self.image_width//2,self.image_height//2),\n",
        "                                                np.random.randint(-10,10), 1.0)\n",
        "                    rotated = cv2.warpAffine(image_resized, M, (self.image_width, self.image_height))\n",
        "                  \n",
        "                    batch_data_aug[folder,idx,:,:,0] = (rotated[:,:,0])/255\n",
        "                    batch_data_aug[folder,idx,:,:,1] = (rotated[:,:,1])/255\n",
        "                    batch_data_aug[folder,idx,:,:,2] = (rotated[:,:,2])/255\n",
        "                \n",
        "            \n",
        "            batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
        "            \n",
        "    \n",
        "        if (augment):\n",
        "            batch_data=np.concatenate([batch_data,batch_data_aug])\n",
        "            batch_labels=np.concatenate([batch_labels,batch_labels])\n",
        "\n",
        "        \n",
        "        return(batch_data,batch_labels)\n",
        "    \n",
        "    \n",
        "    def train_model(self, model, augment_data=False):\n",
        "        train_generator = self.generator(self.train_path, self.train_doc,augment=augment_data)\n",
        "        val_generator = self.generator(self.val_path, self.val_doc)\n",
        "\n",
        "        model_name = 'model_init' + '_' + str(datetime.datetime.now()).replace(' ','').replace(':','_') + '/'\n",
        "    \n",
        "        if not os.path.exists(model_name):\n",
        "            os.mkdir(model_name)\n",
        "        \n",
        "        filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
        "\n",
        "        checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
        "        LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4)\n",
        "        callbacks_list = [checkpoint, LR]\n",
        "\n",
        "        if (self.num_train_sequences%self.batch_size) == 0:\n",
        "            steps_per_epoch = int(self.num_train_sequences/self.batch_size)\n",
        "        else:\n",
        "            steps_per_epoch = (self.num_train_sequences//self.batch_size) + 1\n",
        "\n",
        "        if (self.num_val_sequences%self.batch_size) == 0:\n",
        "            validation_steps = int(self.num_val_sequences/self.batch_size)\n",
        "        else:\n",
        "            validation_steps = (self.num_val_sequences//self.batch_size) + 1\n",
        "    \n",
        "        history=model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=self.num_epochs, verbose=1, \n",
        "                            callbacks=callbacks_list, validation_data=val_generator, \n",
        "                            validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n",
        "        return history\n",
        "\n",
        "        \n",
        "    @abc.abstractmethod\n",
        "    def define_model(self):\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNCI5V1JPp5N"
      },
      "source": [
        "## Model 10 - (3,3,3) Filter & 160x160 Image resolution - similar to Model 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHkhEeqoPoZ-"
      },
      "source": [
        "class ModelConv3D10(ModelBuilderMoreAugmentation):\n",
        "    \n",
        "    def define_model(self,filtersize=(3,3,3),dense_neurons=64,dropout=0.25):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Conv3D(16, filtersize, padding='same',\n",
        "        input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(32, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(64, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(128, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "\n",
        "        model.add(Dense(self.num_classes,activation='softmax'))\n",
        "\n",
        "        optimiser = optimizers.Adam(lr=0.0002)\n",
        "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zxshqMLQNAK"
      },
      "source": [
        "conv_3d10=ModelConv3D10()\n",
        "conv_3d10.initialize_src_path(main_folder)\n",
        "conv_3d10.initialize_image_properties(image_height=160,image_width=160)\n",
        "conv_3d10.initialize_hyperparams(frames_to_sample=20,batch_size=20,num_epochs=30)\n",
        "conv_3d10_model=conv_3d10.define_model(dense_neurons=256,dropout=0.5)\n",
        "conv_3d10_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Qjf4MvRQSSG"
      },
      "source": [
        "print(\"Total Params:\", conv_3d10_model.count_params())\n",
        "accuracy_check_model_10=conv_3d10.train_model(conv_3d10_model,augment_data=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOdRzLqFpbxp"
      },
      "source": [
        "plot(accuracy_check_model_10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGr80KSVriGs"
      },
      "source": [
        "conv_3d10.clear_session(conv_3d10_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1Nyst3JoisB"
      },
      "source": [
        "## Model 11 - (2,2,2) Filter & 120x120 Image resolution - similar to Model 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HRBTvETolll"
      },
      "source": [
        "class ModelConv3D11(ModelBuilderMoreAugmentation):\n",
        "    \n",
        "    def define_model(self,filtersize=(3,3,3),dense_neurons=64,dropout=0.25):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Conv3D(16, filtersize, padding='same',\n",
        "        input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(32, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(64, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(128, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "\n",
        "        model.add(Dense(self.num_classes,activation='softmax'))\n",
        "\n",
        "        optimiser = optimizers.Adam(lr=0.0002)\n",
        "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WWwgLRypCj3"
      },
      "source": [
        "conv_3d11=ModelConv3D11()\n",
        "conv_3d11.initialize_src_path(main_folder)\n",
        "conv_3d11.initialize_image_properties(image_height=160,image_width=160)\n",
        "conv_3d11.initialize_hyperparams(frames_to_sample=20,batch_size=20,num_epochs=30)\n",
        "conv_3d11_model=conv_3d11.define_model(dense_neurons=256,dropout=0.5)\n",
        "conv_3d11_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUG3v2hOpJYd"
      },
      "source": [
        "print(\"Total Params:\", conv_3d11_model.count_params())\n",
        "accuracy_check_model_11=conv_3d11.train_model(conv_3d11_model,augment_data=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlA0dEQ8puAY"
      },
      "source": [
        "plot(accuracy_check_model_11)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWgA-0hcrp5V"
      },
      "source": [
        "conv_3d11.clear_session(conv_3d11_model)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}