{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Gesture Recognition.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/subhabrata-ghosh-1988/Gesture-Recognition/blob/main/Gesture%20Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7ZfgtjxNVrR"
      },
      "source": [
        "# Gesture Recognition\n",
        "In this group project, you are going to build a 3D Conv model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-eH5pKuNVrV"
      },
      "source": [
        "## Import libraries\n",
        "import numpy as np\n",
        "import os\n",
        "from scipy import misc\n",
        "from imageio import imread\n",
        "import cv2\n",
        "from skimage import transform,io\n",
        "import datetime\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import abc\n",
        "from sys import getsizeof\n",
        "import shutil\n",
        "import abc\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J94bqUhVNVrW"
      },
      "source": [
        "import glob, os , shutil\n",
        "for f in glob.glob(\"/content/model_init*\"):\n",
        "    shutil.rmtree(f)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0Ubj-HfSRY3"
      },
      "source": [
        "## remove the existing zip file\n",
        "shutil.rmtree('/content/Project_data.zip', ignore_errors=True)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4ty5a19SiNl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "outputId": "35bc7460-998e-49dd-f227-863fe1dc940a"
      },
      "source": [
        "## Initiate the file download\n",
        "!pip install gdown\n",
        "import gdown\n",
        "url=\"https://drive.google.com/uc?id=1kM4V7pnLjGbuCaDpfBNHig99gr2rdqyJ\"\n",
        "output = \"Project_data.zip\"\n",
        "gdown.download(url, output, quiet=False)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (3.6.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.41.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2.10)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1kM4V7pnLjGbuCaDpfBNHig99gr2rdqyJ\n",
            "To: /content/Project_data.zip\n",
            "1.71GB [00:11, 143MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Project_data.zip'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-fwe5jbTX8r"
      },
      "source": [
        "## unzip the downloaded folder\n",
        "shutil.rmtree('/content/Project_data', ignore_errors=True)\n",
        "shutil.unpack_archive(\"Project_data.zip\", \"Project_data\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIQHPGi_NVrX"
      },
      "source": [
        "We set the random seed so that the results don't vary drastically."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ci_N-NnWNVrZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a2cc83b-0725-4a91-c2ea-5f4f2e05cffc"
      },
      "source": [
        "np.random.seed(30)\n",
        "import random as rn\n",
        "rn.seed(30)\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(30)\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "print(gpus)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bWyzW_oNVrZ"
      },
      "source": [
        "**In this block, you read the folder names for training and validation. You also set the batch_size here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyTupvi3NVrb"
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation\n",
        "from keras.layers.convolutional import Conv3D, MaxPooling3D, Conv2D, MaxPooling2D\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras import optimizers\n",
        "from keras.layers import Dropout"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCjeEg_tNVrc"
      },
      "source": [
        "# the entire dataset is placed in below directory\n",
        "main_folder='/content/Project_data/Project_data'"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSUMkqaSXcC4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "73b33ac5-326d-4c92-ef4f-8991185b4af9"
      },
      "source": [
        "## Checking current TF version\n",
        "tf.version.VERSION"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REgxbVunNVrd"
      },
      "source": [
        "### Plotting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3UJmY03NVre"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot(Model):\n",
        "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,4))\n",
        "    axes[0].plot(Model.history['loss'])   \n",
        "    axes[0].plot(Model.history['val_loss'])\n",
        "    axes[0].legend(['loss','val_loss'])\n",
        "\n",
        "    axes[1].plot(Model.history['categorical_accuracy'])   \n",
        "    axes[1].plot(Model.history['val_categorical_accuracy'])\n",
        "    axes[1].legend(['categorical_accuracy','val_categorical_accuracy'])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtV6apQQNVrf"
      },
      "source": [
        "## Generator\n",
        "This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haSnpGsboLKX"
      },
      "source": [
        "import gc\n",
        "class GCCallback(tf.keras.callbacks.Callback):\n",
        "      def on_epoch_end(self, epoch, logs=None):\n",
        "          print(gc.collect())"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YC-BRedNVri"
      },
      "source": [
        "# referred ABC library use case from this link https://riptutorial.com/python/example/23083/why-how-to-use-abcmeta-and--abstractmethod\n",
        "\n",
        "class ModelBuilder(metaclass= abc.ABCMeta):\n",
        "    \n",
        "    def initialize_src_path(self,main_folder):\n",
        "        self.train_doc = np.random.permutation(open(main_folder + '/' + 'train.csv').readlines())\n",
        "        self.val_doc = np.random.permutation(open(main_folder + '/' + 'val.csv').readlines())\n",
        "        self.train_path = main_folder + '/' + 'train'\n",
        "        self.val_path =  main_folder + '/' + 'val'\n",
        "        self.num_train_sequences = len(self.train_doc)\n",
        "        self.num_val_sequences = len(self.val_doc)\n",
        "        \n",
        "    def initialize_image_properties(self,image_height=100,image_width=100):\n",
        "        self.image_height=image_height\n",
        "        self.image_width=image_width\n",
        "        self.channels=3\n",
        "        self.num_classes=5\n",
        "        self.total_frames=30\n",
        "          \n",
        "    def initialize_hyperparams(self,frames_to_sample=30,batch_size=40,num_epochs=20):\n",
        "        self.frames_to_sample=frames_to_sample\n",
        "        self.batch_size=batch_size\n",
        "        self.num_epochs=num_epochs\n",
        "        \n",
        "        \n",
        "    def generator(self,source_path, folder_list, augment=False):\n",
        "        img_idx = np.round(np.linspace(0,self.total_frames-1,self.frames_to_sample)).astype(int)\n",
        "        batch_size=self.batch_size\n",
        "        while True:\n",
        "            t = np.random.permutation(folder_list)\n",
        "            num_batches = len(t)//batch_size\n",
        "        \n",
        "            for batch in range(num_batches): \n",
        "                batch_data, batch_labels= self.one_batch_data(source_path,t,batch,batch_size,img_idx,augment)\n",
        "                yield batch_data, batch_labels \n",
        "\n",
        "            remaining_seq=len(t)%batch_size\n",
        "        \n",
        "            if (remaining_seq != 0):\n",
        "                batch_data, batch_labels= self.one_batch_data(source_path,t,num_batches,batch_size,img_idx,augment,remaining_seq)\n",
        "                yield batch_data, batch_labels \n",
        "    \n",
        "    \n",
        "    def one_batch_data(self,source_path,t,batch,batch_size,img_idx,augment,remaining_seq=0):\n",
        "    \n",
        "        seq_len = remaining_seq if remaining_seq else batch_size\n",
        "    \n",
        "        batch_data = np.zeros((seq_len,len(img_idx),self.image_height,self.image_width,self.channels)) \n",
        "        batch_labels = np.zeros((seq_len,self.num_classes)) \n",
        "    \n",
        "        if (augment): batch_data_aug = np.zeros((seq_len,len(img_idx),self.image_height,self.image_width,self.channels))\n",
        "\n",
        "        \n",
        "        for folder in range(seq_len): \n",
        "            imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) \n",
        "            for idx,item in enumerate(img_idx): \n",
        "                image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
        "                image_resized=transform.resize(image,(self.image_height,self.image_width,3))\n",
        "            \n",
        "\n",
        "                batch_data[folder,idx,:,:,0] = (image_resized[:,:,0])/255\n",
        "                batch_data[folder,idx,:,:,1] = (image_resized[:,:,1])/255\n",
        "                batch_data[folder,idx,:,:,2] = (image_resized[:,:,2])/255\n",
        "            \n",
        "                if (augment):\n",
        "                    shifted = cv2.warpAffine(image, \n",
        "                                             np.float32([[1, 0, np.random.randint(-30,30)],[0, 1, np.random.randint(-30,30)]]), \n",
        "                                            (image.shape[1], image.shape[0]))\n",
        "                    \n",
        "                    gray = cv2.cvtColor(shifted,cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "                    x0, y0 = np.argwhere(gray > 0).min(axis=0)\n",
        "                    x1, y1 = np.argwhere(gray > 0).max(axis=0) \n",
        "                    \n",
        "                    cropped=shifted[x0:x1,y0:y1,:]\n",
        "                    \n",
        "                    image_resized=transform.resize(cropped,(self.image_height,self.image_width,3))\n",
        "            \n",
        "                    batch_data_aug[folder,idx,:,:,0] = (image_resized[:,:,0])/255\n",
        "                    batch_data_aug[folder,idx,:,:,1] = (image_resized[:,:,1])/255\n",
        "                    batch_data_aug[folder,idx,:,:,2] = (image_resized[:,:,2])/255\n",
        "                \n",
        "            \n",
        "            batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
        "            \n",
        "    \n",
        "        if (augment):\n",
        "            batch_data=np.concatenate([batch_data,batch_data_aug])\n",
        "            batch_labels=np.concatenate([batch_labels,batch_labels])\n",
        "\n",
        "        \n",
        "        return(batch_data,batch_labels)\n",
        "    \n",
        "    \n",
        "    def train_model(self, model, augment_data=False):\n",
        "        train_generator = self.generator(self.train_path, self.train_doc,augment=augment_data)\n",
        "        val_generator = self.generator(self.val_path, self.val_doc)\n",
        "\n",
        "        model_name = 'model_init' + '_' + str(datetime.datetime.now()).replace(' ','').replace(':','_') + '/'\n",
        "    \n",
        "        if not os.path.exists(model_name):\n",
        "            os.mkdir(model_name)\n",
        "        \n",
        "        filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
        "\n",
        "        checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto',save_freq='epoch')\n",
        "        LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4)\n",
        "        callbacks_list = [checkpoint, LR , GCCallback()]\n",
        "\n",
        "        if (self.num_train_sequences%self.batch_size) == 0:\n",
        "            steps_per_epoch = int(self.num_train_sequences/self.batch_size)\n",
        "        else:\n",
        "            steps_per_epoch = (self.num_train_sequences//self.batch_size) + 1\n",
        "\n",
        "        if (self.num_val_sequences%self.batch_size) == 0:\n",
        "            validation_steps = int(self.num_val_sequences/self.batch_size)\n",
        "        else:\n",
        "            validation_steps = (self.num_val_sequences//self.batch_size) + 1\n",
        "    \n",
        "        history=model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=self.num_epochs, verbose=1, \n",
        "                            callbacks=callbacks_list, validation_data=val_generator, \n",
        "                            validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n",
        "        return history\n",
        "\n",
        "    def clear_session(self, model):\n",
        "        del model\n",
        "        gc.collect()\n",
        "        tf.keras.backend.clear_session()\n",
        "        tf.compat.v1.reset_default_graph() # TF graph isn't same as Keras graph\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def define_model(self):\n",
        "        pass\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkdCou-eNVri"
      },
      "source": [
        "## Sample Model\n",
        "Here you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1XR94jpNVrk"
      },
      "source": [
        "class ModelConv3D1(ModelBuilder):\n",
        "    \n",
        "    def define_model(self):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Conv3D(16, (3, 3, 3), padding='same',\n",
        "        input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(32, (2, 2, 2), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(64, (2, 2, 2), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(128, (2, 2, 2), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(128,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.5))\n",
        "\n",
        "        model.add(Dense(64,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.25))\n",
        "\n",
        "\n",
        "        model.add(Dense(self.num_classes,activation='softmax'))\n",
        "\n",
        "        optimiser = optimizers.Adam()\n",
        "        #optimiser = 'sgd'\n",
        "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "        return model"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PIGA0cbNVrl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adf53a68-0895-4b39-e067-9cc7065004db"
      },
      "source": [
        "Conv3D1=ModelConv3D1()\n",
        "Conv3D1.initialize_src_path(main_folder)\n",
        "Conv3D1.initialize_image_properties(image_height=160,image_width=160)\n",
        "Conv3D1.initialize_hyperparams(frames_to_sample=30,batch_size=30,num_epochs=1)\n",
        "Conv3D1_model=Conv3D1.define_model()\n",
        "Conv3D1_model.summary()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv3d (Conv3D)              (None, 30, 160, 160, 16)  1312      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 30, 160, 160, 16)  0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 30, 160, 160, 16)  64        \n",
            "_________________________________________________________________\n",
            "max_pooling3d (MaxPooling3D) (None, 15, 80, 80, 16)    0         \n",
            "_________________________________________________________________\n",
            "conv3d_1 (Conv3D)            (None, 15, 80, 80, 32)    4128      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 15, 80, 80, 32)    0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 15, 80, 80, 32)    128       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_1 (MaxPooling3 (None, 7, 40, 40, 32)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_2 (Conv3D)            (None, 7, 40, 40, 64)     16448     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 7, 40, 40, 64)     0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 7, 40, 40, 64)     256       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_2 (MaxPooling3 (None, 3, 20, 20, 64)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_3 (Conv3D)            (None, 3, 20, 20, 128)    65664     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 3, 20, 20, 128)    0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 3, 20, 20, 128)    512       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_3 (MaxPooling3 (None, 1, 10, 10, 128)    0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 12800)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               1638528   \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 5)                 325       \n",
            "=================================================================\n",
            "Total params: 1,736,389\n",
            "Trainable params: 1,735,525\n",
            "Non-trainable params: 864\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eU6XuXdNVrm",
        "outputId": "958fb8ef-f225-4902-a27d-a3c58ff8b50d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "Conv3D1.train_model(Conv3D1_model)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "23/23 [==============================] - ETA: 0s - batch: 11.0000 - size: 28.8261 - loss: 1.4912 - categorical_accuracy: 0.4284\n",
            "Epoch 00001: saving model to model_init_2021-03-2108_26_18.607216/model-00001-1.49124-0.42836-4.86878-0.21000.h5\n",
            "0\n",
            "23/23 [==============================] - 274s 11s/step - batch: 11.0000 - size: 28.8261 - loss: 1.4912 - categorical_accuracy: 0.4284 - val_loss: 4.8688 - val_categorical_accuracy: 0.2100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc14f727c90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaJ-270dCcnY"
      },
      "source": [
        "Conv3D1.clear_session(Conv3D1_model)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPGUJrJXNVrn",
        "outputId": "72cd7eff-5f18-4b57-ae56-32d341d67867",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "Conv3D1_bs40=ModelConv3D1()\n",
        "Conv3D1_bs40.initialize_src_path(main_folder)\n",
        "Conv3D1_bs40.initialize_image_properties(image_height=160,image_width=160)\n",
        "Conv3D1_bs40.initialize_hyperparams(frames_to_sample=30,batch_size=40,num_epochs=1)\n",
        "Conv3D1_model_bs40=Conv3D1_bs40.define_model()\n",
        "Conv3D1_model_bs40.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv3d (Conv3D)              (None, 30, 160, 160, 16)  1312      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 30, 160, 160, 16)  0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 30, 160, 160, 16)  64        \n",
            "_________________________________________________________________\n",
            "max_pooling3d (MaxPooling3D) (None, 15, 80, 80, 16)    0         \n",
            "_________________________________________________________________\n",
            "conv3d_1 (Conv3D)            (None, 15, 80, 80, 32)    4128      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 15, 80, 80, 32)    0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 15, 80, 80, 32)    128       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_1 (MaxPooling3 (None, 7, 40, 40, 32)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_2 (Conv3D)            (None, 7, 40, 40, 64)     16448     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 7, 40, 40, 64)     0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 7, 40, 40, 64)     256       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_2 (MaxPooling3 (None, 3, 20, 20, 64)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_3 (Conv3D)            (None, 3, 20, 20, 128)    65664     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 3, 20, 20, 128)    0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 3, 20, 20, 128)    512       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_3 (MaxPooling3 (None, 1, 10, 10, 128)    0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 12800)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               1638528   \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 5)                 325       \n",
            "=================================================================\n",
            "Total params: 1,736,389\n",
            "Trainable params: 1,735,525\n",
            "Non-trainable params: 864\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHCAyYjiNVro"
      },
      "source": [
        "#### Got below memory exhaust error with image resolution of 160x160, 30 frames and a batch_size of 40\n",
        "ResourceExhaustedError:  OOM when allocating tensor with shape[40,16,15,80,80] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
        "\t [[node gradient_tape/sequential_2/max_pooling3d_8/MaxPool3D/MaxPool3DGrad (defined at <ipython-input-11-c85facc09113>:122) ]]\n",
        "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
        " [Op:__inference_train_function_7489]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7SaHnFVNVro",
        "outputId": "26b08edd-b93c-4881-8b91-a40372e13eab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"Memory util is {} Gigs\". format(getsizeof(np.zeros((40,16,30,160,160)))/(1024*1024*1024)))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Memory util is 3.662109524011612 Gigs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSsKCHtZNVrp",
        "outputId": "9f99bbc0-5d41-4f28-d232-05b8b57cf771",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "conv_3d1=ModelConv3D1()\n",
        "conv_3d1.initialize_src_path(main_folder)\n",
        "conv_3d1.initialize_image_properties(image_height=160,image_width=160)\n",
        "conv_3d1.initialize_hyperparams(frames_to_sample=16,batch_size=30,num_epochs=3)\n",
        "conv_3d1_model=conv_3d1.define_model()\n",
        "print(\"Total Params:\", conv_3d1_model.count_params())\n",
        "conv_3d1.train_model(conv_3d1_model)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Params: 1736389\n",
            "Epoch 1/3\n",
            "23/23 [==============================] - ETA: 0s - batch: 11.0000 - size: 28.8261 - loss: 1.4302 - categorical_accuracy: 0.4540\n",
            "Epoch 00001: saving model to model_init_2021-03-2108_31_06.879816/model-00001-1.43021-0.45400-2.03139-0.21000.h5\n",
            "0\n",
            "23/23 [==============================] - 161s 7s/step - batch: 11.0000 - size: 28.8261 - loss: 1.4302 - categorical_accuracy: 0.4540 - val_loss: 2.0314 - val_categorical_accuracy: 0.2100\n",
            "Epoch 2/3\n",
            "23/23 [==============================] - ETA: 0s - batch: 11.0000 - size: 28.8261 - loss: 0.9001 - categorical_accuracy: 0.6621\n",
            "Epoch 00002: saving model to model_init_2021-03-2108_31_06.879816/model-00002-0.90008-0.66214-4.21787-0.17000.h5\n",
            "0\n",
            "23/23 [==============================] - 121s 5s/step - batch: 11.0000 - size: 28.8261 - loss: 0.9001 - categorical_accuracy: 0.6621 - val_loss: 4.2179 - val_categorical_accuracy: 0.1700\n",
            "Epoch 3/3\n",
            "23/23 [==============================] - ETA: 0s - batch: 11.0000 - size: 28.8261 - loss: 0.7425 - categorical_accuracy: 0.7044\n",
            "Epoch 00003: saving model to model_init_2021-03-2108_31_06.879816/model-00003-0.74247-0.70437-5.32029-0.24000.h5\n",
            "0\n",
            "23/23 [==============================] - 123s 6s/step - batch: 11.0000 - size: 28.8261 - loss: 0.7425 - categorical_accuracy: 0.7044 - val_loss: 5.3203 - val_categorical_accuracy: 0.2400\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc0c5322750>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jx0kjX0PNVrr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ffe5561-5263-44bb-d898-1eb260a7ae66"
      },
      "source": [
        "conv_3d1.clear_session(conv_3d1_model)\n",
        "conv_3d1=ModelConv3D1()\n",
        "conv_3d1.initialize_src_path(main_folder)\n",
        "conv_3d1.initialize_image_properties(image_height=100,image_width=100)\n",
        "conv_3d1.initialize_hyperparams(frames_to_sample=30,batch_size=30,num_epochs=2)\n",
        "conv_3d1_model=conv_3d1.define_model()\n",
        "print(\"Total Params:\", conv_3d1_model.count_params())\n",
        "conv_3d1.train_model(conv_3d1_model)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Params: 687813\n",
            "Epoch 1/2\n",
            "23/23 [==============================] - ETA: 0s - batch: 11.0000 - size: 28.8261 - loss: 1.6461 - categorical_accuracy: 0.3771\n",
            "Epoch 00001: saving model to model_init_2021-03-2108_37_54.227943/model-00001-1.64606-0.37707-1.90597-0.18000.h5\n",
            "48\n",
            "23/23 [==============================] - 230s 10s/step - batch: 11.0000 - size: 28.8261 - loss: 1.6461 - categorical_accuracy: 0.3771 - val_loss: 1.9060 - val_categorical_accuracy: 0.1800\n",
            "Epoch 2/2\n",
            "23/23 [==============================] - ETA: 0s - batch: 11.0000 - size: 28.8261 - loss: 1.2087 - categorical_accuracy: 0.5596\n",
            "Epoch 00002: saving model to model_init_2021-03-2108_37_54.227943/model-00002-1.20872-0.55958-2.55964-0.26000.h5\n",
            "0\n",
            "23/23 [==============================] - 197s 9s/step - batch: 11.0000 - size: 28.8261 - loss: 1.2087 - categorical_accuracy: 0.5596 - val_loss: 2.5596 - val_categorical_accuracy: 0.2600\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc0ce0badd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xReB_wtlNVrx",
        "outputId": "87795f55-b706-460b-b3e0-92896a085279",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "conv_3d1.clear_session(conv_3d1_model)\n",
        "conv_3d1=ModelConv3D1()\n",
        "conv_3d1.initialize_src_path(main_folder)\n",
        "conv_3d1.initialize_image_properties(image_height=100,image_width=100)\n",
        "conv_3d1.initialize_hyperparams(frames_to_sample=30,batch_size=60,num_epochs=2)\n",
        "conv_3d1_model=conv_3d1.define_model()\n",
        "print(\"Total Params:\", conv_3d1_model.count_params())\n",
        "conv_3d1.train_model(conv_3d1_model)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Params: 687813\n",
            "Epoch 1/2\n",
            "12/12 [==============================] - ETA: 0s - batch: 5.5000 - size: 55.2500 - loss: 1.6515 - categorical_accuracy: 0.3529 \n",
            "Epoch 00001: saving model to model_init_2021-03-2108_45_03.333779/model-00001-1.65146-0.35294-1.57691-0.27000.h5\n",
            "48\n",
            "12/12 [==============================] - 231s 19s/step - batch: 5.5000 - size: 55.2500 - loss: 1.6515 - categorical_accuracy: 0.3529 - val_loss: 1.5769 - val_categorical_accuracy: 0.2700\n",
            "Epoch 2/2\n",
            "12/12 [==============================] - ETA: 0s - batch: 5.5000 - size: 55.2500 - loss: 1.1992 - categorical_accuracy: 0.5505 \n",
            "Epoch 00002: saving model to model_init_2021-03-2108_45_03.333779/model-00002-1.19918-0.55053-2.01046-0.19000.h5\n",
            "0\n",
            "12/12 [==============================] - 202s 18s/step - batch: 5.5000 - size: 55.2500 - loss: 1.1992 - categorical_accuracy: 0.5505 - val_loss: 2.0105 - val_categorical_accuracy: 0.1900\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc1128a1c90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoKopL3YNVry",
        "outputId": "e8c519f4-268c-41f7-e43e-7f1999ed1a9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "conv_3d1.clear_session(conv_3d1_model)\n",
        "conv_3d1=ModelConv3D1()\n",
        "conv_3d1.initialize_src_path(main_folder)\n",
        "conv_3d1.initialize_image_properties(image_height=100,image_width=100)\n",
        "conv_3d1.initialize_hyperparams(frames_to_sample=16,batch_size=60,num_epochs=2)\n",
        "conv_3d1_model=conv_3d1.define_model()\n",
        "print(\"Total Params:\", conv_3d1_model.count_params())\n",
        "conv_3d1.train_model(conv_3d1_model)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Params: 687813\n",
            "Epoch 1/2\n",
            "12/12 [==============================] - ETA: 0s - batch: 5.5000 - size: 55.2500 - loss: 1.6753 - categorical_accuracy: 0.4057\n",
            "Epoch 00001: saving model to model_init_2021-03-2108_52_18.162235/model-00001-1.67529-0.40573-1.59577-0.30000.h5\n",
            "48\n",
            "12/12 [==============================] - 128s 10s/step - batch: 5.5000 - size: 55.2500 - loss: 1.6753 - categorical_accuracy: 0.4057 - val_loss: 1.5958 - val_categorical_accuracy: 0.3000\n",
            "Epoch 2/2\n",
            "11/12 [==========================>...] - ETA: 8s - batch: 5.0000 - size: 60.0000 - loss: 1.2168 - categorical_accuracy: 0.5136 \n",
            "Epoch 00002: saving model to model_init_2021-03-2108_52_18.162235/model-00002-1.23956-0.51282-2.23712-0.17000.h5\n",
            "0\n",
            "12/12 [==============================] - 107s 10s/step - batch: 5.5000 - size: 55.2500 - loss: 1.2396 - categorical_accuracy: 0.5128 - val_loss: 2.2371 - val_categorical_accuracy: 0.1700\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc112379e10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHABNT4GNVry",
        "outputId": "099c9046-18f3-41b2-d04b-9529404bf1d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "conv_3d1.clear_session(conv_3d1_model)\n",
        "conv_3d1=ModelConv3D1()\n",
        "conv_3d1.initialize_src_path(main_folder)\n",
        "conv_3d1.initialize_image_properties(image_height=100,image_width=100)\n",
        "conv_3d1.initialize_hyperparams(frames_to_sample=16,batch_size=80,num_epochs=2)\n",
        "conv_3d1_model=conv_3d1.define_model()\n",
        "print(\"Total Params:\", conv_3d1_model.count_params())\n",
        "conv_3d1.train_model(conv_3d1_model)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Params: 687813\n",
            "Epoch 1/2\n",
            "9/9 [==============================] - ETA: 0s - batch: 4.0000 - size: 73.6667 - loss: 1.7428 - categorical_accuracy: 0.3620 \n",
            "Epoch 00001: saving model to model_init_2021-03-2108_56_15.459566/model-00001-1.74278-0.36199-1.69829-0.21000.h5\n",
            "48\n",
            "9/9 [==============================] - 125s 13s/step - batch: 4.0000 - size: 73.6667 - loss: 1.7428 - categorical_accuracy: 0.3620 - val_loss: 1.6983 - val_categorical_accuracy: 0.2100\n",
            "Epoch 2/2\n",
            "9/9 [==============================] - ETA: 0s - batch: 4.0000 - size: 73.6667 - loss: 1.1840 - categorical_accuracy: 0.5400 \n",
            "Epoch 00002: saving model to model_init_2021-03-2108_56_15.459566/model-00002-1.18396-0.53997-3.00948-0.16000.h5\n",
            "0\n",
            "9/9 [==============================] - 108s 13s/step - batch: 4.0000 - size: 73.6667 - loss: 1.1840 - categorical_accuracy: 0.5400 - val_loss: 3.0095 - val_categorical_accuracy: 0.1600\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc0c758d390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2T9W4HyNVrz",
        "outputId": "f4a96442-add8-470b-e16b-a792438ba9ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "conv_3d1.clear_session(conv_3d1_model)\n",
        "conv_3d1=ModelConv3D1()\n",
        "conv_3d1.initialize_src_path(main_folder)\n",
        "conv_3d1.initialize_image_properties(image_height=160,image_width=160)\n",
        "conv_3d1.initialize_hyperparams(frames_to_sample=30,batch_size=15,num_epochs=2)\n",
        "conv_3d1_model=conv_3d1.define_model()\n",
        "print(\"Total Params:\", conv_3d1_model.count_params())\n",
        "conv_3d1.train_model(conv_3d1_model)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Params: 1736389\n",
            "Epoch 1/2\n",
            "45/45 [==============================] - ETA: 0s - batch: 22.0000 - size: 14.7333 - loss: 1.5440 - categorical_accuracy: 0.4118\n",
            "Epoch 00001: saving model to model_init_2021-03-2109_00_10.907179/model-00001-1.54404-0.41176-4.48453-0.16000.h5\n",
            "48\n",
            "45/45 [==============================] - 266s 6s/step - batch: 22.0000 - size: 14.7333 - loss: 1.5440 - categorical_accuracy: 0.4118 - val_loss: 4.4845 - val_categorical_accuracy: 0.1600\n",
            "Epoch 2/2\n",
            "45/45 [==============================] - ETA: 0s - batch: 22.0000 - size: 14.7333 - loss: 1.2099 - categorical_accuracy: 0.5611\n",
            "Epoch 00002: saving model to model_init_2021-03-2109_00_10.907179/model-00002-1.20990-0.56109-6.74839-0.26000.h5\n",
            "0\n",
            "45/45 [==============================] - 225s 5s/step - batch: 22.0000 - size: 14.7333 - loss: 1.2099 - categorical_accuracy: 0.5611 - val_loss: 6.7484 - val_categorical_accuracy: 0.2600\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc0c606ab50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25WU3xHcNVrz",
        "outputId": "f7d7c24d-9bc7-419e-8256-d7bf93a10d58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "conv_3d1.clear_session(conv_3d1_model)\n",
        "conv_3d1=ModelConv3D1()\n",
        "conv_3d1.initialize_src_path(main_folder)\n",
        "conv_3d1.initialize_image_properties(image_height=160,image_width=160)\n",
        "conv_3d1.initialize_hyperparams(frames_to_sample=16,batch_size=15,num_epochs=2)\n",
        "conv_3d1_model=conv_3d1.define_model()\n",
        "print(\"Total Params:\", conv_3d1_model.count_params())\n",
        "conv_3d1.train_model(conv_3d1_model)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Params: 1736389\n",
            "Epoch 1/2\n",
            "45/45 [==============================] - ETA: 0s - batch: 22.0000 - size: 14.7333 - loss: 1.5820 - categorical_accuracy: 0.4027\n",
            "Epoch 00001: saving model to model_init_2021-03-2109_08_24.499191/model-00001-1.58195-0.40271-6.01646-0.16000.h5\n",
            "48\n",
            "45/45 [==============================] - 143s 3s/step - batch: 22.0000 - size: 14.7333 - loss: 1.5820 - categorical_accuracy: 0.4027 - val_loss: 6.0165 - val_categorical_accuracy: 0.1600\n",
            "Epoch 2/2\n",
            "45/45 [==============================] - ETA: 0s - batch: 22.0000 - size: 14.7333 - loss: 1.0770 - categorical_accuracy: 0.6078\n",
            "Epoch 00002: saving model to model_init_2021-03-2109_08_24.499191/model-00002-1.07701-0.60784-12.25964-0.21000.h5\n",
            "0\n",
            "45/45 [==============================] - 122s 3s/step - batch: 22.0000 - size: 14.7333 - loss: 1.0770 - categorical_accuracy: 0.6078 - val_loss: 12.2596 - val_categorical_accuracy: 0.2100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc0c7dfe3d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPzFJDfUNVr0",
        "outputId": "6006da32-3094-4197-d18c-8525c111f7f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "conv_3d1.clear_session(conv_3d1_model)\n",
        "conv_3d1=ModelConv3D1()\n",
        "conv_3d1.initialize_src_path(main_folder)\n",
        "conv_3d1.initialize_image_properties(image_height=100,image_width=100)\n",
        "conv_3d1.initialize_hyperparams(frames_to_sample=16,batch_size=15,num_epochs=2)\n",
        "conv_3d1_model=conv_3d1.define_model()\n",
        "print(\"Total Params:\", conv_3d1_model.count_params())\n",
        "conv_3d1.train_model(conv_3d1_model)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Params: 687813\n",
            "Epoch 1/2\n",
            "45/45 [==============================] - ETA: 0s - batch: 22.0000 - size: 14.7333 - loss: 1.6018 - categorical_accuracy: 0.3876\n",
            "Epoch 00001: saving model to model_init_2021-03-2109_12_51.659582/model-00001-1.60176-0.38763-3.83403-0.16000.h5\n",
            "48\n",
            "45/45 [==============================] - 124s 3s/step - batch: 22.0000 - size: 14.7333 - loss: 1.6018 - categorical_accuracy: 0.3876 - val_loss: 3.8340 - val_categorical_accuracy: 0.1600\n",
            "Epoch 2/2\n",
            "45/45 [==============================] - ETA: 0s - batch: 22.0000 - size: 14.7333 - loss: 1.1296 - categorical_accuracy: 0.5475\n",
            "Epoch 00002: saving model to model_init_2021-03-2109_12_51.659582/model-00002-1.12957-0.54751-5.04227-0.14000.h5\n",
            "0\n",
            "45/45 [==============================] - 105s 2s/step - batch: 22.0000 - size: 14.7333 - loss: 1.1296 - categorical_accuracy: 0.5475 - val_loss: 5.0423 - val_categorical_accuracy: 0.1400\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc0c7863390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdQ6Aed-NVr4",
        "outputId": "80f4cbb7-aec4-4c7c-899f-6c5993d1c8f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "conv_3d1.clear_session(conv_3d1_model)\n",
        "conv_3d1=ModelConv3D1()\n",
        "conv_3d1.initialize_src_path(main_folder)\n",
        "conv_3d1.initialize_image_properties(image_height=100,image_width=100)\n",
        "conv_3d1.initialize_hyperparams(frames_to_sample=16,batch_size=10,num_epochs=2)\n",
        "conv_3d1_model=conv_3d1.define_model()\n",
        "print(\"Total Params:\", conv_3d1_model.count_params())\n",
        "conv_3d1.train_model(conv_3d1_model)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Params: 687813\n",
            "Epoch 1/2\n",
            "67/67 [==============================] - ETA: 0s - batch: 33.0000 - size: 9.8955 - loss: 1.5822 - categorical_accuracy: 0.3801 \n",
            "Epoch 00001: saving model to model_init_2021-03-2109_16_42.514505/model-00001-1.58219-0.38009-4.92011-0.21000.h5\n",
            "48\n",
            "67/67 [==============================] - 124s 2s/step - batch: 33.0000 - size: 9.8955 - loss: 1.5822 - categorical_accuracy: 0.3801 - val_loss: 4.9201 - val_categorical_accuracy: 0.2100\n",
            "Epoch 2/2\n",
            "67/67 [==============================] - ETA: 0s - batch: 33.0000 - size: 9.8955 - loss: 1.1958 - categorical_accuracy: 0.5309 \n",
            "Epoch 00002: saving model to model_init_2021-03-2109_16_42.514505/model-00002-1.19579-0.53092-7.04926-0.22000.h5\n",
            "0\n",
            "67/67 [==============================] - 105s 2s/step - batch: 33.0000 - size: 9.8955 - loss: 1.1958 - categorical_accuracy: 0.5309 - val_loss: 7.0493 - val_categorical_accuracy: 0.2200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc0c72badd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OYsvNb7NVr5",
        "outputId": "afd43f55-78d1-4586-e84c-326643a50aaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "conv_3d1.clear_session(conv_3d1_model)\n",
        "conv_3d1=ModelConv3D1()\n",
        "conv_3d1.initialize_src_path(main_folder)\n",
        "conv_3d1.initialize_image_properties(image_height=100,image_width=100)\n",
        "conv_3d1.initialize_hyperparams(frames_to_sample=30,batch_size=10,num_epochs=2)\n",
        "conv_3d1_model=conv_3d1.define_model()\n",
        "print(\"Total Params:\", conv_3d1_model.count_params())\n",
        "conv_3d1.train_model(conv_3d1_model)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Params: 687813\n",
            "Epoch 1/2\n",
            "67/67 [==============================] - ETA: 0s - batch: 33.0000 - size: 9.8955 - loss: 1.5933 - categorical_accuracy: 0.4208 \n",
            "Epoch 00001: saving model to model_init_2021-03-2109_20_33.533250/model-00001-1.59332-0.42081-6.02739-0.16000.h5\n",
            "48\n",
            "67/67 [==============================] - 225s 3s/step - batch: 33.0000 - size: 9.8955 - loss: 1.5933 - categorical_accuracy: 0.4208 - val_loss: 6.0274 - val_categorical_accuracy: 0.1600\n",
            "Epoch 2/2\n",
            "67/67 [==============================] - ETA: 0s - batch: 33.0000 - size: 9.8955 - loss: 1.1371 - categorical_accuracy: 0.5581 \n",
            "Epoch 00002: saving model to model_init_2021-03-2109_20_33.533250/model-00002-1.13714-0.55807-10.45722-0.17000.h5\n",
            "0\n",
            "67/67 [==============================] - 202s 3s/step - batch: 33.0000 - size: 9.8955 - loss: 1.1371 - categorical_accuracy: 0.5581 - val_loss: 10.4572 - val_categorical_accuracy: 0.1700\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc0c6d7fc90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6D4BJx6gNVr6",
        "outputId": "12f2aff5-c717-4684-9d27-083d48884340",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "conv_3d1.clear_session(conv_3d1_model)\n",
        "conv_3d1=ModelConv3D1()\n",
        "conv_3d1.initialize_src_path(main_folder)\n",
        "conv_3d1.initialize_image_properties(image_height=160,image_width=160)\n",
        "conv_3d1.initialize_hyperparams(frames_to_sample=30,batch_size=10,num_epochs=2)\n",
        "conv_3d1_model=conv_3d1.define_model()\n",
        "print(\"Total Params:\", conv_3d1_model.count_params())\n",
        "conv_3d1.train_model(conv_3d1_model)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Params: 1736389\n",
            "Epoch 1/2\n",
            "67/67 [==============================] - ETA: 0s - batch: 33.0000 - size: 9.8955 - loss: 1.6431 - categorical_accuracy: 0.3756 \n",
            "Epoch 00001: saving model to model_init_2021-03-2109_27_42.910688/model-00001-1.64306-0.37557-11.39790-0.16000.h5\n",
            "48\n",
            "67/67 [==============================] - 265s 4s/step - batch: 33.0000 - size: 9.8955 - loss: 1.6431 - categorical_accuracy: 0.3756 - val_loss: 11.3979 - val_categorical_accuracy: 0.1600\n",
            "Epoch 2/2\n",
            "67/67 [==============================] - ETA: 0s - batch: 33.0000 - size: 9.8955 - loss: 1.2571 - categorical_accuracy: 0.5158 \n",
            "Epoch 00002: saving model to model_init_2021-03-2109_27_42.910688/model-00002-1.25708-0.51584-15.40731-0.16000.h5\n",
            "0\n",
            "67/67 [==============================] - 227s 3s/step - batch: 33.0000 - size: 9.8955 - loss: 1.2571 - categorical_accuracy: 0.5158 - val_loss: 15.4073 - val_categorical_accuracy: 0.1600\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc0c64e4dd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oo38TA_HNVr6",
        "outputId": "b88138aa-e1b5-497e-9e8b-7db9082667bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "conv_3d1.clear_session(conv_3d1_model)\n",
        "conv_3d1=ModelConv3D1()\n",
        "conv_3d1.initialize_src_path(main_folder)\n",
        "conv_3d1.initialize_image_properties(image_height=160,image_width=160)\n",
        "conv_3d1.initialize_hyperparams(frames_to_sample=16,batch_size=10,num_epochs=2)\n",
        "conv_3d1_model=conv_3d1.define_model()\n",
        "print(\"Total Params:\", conv_3d1_model.count_params())\n",
        "conv_3d1.train_model(conv_3d1_model)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Params: 1736389\n",
            "Epoch 1/2\n",
            "67/67 [==============================] - ETA: 0s - batch: 33.0000 - size: 9.8955 - loss: 1.5755 - categorical_accuracy: 0.4012 \n",
            "Epoch 00001: saving model to model_init_2021-03-2109_35_56.896391/model-00001-1.57553-0.40121-2.34058-0.30000.h5\n",
            "48\n",
            "67/67 [==============================] - 144s 2s/step - batch: 33.0000 - size: 9.8955 - loss: 1.5755 - categorical_accuracy: 0.4012 - val_loss: 2.3406 - val_categorical_accuracy: 0.3000\n",
            "Epoch 2/2\n",
            "67/67 [==============================] - ETA: 0s - batch: 33.0000 - size: 9.8955 - loss: 1.2382 - categorical_accuracy: 0.5083 \n",
            "Epoch 00002: saving model to model_init_2021-03-2109_35_56.896391/model-00002-1.23819-0.50830-3.09266-0.23000.h5\n",
            "0\n",
            "67/67 [==============================] - 123s 2s/step - batch: 33.0000 - size: 9.8955 - loss: 1.2382 - categorical_accuracy: 0.5083 - val_loss: 3.0927 - val_categorical_accuracy: 0.2300\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc110f34790>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ns4lmR9bNVr7",
        "outputId": "2fdc8ac4-ba88-40a6-a775-486881aff9ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "conv_3d1.clear_session(conv_3d1_model)\n",
        "conv_3d1=ModelConv3D1()\n",
        "conv_3d1.initialize_src_path(main_folder)\n",
        "conv_3d1.initialize_image_properties(image_height=160,image_width=160)\n",
        "conv_3d1.initialize_hyperparams(frames_to_sample=16,batch_size=40,num_epochs=2)\n",
        "conv_3d1_model=conv_3d1.define_model()\n",
        "print(\"Total Params:\", conv_3d1_model.count_params())\n",
        "conv_3d1.train_model(conv_3d1_model)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Params: 1736389\n",
            "Epoch 1/2\n",
            "17/17 [==============================] - ETA: 0s - batch: 8.0000 - size: 39.0000 - loss: 1.6114 - categorical_accuracy: 0.3967\n",
            "Epoch 00001: saving model to model_init_2021-03-2109_40_25.844948/model-00001-1.61140-0.39668-2.02471-0.21000.h5\n",
            "48\n",
            "17/17 [==============================] - 144s 8s/step - batch: 8.0000 - size: 39.0000 - loss: 1.6114 - categorical_accuracy: 0.3967 - val_loss: 2.0247 - val_categorical_accuracy: 0.2100\n",
            "Epoch 2/2\n",
            "17/17 [==============================] - ETA: 0s - batch: 8.0000 - size: 39.0000 - loss: 1.0988 - categorical_accuracy: 0.5732\n",
            "Epoch 00002: saving model to model_init_2021-03-2109_40_25.844948/model-00002-1.09876-0.57315-5.65281-0.21000.h5\n",
            "0\n",
            "17/17 [==============================] - 125s 8s/step - batch: 8.0000 - size: 39.0000 - loss: 1.0988 - categorical_accuracy: 0.5732 - val_loss: 5.6528 - val_categorical_accuracy: 0.2100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc111ed1450>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVh5rIRCNVr7"
      },
      "source": [
        "### Observation\n",
        "\n",
        "**As we see from the above experiments image resolution and number of frames in sequence have more impact on training time than batch_size.**\n",
        "\n",
        "So experimentations are carried with batch size fixed around 15-40 and changing the resolution and number of image per sequence based on the device memory constraints . Models are designed such that their memory foot print is less than 50 MB which corresponds to 4.3 million parameters assuming the datatype size of parameters to be 12 bytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKbxe-Szy15j"
      },
      "source": [
        "## Model 1 - Base Model - No Data Augmentation Batch Size 40 and Epoch 15"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LVQM4NsNVr7"
      },
      "source": [
        "class ModelConv3D1(ModelBuilder):\n",
        "    \n",
        "    def define_model(self,filtersize=(3,3,3),dense_neurons=64,dropout=0.25):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Conv3D(16, filtersize, padding='same',\n",
        "        input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(32, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(64, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(128, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "\n",
        "        model.add(Dense(self.num_classes,activation='softmax'))\n",
        "\n",
        "        optimiser = optimizers.Adam()\n",
        "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "        return model"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-89dV5W8WAw3",
        "outputId": "36e34450-cc68-4c46-a410-734464ccd595",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "conv_3d1.clear_session(conv_3d1_model)\n",
        "Conv3D1=ModelConv3D1()\n",
        "Conv3D1.initialize_src_path(main_folder)\n",
        "Conv3D1.initialize_image_properties(image_height=160,image_width=160)\n",
        "Conv3D1.initialize_hyperparams(frames_to_sample=20,batch_size=40,num_epochs=15)\n",
        "Conv3D1_model=Conv3D1.define_model()\n",
        "Conv3D1_model.summary()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv3d (Conv3D)              (None, 20, 160, 160, 16)  1312      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 20, 160, 160, 16)  0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 20, 160, 160, 16)  64        \n",
            "_________________________________________________________________\n",
            "max_pooling3d (MaxPooling3D) (None, 10, 80, 80, 16)    0         \n",
            "_________________________________________________________________\n",
            "conv3d_1 (Conv3D)            (None, 10, 80, 80, 32)    13856     \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 10, 80, 80, 32)    0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 10, 80, 80, 32)    128       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_1 (MaxPooling3 (None, 5, 40, 40, 32)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_2 (Conv3D)            (None, 5, 40, 40, 64)     55360     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 5, 40, 40, 64)     0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 5, 40, 40, 64)     256       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_2 (MaxPooling3 (None, 2, 20, 20, 64)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_3 (Conv3D)            (None, 2, 20, 20, 128)    221312    \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 2, 20, 20, 128)    0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 2, 20, 20, 128)    512       \n",
            "_________________________________________________________________\n",
            "max_pooling3d_3 (MaxPooling3 (None, 1, 10, 10, 128)    0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 12800)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                819264    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 5)                 325       \n",
            "=================================================================\n",
            "Total params: 1,117,061\n",
            "Trainable params: 1,116,325\n",
            "Non-trainable params: 736\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5j884vjoWQD0",
        "outputId": "cc271ce6-4548-46f5-b20f-388c9317c3a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"Total Params:\", Conv3D1_model.count_params())\n",
        "accuracy_check_model_1 = Conv3D1.train_model(Conv3D1_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Params: 1117061\n",
            "Epoch 1/15\n",
            "17/17 [==============================] - ETA: 0s - batch: 8.0000 - size: 39.0000 - loss: 1.5218 - categorical_accuracy: 0.4314\n",
            "Epoch 00001: saving model to model_init_2021-03-2109_44_56.788820/model-00001-1.52181-0.43137-1.53799-0.31000.h5\n",
            "48\n",
            "17/17 [==============================] - 182s 10s/step - batch: 8.0000 - size: 39.0000 - loss: 1.5218 - categorical_accuracy: 0.4314 - val_loss: 1.5380 - val_categorical_accuracy: 0.3100\n",
            "Epoch 2/15\n",
            "17/17 [==============================] - ETA: 0s - batch: 8.0000 - size: 39.0000 - loss: 0.9892 - categorical_accuracy: 0.6169\n",
            "Epoch 00002: saving model to model_init_2021-03-2109_44_56.788820/model-00002-0.98919-0.61689-2.06962-0.23000.h5\n",
            "0\n",
            "17/17 [==============================] - 155s 10s/step - batch: 8.0000 - size: 39.0000 - loss: 0.9892 - categorical_accuracy: 0.6169 - val_loss: 2.0696 - val_categorical_accuracy: 0.2300\n",
            "Epoch 3/15\n",
            "17/17 [==============================] - ETA: 0s - batch: 8.0000 - size: 39.0000 - loss: 0.7462 - categorical_accuracy: 0.7285\n",
            "Epoch 00003: saving model to model_init_2021-03-2109_44_56.788820/model-00003-0.74623-0.72851-2.68113-0.22000.h5\n",
            "0\n",
            "17/17 [==============================] - 155s 10s/step - batch: 8.0000 - size: 39.0000 - loss: 0.7462 - categorical_accuracy: 0.7285 - val_loss: 2.6811 - val_categorical_accuracy: 0.2200\n",
            "Epoch 4/15\n",
            "17/17 [==============================] - ETA: 0s - batch: 8.0000 - size: 39.0000 - loss: 0.5651 - categorical_accuracy: 0.8024\n",
            "Epoch 00004: saving model to model_init_2021-03-2109_44_56.788820/model-00004-0.56514-0.80241-3.14692-0.21000.h5\n",
            "0\n",
            "17/17 [==============================] - 153s 10s/step - batch: 8.0000 - size: 39.0000 - loss: 0.5651 - categorical_accuracy: 0.8024 - val_loss: 3.1469 - val_categorical_accuracy: 0.2100\n",
            "Epoch 5/15\n",
            "17/17 [==============================] - ETA: 0s - batch: 8.0000 - size: 39.0000 - loss: 0.4357 - categorical_accuracy: 0.8522\n",
            "Epoch 00005: saving model to model_init_2021-03-2109_44_56.788820/model-00005-0.43573-0.85219-4.25654-0.17000.h5\n",
            "\n",
            "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "0\n",
            "17/17 [==============================] - 153s 9s/step - batch: 8.0000 - size: 39.0000 - loss: 0.4357 - categorical_accuracy: 0.8522 - val_loss: 4.2565 - val_categorical_accuracy: 0.1700\n",
            "Epoch 6/15\n",
            "17/17 [==============================] - ETA: 0s - batch: 8.0000 - size: 39.0000 - loss: 0.3409 - categorical_accuracy: 0.8929\n",
            "Epoch 00006: saving model to model_init_2021-03-2109_44_56.788820/model-00006-0.34086-0.89291-3.95901-0.25000.h5\n",
            "0\n",
            "17/17 [==============================] - 159s 10s/step - batch: 8.0000 - size: 39.0000 - loss: 0.3409 - categorical_accuracy: 0.8929 - val_loss: 3.9590 - val_categorical_accuracy: 0.2500\n",
            "Epoch 7/15\n",
            "17/17 [==============================] - ETA: 0s - batch: 8.0000 - size: 39.0000 - loss: 0.2443 - categorical_accuracy: 0.9351\n",
            "Epoch 00007: saving model to model_init_2021-03-2109_44_56.788820/model-00007-0.24433-0.93514-4.03968-0.21000.h5\n",
            "0\n",
            "17/17 [==============================] - 152s 9s/step - batch: 8.0000 - size: 39.0000 - loss: 0.2443 - categorical_accuracy: 0.9351 - val_loss: 4.0397 - val_categorical_accuracy: 0.2100\n",
            "Epoch 8/15\n",
            "17/17 [==============================] - ETA: 0s - batch: 8.0000 - size: 39.0000 - loss: 0.2487 - categorical_accuracy: 0.9427\n",
            "Epoch 00008: saving model to model_init_2021-03-2109_44_56.788820/model-00008-0.24874-0.94268-4.34314-0.21000.h5\n",
            "0\n",
            "17/17 [==============================] - 155s 10s/step - batch: 8.0000 - size: 39.0000 - loss: 0.2487 - categorical_accuracy: 0.9427 - val_loss: 4.3431 - val_categorical_accuracy: 0.2100\n",
            "Epoch 9/15\n",
            "17/17 [==============================] - ETA: 0s - batch: 8.0000 - size: 39.0000 - loss: 0.2038 - categorical_accuracy: 0.9502\n",
            "Epoch 00009: saving model to model_init_2021-03-2109_44_56.788820/model-00009-0.20381-0.95023-3.92892-0.21000.h5\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "0\n",
            "17/17 [==============================] - 156s 10s/step - batch: 8.0000 - size: 39.0000 - loss: 0.2038 - categorical_accuracy: 0.9502 - val_loss: 3.9289 - val_categorical_accuracy: 0.2100\n",
            "Epoch 10/15\n",
            " 7/17 [===========>..................] - ETA: 1:01 - batch: 3.0000 - size: 40.0000 - loss: 0.1727 - categorical_accuracy: 0.9536"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r00TPGbNXXhX"
      },
      "source": [
        "plot(accuracy_check_model_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dtxwayy_14Hn"
      },
      "source": [
        "#### Model is clearly overfitting. So we need to do data augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJk2twS_2lRx"
      },
      "source": [
        "## Model 2 - Augment Data , (3,3,3) filter & 160x160 image resolution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQxHzlc92oOq"
      },
      "source": [
        "Conv3D1.clear_session(Conv3D1_model)\n",
        "Conv3D2=ModelConv3D1()\n",
        "Conv3D2.initialize_src_path(main_folder)\n",
        "Conv3D2.initialize_image_properties(image_height=160,image_width=160)\n",
        "Conv3D2.initialize_hyperparams(frames_to_sample=20,batch_size=20,num_epochs=25)\n",
        "Conv3D2_model=Conv3D2.define_model(dense_neurons=256,dropout=0.5)\n",
        "Conv3D2_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCx8waqu3PY9"
      },
      "source": [
        "print(\"Total Params:\", Conv3D2_model.count_params())\n",
        "accuracy_check_model_2=Conv3D2.train_model(Conv3D2_model,augment_data=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXZ8sThy3XSr"
      },
      "source": [
        "plot(accuracy_check_model_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldsJg52PTDPq"
      },
      "source": [
        "Model is not overfitting and we get a best validation accuracy of ~80% and training accuracy of ~82%. Next we will try to reduce the filter size and image resolution and see if get better results. Moreover since we see minor oscillations in loss, let's try lowering the learning rate to 0.0002"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDO0BmuThfN_"
      },
      "source": [
        "Conv3D2.clear_session(Conv3D2_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qrMlLsVTSA7"
      },
      "source": [
        "## Model 3 - Reduce filter size to (2,2,2) and image res to 120 x 120"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkPpJFg3TRLh"
      },
      "source": [
        "class ModelConv3D3(ModelBuilder):\n",
        "    \n",
        "    def define_model(self,filtersize=(3,3,3),dense_neurons=64,dropout=0.25):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Conv3D(16, filtersize, padding='same',\n",
        "        input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(32, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(64, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(128, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "\n",
        "        model.add(Dense(self.num_classes,activation='softmax'))\n",
        "\n",
        "        optimiser = optimizers.Adam(lr=0.0002)\n",
        "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lk9L9pt7UBB1"
      },
      "source": [
        "\n",
        "Conv3D3=ModelConv3D3()\n",
        "Conv3D3.initialize_src_path(main_folder)\n",
        "Conv3D3.initialize_image_properties(image_height=120,image_width=120)\n",
        "Conv3D3.initialize_hyperparams(frames_to_sample=16,batch_size=30,num_epochs=30)\n",
        "Conv3D3_model=Conv3D3.define_model(filtersize=(2,2,2),dense_neurons=256,dropout=0.5)\n",
        "Conv3D3_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WW17sf6iUcQh"
      },
      "source": [
        "##tf.compat.v1.enable_eager_execution()\n",
        "print(tf.executing_eagerly())\n",
        "print(\"Total Params:\", Conv3D3_model.count_params())\n",
        "accuracy_check_model_3=Conv3D3.train_model(Conv3D3_model,augment_data=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HghaRZtGQB9"
      },
      "source": [
        "plot(accuracy_check_model_3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0IJgwK77wsF"
      },
      "source": [
        "This Model has a best validation accuracy of 68% and training accuracy of 68% . Also we were able to reduce the parameter size by half the earlier model. Let's trying adding more layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQ8jW39zjSXp"
      },
      "source": [
        "Conv3D3.clear_session(Conv3D3_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WW5VAmu9ihlH"
      },
      "source": [
        "## Model 4 - Adding more layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cyJpLxiitoB"
      },
      "source": [
        "class ModelConv3D4(ModelBuilder):\n",
        "    \n",
        "    def define_model(self,filtersize=(3,3,3),dense_neurons=64,dropout=0.25):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Conv3D(16, filtersize, padding='same',\n",
        "        input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(Conv3D(16, filtersize, padding='same',\n",
        "        input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(32, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(Conv3D(32, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(64, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(Conv3D(64, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(128, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(Conv3D(128, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "        \n",
        "\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "\n",
        "        model.add(Dense(self.num_classes,activation='softmax'))\n",
        "\n",
        "        optimiser = optimizers.Adam()\n",
        "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmOxJwmei5-S"
      },
      "source": [
        "Conv3D4=ModelConv3D4()\n",
        "Conv3D4.initialize_src_path(main_folder)\n",
        "Conv3D4.initialize_image_properties(image_height=120,image_width=120)\n",
        "Conv3D4.initialize_hyperparams(frames_to_sample=16,batch_size=20,num_epochs=30)\n",
        "Conv3D4_model=Conv3D4.define_model(filtersize=(3,3,3),dense_neurons=256,dropout=0.5)\n",
        "Conv3D4_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoR4sAUBkGDJ"
      },
      "source": [
        "print(\"Total Params:\", Conv3D4_model.count_params())\n",
        "accuracy_check_model_4=Conv3D4.train_model(Conv3D4_model,augment_data=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8lzWQankJXv"
      },
      "source": [
        "plot(accuracy_check_model_4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Q33XjvbSv8q"
      },
      "source": [
        "With more layers we see some performance improvement. We get a best validation accuracy of 73% . Let's try adding dropouts at the convolution layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PP6UEuorS7-z"
      },
      "source": [
        "## Model 5 Adding dropout at convolution layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjEDLlauS6iK"
      },
      "source": [
        "Conv3D4.clear_session(Conv3D4_model)\n",
        "class ModelConv3D5(ModelBuilder):\n",
        "    \n",
        "    def define_model(self,filtersize=(3,3,3),dense_neurons=64,dropout=0.25):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Conv3D(16, filtersize, padding='same',\n",
        "        input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(Conv3D(16, filtersize, padding='same',\n",
        "        input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Conv3D(32, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(Conv3D(32, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Conv3D(64, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(Conv3D(64, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Conv3D(128, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(Conv3D(128, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "        model.add(Dropout(dropout))\n",
        "        \n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "\n",
        "        model.add(Dense(self.num_classes,activation='softmax'))\n",
        "\n",
        "        optimiser = optimizers.Adam()\n",
        "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXzE7zAXTlb5"
      },
      "source": [
        "Conv3D5=ModelConv3D5()\n",
        "Conv3D5.initialize_src_path(main_folder)\n",
        "Conv3D5.initialize_image_properties(image_height=120,image_width=120)\n",
        "Conv3D5.initialize_hyperparams(frames_to_sample=16,batch_size=20,num_epochs=22)\n",
        "Conv3D5_model=Conv3D5.define_model(filtersize=(3,3,3),dense_neurons=256,dropout=0.25)\n",
        "Conv3D5_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JesB78x6TvM3"
      },
      "source": [
        "print(\"Total Params:\", Conv3D5_model.count_params())\n",
        "accuracy_check_model_5=Conv3D5.train_model(Conv3D5_model,augment_data=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSHXypIxT2_c"
      },
      "source": [
        "plot(accuracy_check_model_5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOot9MXnwkBV"
      },
      "source": [
        "\n",
        "Adding dropouts has further reduced validation accuracy as its not to learn generalizable features\n",
        "All models experimental models above have more than 1 million parameters. Let's try to reduce the model size and see the performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCTXMah8xFXr"
      },
      "source": [
        "Conv3D5.clear_session(Conv3D5_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FroiBKKw0un"
      },
      "source": [
        "## Model 6 - reducing the number of parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJj_Nb_IwnLi"
      },
      "source": [
        "class ModelConv3D6(ModelBuilder):\n",
        "    \n",
        "    def define_model(self,dense_neurons=64,dropout=0.25):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Conv3D(16, (3, 3, 3), padding='same',\n",
        "        input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(32, (2, 2, 2), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(64, (2, 2, 2), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(128, (2, 2, 2), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "        \n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(self.num_classes,activation='softmax'))\n",
        "\n",
        "        optimiser = optimizers.Adam(lr=0.0002)\n",
        "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKI57BD-xhK9"
      },
      "source": [
        "Conv3D6=ModelConv3D6()\n",
        "Conv3D6.initialize_src_path(main_folder)\n",
        "Conv3D6.initialize_image_properties(image_height=100,image_width=100)\n",
        "Conv3D6.initialize_hyperparams(frames_to_sample=16,batch_size=20,num_epochs=30)\n",
        "Conv3D6_model=Conv3D6.define_model(dense_neurons=128,dropout=0.25)\n",
        "Conv3D6_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ydywdxnxwbU"
      },
      "source": [
        "tprint(\"Total Params:\", Conv3D6_model.count_params())\n",
        "accuracy_check_model_6=Conv3D6.train_model(Conv3D6_model,augment_data=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7WQvNWYDybQ"
      },
      "source": [
        "plot(accuracy_check_model_6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBJFD6jaJsTV"
      },
      "source": [
        "For the above low memory foot print model the best validation accuracy of 73%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cArLoK-0RXxf"
      },
      "source": [
        "Conv3D6.clear_session(Conv3D6_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHU3qisEKMLX"
      },
      "source": [
        "## Model 7 - reducing the number of parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huD4YXHfJ9E7"
      },
      "source": [
        "class ModelConv3D7(ModelBuilder):\n",
        "    \n",
        "    def define_model(self,dense_neurons=64,dropout=0.25):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Conv3D(16, (3, 3, 3), padding='same',\n",
        "        input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(32, (3, 3, 3), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(64, (2, 2, 2), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(128, (2, 2, 2), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "        \n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(self.num_classes,activation='softmax'))\n",
        "\n",
        "        optimiser = optimizers.Adam(lr=0.0002)\n",
        "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VR3gHG0T256h"
      },
      "source": [
        "conv_3d7=ModelConv3D7()\n",
        "conv_3d7.initialize_src_path(main_folder)\n",
        "conv_3d7.initialize_image_properties(image_height=120,image_width=120)\n",
        "conv_3d7.initialize_hyperparams(frames_to_sample=16,batch_size=20,num_epochs=25)\n",
        "conv_3d7_model=conv_3d7.define_model(dense_neurons=64,dropout=0.25)\n",
        "conv_3d7_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s63kvjqd3UKl"
      },
      "source": [
        "print(\"Total Params:\", conv_3d7_model.count_params())\n",
        "accuracy_check_model_7=conv_3d7.train_model(conv_3d7_model,augment_data=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBJGgwaa3kOR"
      },
      "source": [
        "plot(accuracy_check_model_7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bBbPYK-RgAh"
      },
      "source": [
        "conv_3d7.clear_session(conv_3d7_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OW55j6S4f5y"
      },
      "source": [
        "## Model 8 - reducing the number of parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtU-6-0v4e12"
      },
      "source": [
        "class ModelConv3D8(ModelBuilder):\n",
        "    \n",
        "    def define_model(self,dense_neurons=64,dropout=0.25):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Conv3D(8, (3, 3, 3), padding='same',\n",
        "        input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(16, (3, 3, 3), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(32, (2, 2, 2), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(64, (2, 2, 2), padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "        \n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(self.num_classes,activation='softmax'))\n",
        "\n",
        "        optimiser = optimizers.Adam(lr=0.0002)\n",
        "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BC1Vxikp44zW"
      },
      "source": [
        "conv_3d8=ModelConv3D8()\n",
        "conv_3d8.initialize_src_path(main_folder)\n",
        "conv_3d8.initialize_image_properties(image_height=120,image_width=120)\n",
        "conv_3d8.initialize_hyperparams(frames_to_sample=16,batch_size=20,num_epochs=30)\n",
        "conv_3d8_model=conv_3d8.define_model(dense_neurons=64,dropout=0.25)\n",
        "conv_3d8_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oikZb8Ij5DNw"
      },
      "source": [
        "print(\"Total Params:\", conv_3d8_model.count_params())\n",
        "accuracy_check_model_8=conv_3d8.train_model(conv_3d8_model,augment_data=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ds6Ddq2y5SsD"
      },
      "source": [
        "plot(accuracy_check_model_8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq14-oPeXfA9"
      },
      "source": [
        "conv_3d8.clear_session(conv_3d8_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGRwZahyW2aW"
      },
      "source": [
        "## Model 9 - CNN- LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuOHUaOhW0gJ"
      },
      "source": [
        "class CNNRNN1(ModelBuilder):\n",
        "    \n",
        "    def define_model(self,lstm_cells=64,dense_neurons=64,dropout=0.25):\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(TimeDistributed(Conv2D(16, (3, 3) , padding='same', activation='relu'),\n",
        "        input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
        "        model.add(TimeDistributed(BatchNormalization()))\n",
        "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "        \n",
        "        model.add(TimeDistributed(Conv2D(32, (3, 3) , padding='same', activation='relu')))\n",
        "        model.add(TimeDistributed(BatchNormalization()))\n",
        "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "        \n",
        "        model.add(TimeDistributed(Conv2D(64, (3, 3) , padding='same', activation='relu')))\n",
        "        model.add(TimeDistributed(BatchNormalization()))\n",
        "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "        \n",
        "        model.add(TimeDistributed(Conv2D(128, (3, 3) , padding='same', activation='relu')))\n",
        "        model.add(TimeDistributed(BatchNormalization()))\n",
        "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "        \n",
        "        model.add(TimeDistributed(Conv2D(256, (3, 3) , padding='same', activation='relu')))\n",
        "        model.add(TimeDistributed(BatchNormalization()))\n",
        "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "        \n",
        "        model.add(TimeDistributed(Flatten()))\n",
        "\n",
        "\n",
        "        model.add(LSTM(lstm_cells))\n",
        "        model.add(Dropout(dropout))\n",
        "        \n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(Dropout(dropout))\n",
        "        \n",
        "        model.add(Dense(self.num_classes, activation='softmax'))\n",
        "        optimiser = optimizers.Adam()\n",
        "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCoDy3FXXJj4"
      },
      "source": [
        "cnn_rnn1=CNNRNN1()\n",
        "cnn_rnn1.initialize_src_path(main_folder)\n",
        "cnn_rnn1.initialize_image_properties(image_height=120,image_width=120)\n",
        "cnn_rnn1.initialize_hyperparams(frames_to_sample=18,batch_size=20,num_epochs=20)\n",
        "cnn_rnn1_model=cnn_rnn1.define_model(lstm_cells=128,dense_neurons=128,dropout=0.25)\n",
        "cnn_rnn1_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-t4EsndXVii"
      },
      "source": [
        "print(\"Total Params:\", cnn_rnn1_model.count_params())\n",
        "accuracy_check_model_9=cnn_rnn1.train_model(cnn_rnn1_model,augment_data=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gm-imdTuXwbq"
      },
      "source": [
        "plot(accuracy_check_model_9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZSjWET3rag4"
      },
      "source": [
        "cnn_rnn1.clear_session(cnn_rnn1_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gdy0_yQzOn7y"
      },
      "source": [
        "## Applying More Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onOKdsVRO26u"
      },
      "source": [
        "class ModelBuilderMoreAugmentation(metaclass= abc.ABCMeta):\n",
        "    \n",
        "    def initialize_src_path(self,main_folder):\n",
        "        self.train_doc = np.random.permutation(open(main_folder + '/' + 'train.csv').readlines())\n",
        "        self.val_doc = np.random.permutation(open(main_folder + '/' + 'val.csv').readlines())\n",
        "        self.train_path = main_folder + '/' + 'train'\n",
        "        self.val_path =  main_folder + '/' + 'val'\n",
        "        self.num_train_sequences = len(self.train_doc)\n",
        "        self.num_val_sequences = len(self.val_doc)\n",
        "        \n",
        "    def initialize_image_properties(self,image_height=100,image_width=100):\n",
        "        self.image_height=image_height\n",
        "        self.image_width=image_width\n",
        "        self.channels=3\n",
        "        self.num_classes=5\n",
        "        self.total_frames=30\n",
        "          \n",
        "    def initialize_hyperparams(self,frames_to_sample=30,batch_size=20,num_epochs=20):\n",
        "        self.frames_to_sample=frames_to_sample\n",
        "        self.batch_size=batch_size\n",
        "        self.num_epochs=num_epochs\n",
        "        \n",
        "        \n",
        "    def generator(self,source_path, folder_list, augment=False):\n",
        "        img_idx = np.round(np.linspace(0,self.total_frames-1,self.frames_to_sample)).astype(int)\n",
        "        batch_size=self.batch_size\n",
        "        while True:\n",
        "            t = np.random.permutation(folder_list)\n",
        "            num_batches = len(t)//batch_size\n",
        "        \n",
        "            for batch in range(num_batches): \n",
        "                batch_data, batch_labels= self.one_batch_data(source_path,t,batch,batch_size,img_idx,augment)\n",
        "                yield batch_data, batch_labels \n",
        "\n",
        "            remaining_seq=len(t)%batch_size\n",
        "        \n",
        "            if (remaining_seq != 0):\n",
        "                batch_data, batch_labels= self.one_batch_data(source_path,t,num_batches,batch_size,img_idx,augment,remaining_seq)\n",
        "                yield batch_data, batch_labels \n",
        "    \n",
        "    \n",
        "    def one_batch_data(self,source_path,t,batch,batch_size,img_idx,augment,remaining_seq=0):\n",
        "    \n",
        "        seq_len = remaining_seq if remaining_seq else batch_size\n",
        "    \n",
        "        batch_data = np.zeros((seq_len,len(img_idx),self.image_height,self.image_width,self.channels)) \n",
        "        batch_labels = np.zeros((seq_len,self.num_classes)) \n",
        "    \n",
        "        if (augment): batch_data_aug = np.zeros((seq_len,len(img_idx),self.image_height,self.image_width,self.channels))\n",
        "\n",
        "        \n",
        "        for folder in range(seq_len): \n",
        "            imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) \n",
        "            for idx,item in enumerate(img_idx): \n",
        "                image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
        "                image_resized=transform.resize(image,(self.image_height,self.image_width,3))\n",
        "            \n",
        "\n",
        "                batch_data[folder,idx,:,:,0] = (image_resized[:,:,0])/255\n",
        "                batch_data[folder,idx,:,:,1] = (image_resized[:,:,1])/255\n",
        "                batch_data[folder,idx,:,:,2] = (image_resized[:,:,2])/255\n",
        "            \n",
        "                if (augment):\n",
        "                    shifted = cv2.warpAffine(image, \n",
        "                                             np.float32([[1, 0, np.random.randint(-30,30)],[0, 1, np.random.randint(-30,30)]]), \n",
        "                                            (image.shape[1], image.shape[0]))\n",
        "                    \n",
        "                    gray = cv2.cvtColor(shifted,cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "                    x0, y0 = np.argwhere(gray > 0).min(axis=0)\n",
        "                    x1, y1 = np.argwhere(gray > 0).max(axis=0) \n",
        "                    \n",
        "                    cropped=shifted[x0:x1,y0:y1,:]\n",
        "                    \n",
        "                    image_resized=transform.resize(cropped,(self.image_height,self.image_width,3))\n",
        "                    \n",
        "                    M = cv2.getRotationMatrix2D((self.image_width//2,self.image_height//2),\n",
        "                                                np.random.randint(-10,10), 1.0)\n",
        "                    rotated = cv2.warpAffine(image_resized, M, (self.image_width, self.image_height))\n",
        "                  \n",
        "                    batch_data_aug[folder,idx,:,:,0] = (rotated[:,:,0])/255\n",
        "                    batch_data_aug[folder,idx,:,:,1] = (rotated[:,:,1])/255\n",
        "                    batch_data_aug[folder,idx,:,:,2] = (rotated[:,:,2])/255\n",
        "                \n",
        "            \n",
        "            batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
        "            \n",
        "    \n",
        "        if (augment):\n",
        "            batch_data=np.concatenate([batch_data,batch_data_aug])\n",
        "            batch_labels=np.concatenate([batch_labels,batch_labels])\n",
        "\n",
        "        \n",
        "        return(batch_data,batch_labels)\n",
        "    \n",
        "    \n",
        "    def train_model(self, model, augment_data=False):\n",
        "        train_generator = self.generator(self.train_path, self.train_doc,augment=augment_data)\n",
        "        val_generator = self.generator(self.val_path, self.val_doc)\n",
        "\n",
        "        model_name = 'model_init' + '_' + str(datetime.datetime.now()).replace(' ','').replace(':','_') + '/'\n",
        "    \n",
        "        if not os.path.exists(model_name):\n",
        "            os.mkdir(model_name)\n",
        "        \n",
        "        filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
        "\n",
        "        checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
        "        LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4)\n",
        "        callbacks_list = [checkpoint, LR]\n",
        "\n",
        "        if (self.num_train_sequences%self.batch_size) == 0:\n",
        "            steps_per_epoch = int(self.num_train_sequences/self.batch_size)\n",
        "        else:\n",
        "            steps_per_epoch = (self.num_train_sequences//self.batch_size) + 1\n",
        "\n",
        "        if (self.num_val_sequences%self.batch_size) == 0:\n",
        "            validation_steps = int(self.num_val_sequences/self.batch_size)\n",
        "        else:\n",
        "            validation_steps = (self.num_val_sequences//self.batch_size) + 1\n",
        "    \n",
        "        history=model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=self.num_epochs, verbose=1, \n",
        "                            callbacks=callbacks_list, validation_data=val_generator, \n",
        "                            validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n",
        "        return history\n",
        "\n",
        "        \n",
        "    @abc.abstractmethod\n",
        "    def define_model(self):\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNCI5V1JPp5N"
      },
      "source": [
        "## Model 10 - (3,3,3) Filter & 160x160 Image resolution - similar to Model 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHkhEeqoPoZ-"
      },
      "source": [
        "class ModelConv3D10(ModelBuilderMoreAugmentation):\n",
        "    \n",
        "    def define_model(self,filtersize=(3,3,3),dense_neurons=64,dropout=0.25):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Conv3D(16, filtersize, padding='same',\n",
        "        input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(32, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(64, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(128, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "\n",
        "        model.add(Dense(self.num_classes,activation='softmax'))\n",
        "\n",
        "        optimiser = optimizers.Adam(lr=0.0002)\n",
        "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zxshqMLQNAK"
      },
      "source": [
        "conv_3d10=ModelConv3D10()\n",
        "conv_3d10.initialize_src_path(main_folder)\n",
        "conv_3d10.initialize_image_properties(image_height=160,image_width=160)\n",
        "conv_3d10.initialize_hyperparams(frames_to_sample=20,batch_size=20,num_epochs=30)\n",
        "conv_3d10_model=conv_3d10.define_model(dense_neurons=256,dropout=0.5)\n",
        "conv_3d10_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Qjf4MvRQSSG"
      },
      "source": [
        "print(\"Total Params:\", conv_3d10_model.count_params())\n",
        "accuracy_check_model_10=conv_3d10.train_model(conv_3d10_model,augment_data=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOdRzLqFpbxp"
      },
      "source": [
        "plot(accuracy_check_model_10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGr80KSVriGs"
      },
      "source": [
        "conv_3d10.clear_session(conv_3d10_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1Nyst3JoisB"
      },
      "source": [
        "## Model 11 - (2,2,2) Filter & 120x120 Image resolution - similar to Model 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HRBTvETolll"
      },
      "source": [
        "class ModelConv3D11(ModelBuilderMoreAugmentation):\n",
        "    \n",
        "    def define_model(self,filtersize=(3,3,3),dense_neurons=64,dropout=0.25):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Conv3D(16, filtersize, padding='same',\n",
        "        input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(32, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(64, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Conv3D(128, filtersize, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(dense_neurons,activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "\n",
        "        model.add(Dense(self.num_classes,activation='softmax'))\n",
        "\n",
        "        optimiser = optimizers.Adam(lr=0.0002)\n",
        "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WWwgLRypCj3"
      },
      "source": [
        "conv_3d11=ModelConv3D11()\n",
        "conv_3d11.initialize_src_path(main_folder)\n",
        "conv_3d11.initialize_image_properties(image_height=160,image_width=160)\n",
        "conv_3d11.initialize_hyperparams(frames_to_sample=20,batch_size=20,num_epochs=30)\n",
        "conv_3d11_model=conv_3d11.define_model(dense_neurons=256,dropout=0.5)\n",
        "conv_3d11_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUG3v2hOpJYd"
      },
      "source": [
        "print(\"Total Params:\", conv_3d11_model.count_params())\n",
        "accuracy_check_model_11=conv_3d11.train_model(conv_3d11_model,augment_data=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlA0dEQ8puAY"
      },
      "source": [
        "plot(accuracy_check_model_11)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWgA-0hcrp5V"
      },
      "source": [
        "conv_3d11.clear_session(conv_3d11_model)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}